https://github.com/epsilon-machine/missingpy/issues/22,I get this deprecation warning when I use the package:I see that KNN impute reference sklearn.neighbors.base.,None yet
https://github.com/TeamHG-Memex/Formasaurus/issues/26,"scikit-learn 0.23 was released three days ago: https://github.com/scikit-learn/scikit-learn/releases
New installations of formasaurus are not possible any more using the default pip installation.Either, the code is adapted (import directly from joblib), or requirements.txt is updated to:Current workaround is:@al3xdelarge This is fixed by 5f78264Any way to get this merged/closed and pushed to PyPi? Thanks in advance.Looks like the environment variable for python 3.6 test is incorrect (py34).https://travis-ci.org/github/TeamHG-Memex/Formasaurus/jobs/693885894",None yet
https://github.com/USGS-Astrogeology/PyHAT_Point_Spectra_GUI/issues/315,,deprecation
https://github.com/andosa/treeinterpreter/issues/33,"I still get the sklearn deprecation warning about sklearn.ensemble.forest, and when I ran pip install --upgrade treeinterpreter, I got the message that I was already up-to-date.",None yet
https://github.com/chriswbartley/monoensemble/issues/2,"sklearn.utils.validation.check_is_fitted no longer takes the second argument (e.g. 'prior' in mono_gradient_boosting.py lines 130, 156.",None yet
https://github.com/rapidsai/cuml/issues/2840,"Describe the bug
A few examples are: rows_sample, bootstrap_features, etc. One approach is to keep these names, but add deprecation message for this release.Tagging @JohnZed @hcho3 and @vinaydes for discussions.",Algorithm API Change bug doc
https://github.com/PacificBiosciences/ANGEL/issues/36,"Hi @Magdoll,
I'm running the ANGEL v3.0 with the example data and I got the following error:$angel_predict.py test.fa MCF7_2015.dumb.final.training.pickle test_angel Reading classifer pickle: MCF7_2015.dumb.final.training.pickle /root/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.weight_boosting module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API. warnings.warn(message, FutureWarning) /root/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.tree.tree module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API. warnings.warn(message, FutureWarning) Traceback (most recent call last): File ""/root/ANGEL/angel_predict.py"", line 22, in <module> distribute_ANGEL_predict(args.fasta_filename, args.output_prefix, args.classifier_pickle, args.cpus, args.min_angel_aa_length, args.min_dumb_aa_length, args.use_rev_strand, args.output_mode, args.max_angel_secondORF_distance) File ""/root/anaconda3/envs/py37/lib/python3.7/site-packages/Angel-3.0-py3.7-linux-x86_64.egg/Angel/SmartORF.py"", line 254, in distribute_ANGEL_predict a = load(f) UnicodeDecodeError: 'ascii' codec can't decode byte 0x81 in position 1: ordinal not in range(128).Any advice will be appreciated.Hi @yy-liang ,
Oh my bad! The trained classifier pickles need to be updated for Python 3 version. Please don't use them for now. I am going to remove them from the GItHub in the meantime until a new one is up.",None yet
https://github.com/LLNL/barni/issues/5,"There is warning:
/home/kai/env/python3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is deprecated in version 0.22 and will be removed in version 0.24
When running training data",None yet
https://github.com/skylergrammer/SimulatedAnnealing/issues/5,When trying to run the example I get this error message:Would be great to have the requirements.txt updated.,None yet
https://github.com/gkhayes/mlrose/pull/50,"The changes here should avoid the deprecation warnings from sklearn about its internal six module being deprecated in favor of the ""official"" six module:I also changed the name used to identify scikit-learn as a requirement to (what I understand to be) the more correct name scikit-learn which avoids the ""alias"" package sklearn-0.0 from getting installed.All the tests run by ./test/test.sh still pass with this change, so hopefully this is correct. If not, hopefully this is a hint in the direction on how to avoid the deprecation warning.@gkhayes, any chance you could take a look at this?I am getting an error when trying to use mlrose as mentioned above:And making the first change get's rid of any error occurring during import.It seems that scikit-learn has released 0.23.1 and removed six from their sklearn.externals module, so this warning is now an error. Hopefully mlrose will be updated soon as well?I downgraded my scikit-learn to version 0.22 and my code is now working again. So my requirements.txt file now has:scikit-learn==0.22hi guys, love the package. Any plans to refactor this?@arainboldt, it appears that mlrose-hiive (a fork of this repo) has done this.We've moved to using that module now.",None yet
https://github.com/Teichlab/cellphonedb/issues/159,"Hello,
I am new to CellphoneDB and it looks very exciting!
However, after downloading and installation this message came up:
/Users/chrsyl/cpdb-venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API. warnings.warn(message, FutureWarning)
Can I get a hint on how to update?Hi Chryl,
I am getting the same message. Did you solve the issue?Hi @chrsyl @life-gif ... I think it is just a warning. I had a similar warning but the later commands should run smoothly.Hi. I get the same error message when I try to run the dotplot or heatmap functions. As a consequence, I can't find the plot neither in the output directory nor anywhere else. Even though the plot pops up briefly...
Any help?cellphonedb plot dot_plot The sklearn.cluster.k_means_ module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.",None yet
https://github.com/aws-samples/serverless-ai-workshop/pull/6,"Issue #, if available:Description of changes:By submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.",None yet
https://github.com/ysig/GraKeL/issues/44,"DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
warnings.warn(msg, category=DeprecationWarning)cannot import name 'joblib' from 'sklearn.externalsI think it has been removed and I am unable to use it.",None yet
https://github.com/scikit-garden/scikit-garden/issues/85,"As of scikit-learn v0.22, the presort parameter to BaseDecisionTree is deprecated.
As the RandomForestQuantileRegressor uses a lot of these, this results in a LOT of deprecation warnings being printed, when using newer scikit-learn versions.",None yet
https://github.com/PIA-Group/BioSPPy/pull/65,"Removed the deprecation warning shown by current sklearn version, by importing joblib independently in biosppy/storage.py rather than from sklearn.externals.
Added joblib as a requirement in setup.py, requirements.txt and README.mdNow, this PR is critical. The sklearn removed the joblib from the externals, so, if you try to use the biosppy storage it raises an error of cannot import like",None yet
https://github.com/skggm/skggm/issues/124,"Hi, when I try to import inverse_covarance in skggm, the following error pops up.The issue is because sklearn.externals.joblib is deprecated from scikit-learn 0.21 onwards and has been removed in 0.23 of scikit-learn. Downgrading scikit-learn to 0.20.1 removes the error message. While, v0.21.1 of scikit-learn doesn't throw up the error, it does throw a deprecation warning.This is something you might want to update in the requirements.txt and also in the setup.py file.EDIT: I am using Python 3.6.10 if it is relevant.Hey, could you solve this one. Please let me know how to solve this errorThanks @sunidhibatra",bug help wanted sklearn cleanup
https://github.com/coreylynch/pyFM/issues/29,sklearn 0.19.1You just need to modify two lines in file 'pylibfm.py' to get rid of the warning message as below:,None yet
https://github.com/udacity/machine-learning/issues/347,"https://github.com/udacity/machine-learning/blob/master/projects/boston_housing/boston_housing.ipynbThe below line of code in udacity/machine-learning/boston_housing.ipynb failed for me:I believe it should be:This is also the case in https://github.com/udacity/machine-learning/blob/master/projects/visuals.py. I believe the following line:should be@arembridge Is code failed or it gave Deprecation warning? sklearn.cross_validation was deprecated in version 0.18 and as you said new version has sklearn.model_selection.It is deprecated. I have changed the code and got it to work here:https://github.com/arembridge/machine-learning/blob/master/projects/boston_housing/visuals.pyExactly sklearn.cross_validation was deprecated. Work with sklearn.model_selection.Sklearn.cross_validation was depricated
Try using sklearn.model_selection",None yet
https://github.com/manuel-calzolari/sklearn-genetic/issues/11,"Environment VersionsSteps to replicateExpected resultRunning from genetic_selection import GeneticSelectionCV works without error.Actual resultThis is happening because in scikit-learn 0.20.0 they relocated sklearn.externals.joblib to from sklearn.utils._joblib import cpu_count (see https://scikit-learn.org/dev/whats_new/v0.20.html#miscellaneous).The easiest fix here is to simply state that sklearn-genetic only works with scikit-learn>=0.20.0. If you want to add backwards compatibility to 0.18 then you'll need some conditional logic to check the sklearn version in order to determine where to import cpu_count from.For now I can give you a PR which gets this library working again and pip installable via PyPI. There are further issues for getting this working with Python 3.8 which I shall detail in a separate issue.Another reason to simply require scikit-learn>=0.20.0 is because scitki-learn version 0.18 doesn't follow modern Python packaging conventions in that it doesn't specify or automatically install numpy and scipy as required sub-dependencies, instead requiring the user to manually install these in their environment first.See scikit-learn/scikit-learn#7867 and scikit-learn/scikit-learn#8242 for discussion of this problem and why it existed. This was resolved in sklearn 0.20 - https://github.com/scikit-learn/scikit-learn/blob/0.20.0/setup.pyIn certain situations, this may cause issues with even installing sklearn-genetic==0.2, e.g. if someone does indeed want to use scitkit-learn==0.18 and uses pip-tools to pin the version, pip-tools won't put numpy or scipy in it's auto-generated requirements.txt due to sklearn 0.18 not correctly specifying them as requirements, which means the installation fails. This was an issue I faced when trying to test to see how hard it would be to make this package work for both 0.18 and >=0.20.0",None yet
https://github.com/scikit-learn/scikit-learn/pull/18436,"This PR adds some missing _deprecate_positional_args to the public classes and functions (if I am not wrong).@NicolasHug I have double-check and sklearn.metrics.ConfusionMatrixDisplay, sklearn.metrics.PrecisionRecallDisplay and sklearn.metrics.RocCurveDisplay still need the deprecation cycle (they were introduced without *).Looks like DetCurveDisplay and plot_det_curve haven't been released yet and were properly introduced with a *. No need to change these. Please revert them, I'll pre-aprove though. Thanks!Yep, you are right (indeed, I edited the comment when I noticed).Note that I have removed the @_deprecate_positional_args in the plot method of sklearn.metrics.DetCurveDisplay because it was released with *.",module:linear_model module:metrics
https://github.com/yoavram/curveball/issues/147,Code:Warning message:,refactoring
https://github.com/rohinkumar/correlcalc/issues/1,"Failed to install pymangle to install correlcalc. I already installed wheel, but unable to install pymangle. Don't know how to solve this problem. I also already try to install from this git project. Could you please give me some suggestion? Here is the output of sudo pip install correlcalc:ERROR: Failed building wheel for pymangle
Running setup.py clean for pymangle
Building wheel for sklearn (setup.py) ... done
Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1319 sha256=143da93891660e894c49d41c7cd920f490fb6376ec9b4590157dc875b2b75b61
Stored in directory: /private/tmp/pip-ephem-wheel-cache-8ja5dfx2/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c
Successfully built sklearn
Failed to build correlcalc pymangle
DEPRECATION: Could not build wheels for correlcalc, pymangle which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at pypa/pip#8368.
Installing collected packages: pymangle, sklearn, correlcalc
Running setup.py install for pymangle ... error
ERROR: Command errored out with exit status 1:
command: /opt/local/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/tmp/pip-install-_2q_ti8t/pymangle/setup.py'""'""'; file='""'""'/private/tmp/pip-install-_2q_ti8t/pymangle/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(file);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, file, '""'""'exec'""'""'))' install --record /private/tmp/pip-record-e9qfsagx/install-record.txt --single-version-externally-managed --compile --install-headers /opt/local/Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m/pymangle
cwd: /private/tmp/pip-install-_2q_ti8t/pymangle/
Complete output (21 lines):
running install
running build
running build_py
creating build
creating build/lib.macosx-10.14-x86_64-3.6
creating build/lib.macosx-10.14-x86_64-3.6/pymangle
copying pymangle/version.py -> build/lib.macosx-10.14-x86_64-3.6/pymangle
copying pymangle/mangle.py -> build/lib.macosx-10.14-x86_64-3.6/pymangle
copying pymangle/init.py -> build/lib.macosx-10.14-x86_64-3.6/pymangle
copying pymangle/test.py -> build/lib.macosx-10.14-x86_64-3.6/pymangle
running build_ext
building 'pymangle._mangle' extension
creating build/temp.macosx-10.14-x86_64-3.6
creating build/temp.macosx-10.14-x86_64-3.6/pymangle
/usr/bin/clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -pipe -Os -isysroot/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk -I/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/include -I/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/include -I/opt/local/Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c pymangle/_mangle.c -o build/temp.macosx-10.14-x86_64-3.6/pymangle/_mangle.o
clang: warning: no such sysroot directory: '/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk' [-Wmissing-sysroot]
pymangle/_mangle.c:3:10: fatal error: 'string.h' file not found
#include <string.h>
^~~~~~~~~~
1 error generated.
error: command '/usr/bin/clang' failed with exit status 1
----------------------------------------
ERROR: Command errored out with exit status 1: /opt/local/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/tmp/pip-install-_2q_ti8t/pymangle/setup.py'""'""'; file='""'""'/private/tmp/pip-install-_2q_ti8t/pymangle/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(file);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, file, '""'""'exec'""'""'))' install --record /private/tmp/pip-record-e9qfsagx/install-record.txt --single-version-externally-managed --compile --install-headers /opt/local/Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m/pymangle Check the logs for full command output.`",None yet
https://github.com/cgre-aachen/gempy/issues/475,"Describe the bug
When sampling from a von Mises Fisher Distribution (fishdist.py) in GemPy the, code breaks since sklearn.externals.joblib was removed from sklearn in version 0.23(.1) (currently the lastest version). The package spherecluster (master updated 2 years ago) is still dependent on this package.To Reproduce
Run a model with scikit-learn==0.23 and try to sample from a vMF distribution.Expected behavior
Sampling from the vMF distribution should not result in an error. It seems like the development branch of spherecluster does not have this dependency anymore and was updated 'only' 6 months ago. So this could work. What worked for me now is downgrading sklearn. In the long term, we could try to implement our own solution expanding the previous work of @elimh to remove the dependency on spherecluster. Could be interesting for @Japhiolite as well in combination with #449.Desktop (please complete the following information):Additional context
Add any other context about the problem here.",dependency
https://github.com/jakevdp/sklearn_pycon2015/pull/7,\miniconda\lib\site-packages\sklearn\utils\validation.py:395 : DeprecationWarning : Passing 1d arrays as data is deprecated in 0.17 and will raise value error in 0.19,None yet
https://github.com/mynameisfiber/pyxmeans/issues/12,"Clustering with multi-threaded pyxmeans (starting k at 20)
/usr/local/lib/python2.7/dist-packages/pyxmeans/xmeans.py:135: FutureWarning: elementwise comparison failed; returning s calar instead, but in the future will perform elementwise comparison
if centroids in ('kmeans++', 'random'):
/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
DeprecationWarning)",None yet
https://github.com/sevamoo/SOMPY/issues/108,"I will patch if I can find the time in the near future, but just wanted to flag that installing the most recent version of SOMPY with a clean install of sklearn triggers this alert:PR issued. Note potential implications for Py2 compatibility. If you want me to move this to a sompy3 namespace and push to PyPi then I can probably work out how to do that.",None yet
https://github.com/scikit-learn-contrib/imbalanced-learn/issues/727,"Over the last few days, users have reported issues after installing imbalanced-learn (e.g. #723, #725, #726)TL;DR: Users with systems that build based on setup.py experience errors.Set up an environment to imitate a user who currently has scikit-learn==0.22.1:These are the default paths on my system:Installing imbalanced-learn with pip on Ubuntu uses the wheel from PyPi:And the paths appear correct following installation:Again, these seem to be the default paths on my system:But now, installing imbalanced-learn with setup.pyThis method creates three new entries under site-packages:... but does not appear to properly clean up the old scikit-learn version:It looks like something during setup is creating an easy-install.pth with the following contents:Hi,Unfortunately, Anaconda doesn't have the latest version of SKLearn (they are on ver 0.22.1). This means I still get the error when I run my code on Jupyter. I tried installing the latest SKlearn via pip but I think Anaconda reads from their own file path.I'm thinking if :
I can replace the SKlearn lib in Anaconda with the one installed via pip; but I don't want to mess with the files.
OR
I can wait for Anaconda to update their lib.Any comments about this would be helpful!@xjyribro you can install the version from the conda-forge: conda insall scikit-learn -c conda-forge.
It is packaged by the people from scikit-learn so it will be OK. When the package will be available in the default channel then you can use it.@hayesall I think that people should not use python setup.py install and instead pip install . as mentioned in the documentation. I remember some old deprecation from pip that was complaining about not cleaning properly stuff when using the former solution. I did the following:and it seems to work:Shouldn't we then document it?I am still not sure if it was a pip issue. If it was a pip issue, we should probably report the problem instead of documenting a workaround, isn't it?Windows-10-10.0.19041-SP0
Python 3.7.7 (default, Apr 15 2020, 05:09:04) [MSC v.1916 64 bit (AMD64)]
NumPy 1.18.1
SciPy 1.4.1
Scikit-Learn 0.23.2
imblearn 0.7.0",None yet
https://github.com/scikit-optimize/scikit-optimize/issues/924,"I haven't had any issues before, but I try to run the same code as before I now I get an error ""object.init() takes exactly one argument (the instance to initialize)""I'm having the same issue.skopt version: 0.7.4
sklearn version: 0.23.1In BayesSearchCV the MaskedArray is used to mask non-applicable params. But in sklearn it will trigger a warning:
""FutureWarning: Class MaskedArray is deprecated; MaskedArray is deprecated in version 0.23 and will be removed in version 0.25. Use numpy.ma.MaskedArray instead.""In sklearn/utils/deprecation.py, the class deprecated is initiated as def __init__(self, extra=''). I guess it is contradictory with line 63-68:Is there anyone found a solution to this error.??",None yet
https://github.com/scikit-garden/scikit-garden/issues/81,Hi when I import skgarden I get those warnings :Are there plans to upgrade the dependencies ?Thanks,None yet
https://github.com/saagie/technologies/pull/141,"Closes #140auto-sklearn lib has been removed temporarily because it leads the build to fail due to :@PierreLeresteux you can review this one, tests are OK for me.
May be someone from the service could test and review it as well ?@PierreLeresteux tests updated.
Review par service OK",None yet
https://github.com/cassiobotaro/machine_learning_recipes/issues/6,Remove deprecation warnings in videos 4 and 5.,enhancement
https://github.com/amueller/scipy_2015_sklearn_tutorial/issues/60,"Hi, I'm getting DeprecationWarning when executing:
from sklearn.cross_validation import train_test_split/home/user/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. ""This module will be removed in 0.20."", DeprecationWarning)Solution:
from sklearn.model_selection import train_test_splitWell, this is the 2015 tutorial with worked with the 2015 scikit-learn ;) There is many places that would need fixing to not raise deprecation warnings.",None yet
https://github.com/scikit-garden/scikit-garden/issues/88,"This is an attempt to collect all the sklearn dependency issues currently present.Presort parameter
The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter.
See also: #85
Solution: Remove presort parameter when calling BaseDecisionTree.Joblib
C:\Users\natha\Miniconda3\envs\ml\lib\site-packages\sklearn\externals\joblib\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib.
See also: #81
Solution: Use joblib directly, rather than sklearn.externals.joblib.Six
C:\Users\natha\Miniconda3\envs\ml\lib\site-packages\sklearn\externals\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7.
See also: #81
Solution: Replace sklearn.externals.six with six.Ensemble and tree
/home/leidem/gitsky/streetwise-model/venv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.
/home/leidem/gitsky/streetwise-model/venv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.tree.tree module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.
Solution: Replace sklearn.ensemble.forest by sklearn.ensemble, and sklearn.tree.tree by sklearn.tree.Testing
FutureWarning: The sklearn.utils.testing module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.
Solution: Replace sklearn.utils.testing with sklearn.utils.Implemented the above and more in #89",None yet
https://github.com/mlp2018/BagofWords/issues/42,"DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
""This module will be removed in 0.20."", DeprecationWarning)Also check if there are other deprecation warnings - I think I saw another one but I'm not sure.I looked into this deprecation warning and don't think there is much we can do.It arises when importing StratifiedKFold from sklearn.cross_validation (see below). However, this method is needed to make the code work when working with Python 3.5, and working with Python 3.5 in turn is needed to make TensorFlow work (at least on Windows). So I don't see how this could be fixed... Thoughts?There is, it should already be fixed in gnnc branch. Not sure why it's not in master yet.",None yet
https://github.com/Swarchal/morar/issues/36,,None yet
https://github.com/udacity/DSND_Term1/pull/18,"sklearn.cross_validation is deprecated (https://stackoverflow.com/questions/43302400/deprecation-warnings-from-sklearn)
so using sklearn.model_selection for importing train_test_split (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)",None yet
https://github.com/Teichlab/cellphonedb/issues/198,"Hello, CellPhoneDB is a great software.
I have a small problem when I use mean.txt and pvalue.txt to draw pictures. This is my code and error:cellphonedb plot dot_plot --means-path ./out/means.txt --pvalues-path ./out/pvalues.txt --output-path ./out_plot./lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.cluster.k_means_ module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API. warnings.warn(message, FutureWarning) [ ][APP][09/08/20-15:02:43][ERROR] Unexpected error Traceback (most recent call last): File ""./lib/python3.6/site-packages/cellphonedb/src/api_endpoints/terminal_api/plot_terminal_api_endpoints/plot_terminal_commands.py"", line 38, in dot_plot columns=columns) File ""./lib/python3.6/site-packages/cellphonedb/src/plotters/r_plotter.py"", line 35, in wrapper from rpy2.rinterface_lib.embedded import RRuntimeError File ""./lib/python3.6/site-packages/rpy2/rinterface_lib/embedded.py"", line 7, in <module> from . import openrlib File ""./lib/python3.6/site-packages/rpy2/rinterface_lib/openrlib.py"", line 21, in <module> rlib = _dlopen_rlib(R_HOME) File ""./lib/python3.6/site-packages/rpy2/rinterface_lib/openrlib.py"", line 17, in _dlopen_rlib rlib = ffi.dlopen(lib_path) OSError: cannot load library '/opt/R-3.6.1/lib/libR.so': /opt/R-3.6.1/lib/libR.so: cannot open shared object file: No such file or directory
I don’t know why this problem occurred, I installed the latest version of cellphonedb and R v3.6.3.
Expect a reply! Thank you!Hi,
Do you solve this problem, any ideas?
I also meet this problem.
thx",None yet
https://github.com/alaindomissy/sklearn_pycon2015/pull/1,"Correction to avoid deprecation warning:
/home/user/anaconda3/envs/sklearn_pycon2015/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
DeprecationWarning)",None yet
https://github.com/astroML/astroML/issues/204,"To follow-up on scikit-learn/scikit-learn#9250.Easy workaround is to switch to the private baseclass, but preferable solution is to use a public class. The deprecation was added in the new 0.22 version, removal is foreseen in 0.24.",refactoring
https://github.com/TeamHG-Memex/eli5/issues/360,"Started receiving the following message after a few updates. Sklearn access in upcoming versions should only be through their public api.FutureWarning: The sklearn.metrics.scorer module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API. warnings.warn(message, FutureWarning)FutureWarning: The sklearn.feature_selection.base module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API. warnings.warn(message, FutureWarning)I get the same too, it's because of upgrading to scikit-learn 0.22.x. You can suppress it like:",None yet
https://github.com/meaghanflagg/jackstraw/issues/1,"jackstraw.py
DeprecationWarning:
sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+",good first issue
https://github.com/VowpalWabbit/vowpal_wabbit/issues/2240,"Related to #2183
Sklearn has deprecated a lot of interfaces into the library in sklearn 0.22. Most of the issues have been addressed in #2208, but there are 2 major objects we'll lose access to with no good replacements.The two Mixin classes from LinearModel have been deprecated as of sklearn 0.22, and will be removed entirely by sklearn 0.24. We'll need to figure out exactly what we lose and how to replace it.Can you add some sklearn references here from sklearn?
I could not find it.Here's the PR: scikit-learn/scikit-learn#15324I wonder what do we lose if we switch to ClassifierMixin/RegressorMixin.Here's an explanation of the deprecations (though it doesn't say exactly when the private API will be closed off)
https://scikit-learn.org/stable/whats_new/v0.22.html#clear-definition-of-the-public-apiThe deprecation warning in sklearn 0.22 mention we'll lose them in 0.24. Their public API is documented here
https://scikit-learn.org/dev/modules/classes.htmlHey @peterychangWhat do you think about writing the above classes by ourselves using the Public API instead of importing them from the new version? This will also ensure that in future we can be sure that we don't end up having broken code often.I believe it should be fairly easy as it'll only require us to write one or two methods by ourselves and use the parent of the LinearClassifierMixin to derive our own class.A similar approach is possible for SparseCoefMixin.@jackgerrits @peterychang Any feedback on my comment?I'm starting work on this.I'm wondering if it would be possible to adapt sklearn.svm.LinearSVM for this (This object is in the public API).LinearSVM's inheritance diagram looks similar to the one we use
class LinearSVC(BaseEstimator, LinearClassifierMixin, SparseCoefMixin)vs what we have in sklearn_vwI'm not particularly familiar with python's object inheritance model though, so I'm not sure if we'd be able to modify the behavior in LinearSVM to run what we need.I'll look into this @peterychang . I've quite some experience with inheritance and I'll update the PR accordingly.",Lang: Python
https://github.com/bertrandlalo/timeflux_rasr/issues/28,"I know it is not directly related to rASR, but importing pyriemann give these warnings:I am concerned that rASR will stop working in the near future because of that. Is someone willing to fix this upstream ?",None yet
https://github.com/wikilinks/nel/issues/3,No description provided.,bug
https://github.com/Lucas-Kohorst/Python-Stock/issues/7,"Apparently cross_validation is deprecated in sklearn: https://stackoverflow.com/questions/53978901/importerror-cannot-import-name-cross-validation-from-sklearnAlso get a message:
ImportError: cannot import name 'get_historical_data' from 'iexfinance' which also sounds like a deprecation problem, but I can't be sure.Hope this helps!",None yet
https://github.com/pypeit/PypeIt/issues/952,This warning is disconcerting:UserWarning: Trying to unpickle estimator GaussianMixture from version 0.20.0 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk. (base.py:318)Please find a way to mitigate it.This is worse.FutureWarning: The sklearn.mixture.gaussian_mixture module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.mixture. Anything that cannot be imported from sklearn.mixture is now part of the private API. (deprecation.py:144)Yeah we should not be pickling things anyway. It turns out this mixture model is not even used.,None yet
https://github.com/ageitgey/face_recognition/issues/1014,"I am getting the following warning and exception after updating the sklearn to the latest version 0.22scikit-learn==0.22/usr/local/lib/python3.5/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.classification module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.
warnings.warn(message, FutureWarning)
/usr/local/lib/python3.5/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.ball_tree module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.
warnings.warn(message, FutureWarning)
/usr/local/lib/python3.5/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.dist_metrics module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.
warnings.warn(message, FutureWarning)
/usr/local/lib/python3.5/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.20.3 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.Exception : 'KNeighborsClassifier' object has no attribute 'n_samples_fit_'I installed the scikit-learn==0.20 package and it worked as expected.So, i request to add the scikit-learn==0.20 in the ""requirements.txt"" file",None yet
https://github.com/Avik-Jain/100-Days-Of-ML-Code/pull/42,"modify cross_validation to sklearn.model_selection :)Thank you for your contribution :)Please complete the below steps before filing your PR:I have read and understood CONTRIBUTING document.
I have read and understood CODE_OF_CONDUCT document.
I have included tests for the changes in my PR. If not, I have included a rationale for why I haven't.
I understand my PR may be closed if it becomes obvious I didn't actually perform all of these steps.[Please explain in detail why the changes in this PR are needed.]",None yet
https://github.com/Ji-Zhang/datacleanbot/issues/2,"I can't run autoclean with the sklearn dev version, I getNot sure I understand this problem. Is it because that sklearn.externals.joblib is changed in the dev version?I don't think it's related to that but not entirely sure. I assume that whatever you pickled had a DeprecationDict in it, but that doesn't exist in sklearn any more.I referred to this post https://www.pythonanywhere.com/forums/topic/13053/ and I think the problem is related to GridSearchCV which I also used. I just updated sklearn to the latest version and retrained the meta-learner. Please let me know if this problem still exists. Thanks in advance.",None yet
https://github.com/hse-aml/intro-to-dl/issues/20,"Apparently, sklearn.cross_validation has been replaced with model_selection in newer versions.POS-task from week5 doesn't work in collab anymoreMaybe it's due to the deprecation of sklearn.cross_validation. Please replace sklearn.cross_validation with sklearn.model_selectionRef- amueller/scipy_2015_sklearn_tutorial#60",None yet
https://github.com/BUPTLdy/human-detector/issues/8,"I encountered with following error
C:\ProgramData\Anaconda3\envs\human-detector\python.exe ""D:/Campus/FYP/Implementations/Codes From Github/human_detection_using_hog/human-detector/object_detector/detector.py"" C:\ProgramData\Anaconda3\envs\human-detector\lib\site-packages\sklearn\base.py:253: UserWarning: Trying to unpickle estimator LinearSVC from version pre-0.18 when using version 0.20.4. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) C:\ProgramData\Anaconda3\envs\human-detector\lib\site-packages\skimage\feature\_hog.py:150: skimage_deprecation: Default value of block_norm==L1is deprecated and will be changed toL2-Hysin v0.15. To supress this message specify explicitly the normalization method. skimage_deprecation) Traceback (most recent call last): File ""D:/human_detection_using_hog/human-detector/object_detector/detector.py"", line 104, in <module> test_folder(foldername) File ""D:/human_detection_using_hog/human-detector/object_detector/detector.py"", line 100, in test_folder detector(filename) File ""D:/human_detection_using_hog/human-detector/object_detector/detector.py"", line 58, in detector pred = clf.predict(fd) File ""C:\ProgramData\Anaconda3\envs\human-detector\lib\site-packages\sklearn\linear_model\base.py"", line 281, in predict scores = self.decision_function(X) File ""C:\ProgramData\Anaconda3\envs\human-detector\lib\site-packages\sklearn\linear_model\base.py"", line 262, in decision_function % (X.shape[1], n_features)) ValueError: X has 100620 features per sample; expecting 6480
any guesses on how to solve this?Same problem. Do you solved it?",None yet
https://github.com/keras-team/keras/issues/8890,"Hey, just starting to try out Keras for some ML problems and I like it a lot so far, thanks!My model was having a terrible time fitting the training data, which I only fixed when I realized that I had followed an out-of-date tutorial and used nb_epoch instead of epochs. This was a problem because there's no feedback at all that nb_epoch doesn't do anything! A deprecation warning, or simply an exception at this point, would be quite useful.",None yet
https://github.com/annaformaniuk/smoke-detection/issues/1,"Hi Anna,
Many thanks for share this algorithm for smoke detection.
So i have installed all the required software and when i use the feature model:
python3.6 init.py
i not get any output
Using the combination model i get this errors/warnings(maybe video file not present):brunetti@brunetti:/media/brunetti/FLAVIO/smoke-detection-master/combination$ python3.6 init.py
/home/brunetti/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.svm.classes module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API.
warnings.warn(message, FutureWarning)
/home/brunetti/.local/lib/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LinearSVC from version 0.19.1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)
Traceback (most recent call last):
File ""init.py"", line 9, in
'../inputs/DJI_0843_small.mp4', 60)
File ""/media/brunetti/FLAVIO/smoke-detection-master/combination/detection.py"", line 170, in detect_smoke
return(next_frame, current_frame, overlapping_contours)
UnboundLocalError: local variable 'next_frame' referenced before assignmentPlease explain in more detail how use this useful software, in particular how change init.py in video streaming real time.",None yet
https://github.com/salivatec/SalivaPRINT/pull/1,,None yet
https://github.com/scikit-optimize/scikit-optimize/issues/928,"I am trying to understand how to use the BayesSearchCV, and tried the example provided, but a Python-BaseException was raised. I've used the exact code that was provided in the example.Here is the error message:Connected to pydev debugger (build 202.6397.94)
/Users/heather.wilson/Code/code_lab/venv/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass return_X_y=True as keyword args. From version 0.25 passing these as positional arguments will result in an error
FutureWarning)
/Users/heather.wilson/Code/code_lab/venv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:67: FutureWarning: Class MaskedArray is deprecated; MaskedArray is deprecated in version 0.23 and will be removed in version 0.25. Use numpy.ma.MaskedArray instead.
warnings.warn(msg, category=FutureWarning)
Traceback (most recent call last):
File ""/Users/heather.wilson/Library/Application Support/JetBrains/IntelliJIdea2020.2/plugins/python/helpers/pydev/pydevd.py"", line 1448, in _exec
pydev_imports.execfile(file, globals, locals) # execute the script
File ""/Users/heather.wilson/Library/Application Support/JetBrains/IntelliJIdea2020.2/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
exec(compile(contents+""\n"", file, 'exec'), glob, loc)
File ""/Users/heather.wilson/Code/code_lab/main.py"", line 27, in
main()
File ""/Users/heather.wilson/Code/code_lab/main.py"", line 22, in main
_ = opt.fit(X_train, y_train)
File ""/Users/heather.wilson/Code/code_lab/venv/lib/python3.7/site-packages/skopt/searchcv.py"", line 680, in fit
groups=groups, n_points=n_points_adjusted
File ""/Users/heather.wilson/Code/code_lab/venv/lib/python3.7/site-packages/skopt/searchcv.py"", line 566, in _step
self._fit(X, y, groups, params_dict)
File ""/Users/heather.wilson/Code/code_lab/venv/lib/python3.7/site-packages/skopt/searchcv.py"", line 479, in fit
param_results[""param%s"" % name][cand_i] = value
File ""/Users/heather.wilson/Code/code_lab/venv/lib/python3.7/site-packages/sklearn/utils/deprecation.py"", line 68, in wrapped
return init(*args, **kwargs)
TypeError: object.init() takes exactly one argument (the instance to initialize)
python-BaseExceptionIt seems to be an issue of scikit-optimize not being updated for the newest scikit-learn version. Many people have the same problem.",None yet
https://github.com/vzhou842/profanity-check/issues/10,"The library is generating lot of warnings (text below).sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.svm.classes module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API.warnings.warn(message, FutureWarning)sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearSVC from version 0.20.2 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.UserWarning): FutureWarning: The sklearn.preprocessing.label module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.warnings.warn(message, FutureWarning)/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.20.2 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.UserWarning)sklearn/base.py:318: UserWarning: Trying to unpickle estimator _SigmoidCalibration from version 0.20.2 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.UserWarning)sklearn/base.py:318: UserWarning: Trying to unpickle estimator CalibratedClassifierCV from version 0.20.2 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.UserWarning)@vzhou842 @vchulskiYeah the usage demo produces the following output, meaning I can't use this library :(How do I get rid of this? Would like to use this.Install scikit-learn==0.20.2 to solve the problem.The author really should have specified the exact version.It's building wheel (setup.py) endlessly when installing that version using pip3.",None yet
https://github.com/scikit-learn/scikit-learn/issues/12437,"Where estimators can be configured for metric, the point of providing a parameter p is that it should be possible to switch between Manhattan and Euclidian and other Minkowski distances just by modifying p (without changing metric). However this is not possible where metric='euclidean' by default. Rather, where p is available, metric should be 'minkowski' by default.We should change the default metric in OPTICS without deprecation as it has not been released. Elsewhere, we need to:which estimators are those?+1 to change the default in OPTICS (without deprecation cycle), especially considering that our euclidean is not robust.So we have:Hmmm.... @qinhanmin2014 has implied that this isn't the full story. Where sklearn.metrics.pairwise is used, unlike sklearn.neighbors, the implementation of minkowski(p=2) is not the same as the the implementation of euclidean.Should pairwise_distances be special-casing the minkowski(p=2) case as equivalent to euclidean (and similar for manhattan)? Perhaps we should only do this after we've solved open issues with the precision of euclidean_distances.I'll try to do this!Yes, the solution proposed by @jnothman in #12601 (review) would be most elegant, but we need to come up with a solution to Euclidean distance accuracy first.This is available for contributors to attempt, but it is not trivial.",Moderate help wanted
https://github.com/atavory/ibex/issues/33,"ibex version: 0.1.0
sklearn version 0.20.2When I run import ibex.sklearn.preprocessing, I getStrangely, however, import sklearn.preprocessing gives no warnings.Since the warnings mention Imputer and OneHotEncoder. I I also tried import them from pure sklearn, but still got no warnings.I guess the reason is that ibex imports some deprecated classes from sklearn. But I cannot figure out which part triggers those warnings, and why warnings only occur when they are imported by ibex.This may not affect the core functionality of ibex, except a bit annoying. But the auto-generating mechanism interests me so I hope to dig deeper into it anyway. Thanks.",None yet
https://github.com/scikit-learn/scikit-learn/issues/8182,"hmmlearn currently uses the deprecated log_multivariate_normal_density, and thus using hmmlearn results in a deprecation warning being emitted -- so far it's all fair.However, the warning actually gets repeated a very large number of times, annoyingly flooding the terminal, as can be seen e.g. by running the example https://github.com/hmmlearn/hmmlearn/blob/master/examples/plot_hmm_stock_analysis.py. I suspect this is likely due to sklearn calling warnings.simplefilter(""always"") at some point.Probably from sklearn/__init__.py:Is there no equivalent function you can use to get rid of the warnings? cc @tguillemot.We have the context manager and decorator ignore_warnings in sklearn.utils.testing designed for that.
You can use it in two different ways :orI think the context manager version is what you need.I wonder if we could add a message property to be sure we remove the Warning we want (as it's done in assert_warning for example). Any opinion @lesteve ?Why not simply initially add a ""default"" filter (once per location... should be more than enough) for Deprecations rather than an ""always"" one?I think the explanation is here #3478.I can understand using ""always"" for ConvergenceWarnings, but not for DeprecationWarnings.Indeed @anntzer I have read too fastly #3748.
In my point of view it's good to have always when we have to clean the deprecation function from the tests in sklearn. But as I don't really see the implication of changing always by default I prefer ask the point of view of someone else @agramfort @amueller @jnothman @lesteve ?You can silence warnings using the warnings.catch_warnings context manager, see https://docs.python.org/3.5/library/warnings.html#temporarily-suppressing-warnings for example. I have to say I am not that familiar with the scikit-learn utilities for this purpose. You could adopt a similar approach and wrap the deprecated function in hmmlearn if you find the warnings very irritating.As a maintainer I like 'always' I have to admit, this means there is more chance that the warning is acted on. As a user, I agree there are cases where it can be annoying.See e.g. the travis build log of hmmlearn: https://travis-ci.org/hmmlearn/hmmlearn/jobs/189174342. There are (literally) 10,000 lines of the same deprecation warning, causing travis to indicateAny reason you can not use numpy.random.multivariate_normal as mentioned in the deprecation message?It is not up to me but up to the maintainer of hmmlearn.
Yes, in that case, it would be a relatively trivial PR to make, but hmmlearn hasn't had a release for quite a while as well, so I don't know when the next one would be either.I assumed you were since you were pointing to the Travis log, sorry about the confusion.I would say open a PR in hmmlearn and see what happens.I'm using a library which uses sklearn internally and keep getting these DeprecationWarnings. Why is there no easy way to turn these off?!@timc13 there is:@amueller that doesn't seem to work no matter when I do it (before or after importing this lib) I believe because it's in sklearn/__init__.py and so there's no chance to introduce any other filters before sklearn code is run.if you doit should work.
Because then it's run after __init__.py.I think the problem is that I am using https://github.com/mailgun/talon which calls sklearn internally and no matter how I try to filter the warning, it will get raised in the talon library.It should really not, unless talon changes the registry.
Can you give a minimal example? Also, you should open an issue with talon. They shouldn't use deprecated behavior.I am having the same problem @timc13 mentioned with https://github.com/mailgun/talon, no matter what I tried, I don't seem able to suppress sklearn warnings. The inclusion of always in __init__.py is certainly an error:",None yet
https://github.com/skorch-dev/skorch/issues/674,"Looking at the code at NeuralNet.fit_loop-function:From what I understand, the max_epochs is actually used in a once-in-fit fashion. With early stopping and continued training however what I'd expect would be that the model is trained only for max_epochs regardless.In other words and my opinion, the training should not proceed further than what has been defined by max_epochs (i.e. net.history[-1].epoch should never exceed net.max_epochs).Thank you for raising this issue!I agree the current behavior of max_epochs is strange. It is not consistent with sklearn, for example:Would changing this behavior break something somewhere else?I'm thinking in the lines of making the iteration check against the history attribute for the last epoch number. But then again, it seems to be modified via an event listener and would be more on the side of ""implied and expected"" rather than clearly written out code.With that said, the fit_loop calls out the on_epoch_begin of the NeuralNet, which in turn assigns a new epoch.What are your thoughts on this?This change is technically possible.Since this is a backward incompatible change, we have to decide if it is worth a deprecation cycle.Yes, we have a few divergences here from sklearn. On the one hand, as you said, the epoch counter in the fit loop will always start from 0, even if you've already cycled through a few epochs. Relatedly, calling partial_fit also goes through max_epoch epochs, instead of just 1.I think in this case, we should think pragmatically. A few design choices in skorch diverge from sklearn because training a neural net often is a different beast than training a ""regular"" estimator.Personally, the most common use case I can think of where this behavior matters is when I'm still in an exploratory phase. Training the net for a few epochs, then cancelling with KeyboardInterrupt, checking stuff, then continue training. At that point, I would be annoyed if the net stopped training at some point because I ""exhausted"" my max_epochs (same goes for only fitting one epoch with partial_fit). However, this can be circumvented by increasing max_epochs, so it's not a big deal, just annoying.So before making a change and going through an expensive deprecation cycle, as Thomas mentioned, we should collect the most relevant use cases and determine how a change would affect them. In this specific case, I believe ergonomics should trump sklearn compatibility.As odd as it might sound to you guys, I'm actually not after sklearn-compatibility here. The idea of having a parameter named as max_epochs which is actually used in the manner of fit_epochs at best is what seems inconsistent to me. @BenjaminBossan I can see what you mean with exploration, but that would be using the system disregarding what the parameters seem to stand for at face value - you know how it works despite a bit misleading parameter names.But I really appreciate how you guys managed to get to this issue so fast! I hope you discuss this through and are able to determine what's best for the library w.r.t. this.",None yet
https://github.com/databricks/spark-sklearn/issues/53,"Using the current head 0.2.0 release of spark-sklearn and the current release of scikit-learn (0.18.1), I'm getting the following deprecation warning:/.../python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
""This module will be removed in 0.20."", DeprecationWarning)the library needs to be updated to use the new model_selection module and iterator interfaces.In addition, due to changes in sklearn.model_selection.GridSearchCV, the attributes available on the fitted spark-sklearn.GridSearchCV are out of date.sklearn.model_selection.GridSearchCV now has:While spark-sklearn.GridSearchCV has:The most critical difference is that sklearn added the more comprehensive cv_results_ which adds data that the formerly compatible grid_scores_ is lacking.I've been working on this and almost have a PR ready. It will be out this upcoming Monday.Thank you for the quick attention. Is anything required of me? I see I was CCed on the related issue, but it looks like that was just for info.An update making spark-sklearn compatible with sklearn version >= 0.18.1 has been merged.I'm just about to adopt this update. Can you mark a new release in github and and update the version in PyPi? I currently rely on pip for the installs in my environments. I was hoping not to have to change to git just for this package.@dsackin Did you end up doing the git install? I'm also running into version issues when installing through pip.No. I haven't updated yet. I was hoping they would push it into PyPi before I switched to git install.Got it. Just an fyi, ended up doing the git install, and it worked.Can you please let us know when a new release be marked and push to PyPi would happen.@gordontsai @dsackin I am quite new to git install...can you tell me how to perform git install while we wait this to be pushed pypi@thunterdb this is more about what it might take to support 0.20. We have a related issue about not setting things like best_params_ at #73, which seems like an easy fix but the simple fix doesn't run. This PR might also contain some of the necessary changes: #74 . This much I haven't looked into yet.I see, this is more than pointing to the right package. The 0.20 release is less than 2 months old, so let us focus on the 0.1x releases until there is a more general need for that. What are your thoughts?Yeah, certainly more concerned this second with a new release to fix some bugs, and maybe get random search in. If you have a sec to look at #73 you might know the quick answer; that might also be a quick fix relevant to 0.19",enhancement
https://github.com/viveksck/simplicity/issues/3,"Hi,
I've been having trouble using this repo, even just for replication or inference on a new phrase (using existing models). Some issues include:Something that can solve, hopefully, all of these is a requirements file or a yaml for conda envs.ok this was painful but i may have found a conda .yml env file that works:(kenlm has to be installed from source as per here).Ok! Let me know if this serves your needs. I will anyways plan to move this to Python 3.",None yet
https://github.com/MicrosoftResearch/Azimuth/issues/30,"Hi Author,
I am new to Python, so I know the problem is relate to the renaming and deprecation of cross_validation sub-module to model_selection.
But I don't know how can I fixed.
Look forward to your reply.
YaoI am running into the exact same problem. The only workaround I can see at the moment is to install sklearn version 0.16 or 0.17 which still have the cross_validation method. As far as I can see in the sklearn docs, it is not just a case of changing the name of the method in the azimuth code but the interface has changed too.
I also installed an older version of numpy into the virtualenv for azimuth: 1.12.1 because the latest version shows a deprecation warning ""DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release."" when used with the older sklearn module.
So in summary I installed this into my virtualenv:which works for now but of course it isn't great to be using outdated dependencies.",None yet
https://github.com/jonathf/chaospy/issues/267,"Hello,I hope it is okay that I am opening this issue with regard to chaospy as it seem like the TypeError is thrown by this library due to being called upon by uncertainpy.After finally having managed to install uncertainpy and all its requirements on windows I ran the following code:`if name == 'main':However, I get the following error:I tried to trace it back and understand the code to the very best of my ability. As far as I understood, the actual error is thrown in line 647 in create_PCE_collocation.
if (np.all(mask) or allow_incomplete) and sum(mask) > 0: U_hat[feature] = cp.fit_regression(P, masked_nodes, masked_evaluations, rule=""T"")
So the function fit_regression from the chaospy library is called.
It is defined as def fit_regression( polynomials, abscissas, evals, model=None, retall=False, ):
While the first parameters passed, P, masked_nodes, masked_evaluations, seem to correspond to the expected ones, I am not quite sure how and where the 'rule=""T""' argument comes into play. Furthermore, I could not find this TypeError within ""regression.py"" so I am not quite sure where this error is actually thrown.I just tried running the simulation with MC instead of CP and it worked fine.I have the following version of chaospy installed:I hope I have added all the required information and do apologize if not as this is my very first issue I have submitted.edit: actually, the more I think about it I should have rather opened this issue with regard to uncertainpy as it is that library who passes these arguments among which the faulty one is. Will still just wait for your answer before opening a second issue as you might still now a solution to this issue.This is indeed a Uncertainpy issue, but it comes from a deprecation in Chaospy.
The ""T"" is no more, as scikit-learn does regularization a lot better.
I've made a tutorial listing up how to use sklearn.linear_model together with chaospy here:
https://chaospy.readthedocs.io/en/master/tutorials/advanced/scikitlearn_regression.html
The ""T"" uses an L2-regularizaation, making it roughly equivalent to Ridge in the tutorial.@simetenn, do you want to make a patch to Uncertainpy, or should I?Thanks for the notice! I will take a look :)",None yet
https://github.com/seffnet/seffnet/issues/32,"I'm getting two warnings from sklearn that we should prepare to fix:the first warning is weird because we haven't used sklearn.linear_model.logistic, we are already using sklearn.linear_model to import models",None yet
https://github.com/r9y9/wavenet_vocoder/issues/204,"Hey, when i try to synthesis wav from mel files , this error occurs:python synthesis.py --conditional=./output_mel/0_mel.npy ./wavenet_premodel/20180510_mixture_lj_checkpoint_step000320000_ema.pth generated
/root/anaconda3/envs/keras/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.preprocessing.data module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.
warnings.warn(message, FutureWarning)
Using TensorFlow backend.
Command line args:
{'--conditional': './output_mel/0_mel.npy',
'--file-name-suffix': '',
'--help': False,
'--hparams': '',
'--initial-value': None,
'--length': '32000',
'--max-abs-value': '-1',
'--output-html': False,
'--preset': None,
'--speaker-id': None,
'--symmetric-mels': False,
'': './wavenet_premodel/20180510_mixture_lj_checkpoint_step000320000_ema.pth',
'<dst_dir>': 'generated'}
Load checkpoint from ./wavenet_premodel/20180510_mixture_lj_checkpoint_step000320000_ema.pth
0%| | 0/20480 [00:00<?, ?it/s]
Traceback (most recent call last):
File ""synthesis.py"", line 200, in
waveform = wavegen(model, length, c=c, g=speaker_id, initial_value=initial_value, fast=True)
File ""synthesis.py"", line 127, in wavegen
log_scale_min=hparams.log_scale_min)
File ""/root/AI_/wavenet_vocoder/wavenet_vocoder/wavenet.py"", line 335, in incremental_forward
x, h = f.incremental_forward(x, ct, gt)
File ""/root/AI_/wavenet_vocoder/wavenet_vocoder/modules.py"", line 135, in incremental_forward
return self.forward(x, c, g, True)
File ""/root/AI/wavenet_vocoder/wavenet_vocoder/modules.py"", line 165, in _forward
c = conv1x1_forward(self.conv1x1c, c, is_incremental)
File ""/root/AI/wavenet_vocoder/wavenet_vocoder/modules.py"", line 55, in conv1x1_forward
x = conv.incremental_forward(x)
File ""/root/AI/wavenet_vocoder/wavenet_vocoder/conv.py"", line 45, in incremental_forward
output = F.linear(input.view(bsz, -1), weight, self.bias)
File ""/root/anaconda3/envs/keras/lib/python3.6/site-packages/torch/nn/functional.py"", line 1370, in linear
ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [1 x 48], m2: [80 x 512] at /tmp/pip-req-build-808afw3c/aten/src/THC/generic/THCTensorMathBlas.cu:290The mel(.npy) files is the file generated from ""deepvpice3-pytorch"" project. I extract the mel-spec (don't use lws to synthesis wav in deepvoice3-pytorch) and want to synthesis with wavenet-vovoder. But this error occurs, can you help me this?The follow info may be helpful .ar = np.load(0_mel.npy)
ar.shape ==> (80,48)",None yet
https://github.com/erichson/ristretto/issues/39,"My environment is windows10 and install all package the project need.
Cython 0.29.20
numpy 1.19.0
scipy 1.5.1
scikit-learn 0.23.1
nose 1.3.7(rnmf) C:\Users\tsai\Desktop\ristretto-master>python setup.py test
running test
WARNING: Testing via this command is deprecated and will be removed in a future version. Users looking for a generic test entry point independent of test runner are encouraged to use tox.
running egg_info
writing ristretto.egg-info\PKG-INFO
writing dependency_links to ristretto.egg-info\dependency_links.txt
writing requirements to ristretto.egg-info\requires.txt
writing top-level names to ristretto.egg-info\top_level.txt
reading manifest file 'ristretto.egg-info\SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '.c' under directory 'ristretto'
warning: no files found matching '.pyx' under directory 'ristretto'
warning: no files found matching 'README.md'
writing manifest file 'ristretto.egg-info\SOURCES.txt'
running build_ext
C:\Users\tsai\anaconda3\envs\rnmf\lib\site-packages\sklearn\utils\deprecation.py:143: FutureWarning: The sklearn.decomposition.cdnmf_fast module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.
warnings.warn(message, FutureWarning)
C:\Users\tsai\anaconda3\envs\rnmf\lib\site-packages\sklearn\utils\deprecation.py:143: FutureWarning: The sklearn.decomposition.nmf module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.
warnings.warn(message, FutureWarning)
test_sketches.test_random_axis_sample ... ok
test_sketches.test_random_gaussian_map ... ok
test_sketches.test_sparse_random_map ... ok
test_transforms.test_randomized_uniform_sampling ... ok
test_transforms.test_johnson_linderstrauss ... ok
test_transforms.test_sparse_johnson_linderstrauss ... ok
test_transforms.test_fast_johnson_linderstrauss ... ok
test_utils.test_orthonormalize ... ok
test_utils.test_perform_subspace_iterations ... ok
ristretto.tests.test_cur.test_compute_cur ... C:\Users\tsai\Desktop\ristretto-master\ristretto\utils.py:12: DeprecationWarning: Converting np.complex to a dtype is deprecated. The current result is complex128 which is not strictly correct.
if A.dtype == np.complexfloating:
ok
ristretto.tests.test_cur.test_compute_rcur ... ok
ristretto.tests.test_dmd.test_dmd ... ok
ristretto.tests.test_dmd.test_compute_rdmd ... ok
ristretto.tests.test_dmd.test_DMD ... ok
ristretto.tests.test_dmd.test_RDMD ... ok
ristretto.tests.test_eigen.test_compute_reigh_float64 ... ok
ristretto.tests.test_eigen.test_compute_reigh_complex128 ... ok
ristretto.tests.test_eigen.test_reig_nystroem_float64 ... ok
ristretto.tests.test_eigen.test_reig_nystroem_complex128 ... ok
ristretto.tests.test_eigen.test_reig_nystroem_col_float64 ... ok
ristretto.tests.test_eigen.test_reig_nystroem_col_complex128 ... ok
ristretto.tests.test_interp_decomp.test_id_col ... ok
ristretto.tests.test_interp_decomp.test_id_row ... ok
ristretto.tests.test_interp_decomp.test_rid_col ... ok
ristretto.tests.test_interp_decomp.test_rid_row ... ok
ristretto.tests.test_lu.test_compute_rlu_float64 ... ok
ristretto.tests.test_lu.test_compute_rlu_complex128 ... ok
ristretto.tests.test_nmf.test_nmf_fhals ... ERROR
ristretto.tests.test_nmf.test_rnmf_fhals ... ERROR
ristretto.tests.test_pca.test_spca ... ok
ristretto.tests.test_pca.test_robspca ... ok
ristretto.tests.test_pca.test_rspca ... ok
ristretto.tests.test_qb.test_rqb_float64 ... ok
ristretto.tests.test_qb.test_rqb_complex128 ... ok
ristretto.tests.test_qb.test_rqb_block_float64 ... ok
ristretto.tests.test_qb.test_rqb_block_wide_float64 ... ok
ristretto.tests.test_qb.test_rqb_block_complex128 ... ok
ristretto.tests.test_qb.test_rqb_block_wide_complex128 ... ok
ristretto.tests.test_svd.test_compute_rsvd_float64 ... ok
ristretto.tests.test_svd.test_compute_rsvd_complex128 ... okTraceback (most recent call last):
File ""C:\Users\tsai\anaconda3\envs\rnmf\lib\site-packages\nose\case.py"", line 198, in runTest
self.test(*self.arg)
File ""C:\Users\tsai\Desktop\ristretto-master\ristretto\tests\test_nmf.py"", line 15, in test_nmf_fhals
W, H = compute_nmf(Anoisy, rank=10)
File ""C:\Users\tsai\Desktop\ristretto-master\ristretto\nmf.py"", line 154, in compute_nmf
violation = _update_cdnmf_fast(Ht, WtW, AtW, permutation)
File ""sklearn\decomposition_cdnmf_fast.pyx"", line 13, in sklearn.decomposition._cdnmf_fast._update_cdnmf_fast
ValueError: Buffer dtype mismatch, expected 'Py_ssize_t' but got 'long'Traceback (most recent call last):
File ""C:\Users\tsai\anaconda3\envs\rnmf\lib\site-packages\nose\case.py"", line 198, in runTest
self.test(*self.arg)
File ""C:\Users\tsai\Desktop\ristretto-master\ristretto\tests\test_nmf.py"", line 26, in test_rnmf_fhals
W, H = compute_rnmf(Anoisy, rank=10)
File ""C:\Users\tsai\Desktop\ristretto-master\ristretto\nmf.py"", line 335, in compute_rnmf
violation = _update_cdnmf_fast(Ht, WtW, BtW, permutation)
File ""sklearn\decomposition_cdnmf_fast.pyx"", line 13, in sklearn.decomposition._cdnmf_fast._update_cdnmf_fast
ValueError: Buffer dtype mismatch, expected 'Py_ssize_t' but got 'long'Ran 40 tests in 0.850sFAILED (errors=2)
Test failed: <unittest.runner.TextTestResult run=40 errors=2 failures=0>
error: Test failed: <unittest.runner.TextTestResult run=40 errors=2 failures=0>I debug the problem from this issue
https://stackoverflow.com/questions/61369007/expected-py-ssize-t-but-got-long
The variable permutation the type is Array of int32 is error
Using this code to change type to Array of int64:
permutation = permutation.astype(np.intp) under the permutation = random_state.permutation(rank) if shuffle else np.arange(rank)
It will work !
nmf is also.@tsaigaga thanks for spotting this. Do you like to push the fix to the package?Best,
ben",None yet
https://github.com/huanghe314/Text-Independent-Speaker-Indentification-System/issues/1,"when i'm trying to test the speaker it shows as
Do you wanna test Now? [Y/N] Y
Bus error: 10
and also after recording the voice it shows the warning as
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class GMM is deprecated; The class GMM is deprecated in 0.18 and will be removed in 0.20. Use class GaussianMixture instead.
warnings.warn(msg, category=DeprecationWarning)
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function distribute_covar_matrix_to_match_covariance_type is deprecated; The function distribute_covar_matrix_to_match_covariance_typeis deprecated in 0.18 and will be removed in 0.20.
warnings.warn(msg, category=DeprecationWarning)
could you please show me the way to sort these issues",None yet
https://github.com/WillKoehrsen/feature-selector/issues/36,"I've been trying to get this to install on my Mac for like 2hours, I got the install to work on a slightly legacy Python environment, but when I import in the env it throws this error:OSError Traceback (most recent call last)
in
----> 1 from feature_selector import FeatureSelector~/opt/anaconda3/envs/env_zipline/lib/python3.5/site-packages/feature_selector/init.py in
----> 1 from .feature_selector import FeatureSelector~/opt/anaconda3/envs/env_zipline/lib/python3.5/site-packages/feature_selector/feature_selector.py in
4
5 # model used for feature importances
----> 6 import lightgbm as lgb
7
8 # utility for early stopping with a validation set~/opt/anaconda3/envs/env_zipline/lib/python3.5/site-packages/lightgbm/init.py in
6 from future import absolute_import
7
----> 8 from .basic import Booster, Dataset
9 from .callback import (early_stopping, print_evaluation, record_evaluation,
10 reset_parameter)~/opt/anaconda3/envs/env_zipline/lib/python3.5/site-packages/lightgbm/basic.py in
30
31
---> 32 _LIB = _load_lib()
33
34~/opt/anaconda3/envs/env_zipline/lib/python3.5/site-packages/lightgbm/basic.py in _load_lib()
25 if len(lib_path) == 0:
26 return None
---> 27 lib = ctypes.cdll.LoadLibrary(lib_path[0])
28 lib.LGBM_GetLastError.restype = ctypes.c_char_p
29 return lib~/opt/anaconda3/envs/env_zipline/lib/python3.5/ctypes/init.py in LoadLibrary(self, name)
427
428 def LoadLibrary(self, name):
--> 429 return self._dlltype(name)
430
431 cdll = LibraryLoader(CDLL)~/opt/anaconda3/envs/env_zipline/lib/python3.5/ctypes/init.py in init(self, name, mode, handle, use_errno, use_last_error)
349
350 if handle is None:
--> 351 self._handle = _dlopen(self._name, mode)
352 else:
353 self._handle = handleOSError: dlopen(/Users/zoakes/opt/anaconda3/envs/env_zipline/lib/python3.5/site-packages/lightgbm/lib_lightgbm.so, 6): Library not loaded: /usr/local/opt/gcc/lib/gcc/7/libgomp.1.dylib
Referenced from: /Users/zoakes/opt/anaconda3/envs/env_zipline/lib/python3.5/site-packages/lightgbm/lib_lightgbm.so
Reason: image not foundIn regular Python 3.7 env it fails to build wheel in scikitlearn, or pandas -- I've upgraded both to as current as possible, tried downgrading them, nothing works. Will just not install man.(base) zacharys-mbp:~ zoakes$ pip install feature-selector
Collecting feature-selector
Using cached feature_selector-1.0.0-py3-none-any.whl (21 kB)
Processing ./Library/Caches/pip/wheels/26/0a/44/53ddd89769e62f7c6691976375b86c6492e7dd20a2d3970e32/seaborn-0.8.1-cp37-none-any.whl
Processing ./Library/Caches/pip/wheels/af/e7/a0/07e4882052774ad99ce5d91e8756d1210a8a0f5966dfb47a8f/pandas-0.23.1-cp37-cp37m-macosx_10_9_x86_64.whl
Requirement already satisfied: numpy==1.14.5 in ./opt/anaconda3/lib/python3.7/site-packages (from feature-selector) (1.14.5)
Processing ./Library/Caches/pip/wheels/83/9d/e6/cd76b09328833d683b447bc658dbd436bc9af0cd152c654407/matplotlib-2.1.2-cp37-cp37m-macosx_10_9_x86_64.whl
Collecting scikit-learn==0.19.1
Using cached scikit-learn-0.19.1.tar.gz (9.5 MB)
Collecting lightgbm==2.1.1
Using cached lightgbm-2.1.1-py2.py3-none-macosx_10_9_x86_64.macosx_10_10_x86_64.macosx_10_11_x86_64.macosx_10_12_x86_64.whl (689 kB)
Requirement already satisfied: python-dateutil>=2.5.0 in ./opt/anaconda3/lib/python3.7/site-packages (from pandas==0.23.1->feature-selector) (2.8.0)
Requirement already satisfied: pytz>=2011k in ./opt/anaconda3/lib/python3.7/site-packages (from pandas==0.23.1->feature-selector) (2019.3)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in ./opt/anaconda3/lib/python3.7/site-packages (from matplotlib==2.1.2->feature-selector) (2.4.2)
Requirement already satisfied: six>=1.10 in ./opt/anaconda3/lib/python3.7/site-packages (from matplotlib==2.1.2->feature-selector) (1.12.0)
Requirement already satisfied: cycler>=0.10 in ./opt/anaconda3/lib/python3.7/site-packages (from matplotlib==2.1.2->feature-selector) (0.10.0)
Requirement already satisfied: scipy in ./opt/anaconda3/lib/python3.7/site-packages (from lightgbm==2.1.1->feature-selector) (1.3.1)
Building wheels for collected packages: scikit-learn
Building wheel for scikit-learn (setup.py) ... error
ERROR: Command errored out with exit status 1:
command: /Users/zoakes/opt/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-install-oiint0vb/scikit-learn/setup.py'""'""'; file='""'""'/private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-install-oiint0vb/scikit-learn/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(file);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, file, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-wheel-ueypoqgw
cwd: /private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-install-oiint0vb/scikit-learn/
Complete output (752 lines):
Partial import of sklearn during the build process.
blas_opt_info:
blas_mkl_info:
customize UnixCCompiler
FOUND:
libraries = ['mkl_rt', 'pthread']
library_dirs = ['/Users/zoakes/opt/anaconda3/lib']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
include_dirs = ['/usr/local/include', '/Users/zoakes/opt/anaconda3/include']running bdist_wheel
running build
running config_cc
unifing config_cc, config, build_clib, build_ext, build commands --compiler options
running config_fc
unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
running build_src
build_src
building library ""libsvm-skl"" sources
building extension ""sklearn.__check_build._check_build"" sources
building extension ""sklearn.cluster._dbscan_inner"" sources
building extension ""sklearn.cluster._hierarchical"" sources
building extension ""sklearn.cluster._k_means_elkan"" sources
building extension ""sklearn.cluster._k_means"" sources
building extension ""sklearn.datasets._svmlight_format"" sources
building extension ""sklearn.decomposition._online_lda"" sources
building extension ""sklearn.decomposition.cdnmf_fast"" sources
building extension ""sklearn.ensemble._gradient_boosting"" sources
building extension ""sklearn.feature_extraction._hashing"" sources
building extension ""sklearn.manifold._utils"" sources
building extension ""sklearn.manifold._barnes_hut_tsne"" sources
building extension ""sklearn.metrics.pairwise_fast"" sources
building extension ""sklearn.metrics/cluster.expected_mutual_info_fast"" sources
building extension ""sklearn.neighbors.ball_tree"" sources
building extension ""sklearn.neighbors.kd_tree"" sources
building extension ""sklearn.neighbors.dist_metrics"" sources
building extension ""sklearn.neighbors.typedefs"" sources
building extension ""sklearn.neighbors.quad_tree"" sources
building extension ""sklearn.tree._tree"" sources
building extension ""sklearn.tree._splitter"" sources
building extension ""sklearn.tree._criterion"" sources
building extension ""sklearn.tree._utils"" sources
building extension ""sklearn.svm.libsvm"" sources
building extension ""sklearn.svm.liblinear"" sources
building extension ""sklearn.svm.libsvm_sparse"" sources
building extension ""sklearn._isotonic"" sources
building extension ""sklearn.linear_model.cd_fast"" sources
building extension ""sklearn.linear_model.sgd_fast"" sources
building extension ""sklearn.linear_model.sag_fast"" sources
building extension ""sklearn.utils.sparsefuncs_fast"" sources
building extension ""sklearn.utils.arrayfuncs"" sources
building extension ""sklearn.utils.murmurhash"" sources
building extension ""sklearn.utils.lgamma"" sources
building extension ""sklearn.utils.graph_shortest_path"" sources
building extension ""sklearn.utils.fast_dict"" sources
building extension ""sklearn.utils.seq_dataset"" sources
building extension ""sklearn.utils.weight_vector"" sources
building extension ""sklearn.utils._random"" sources
building extension ""sklearn.utils._logistic_sigmoid"" sources
building data_files sources
build_src: building npy-pkg config files
running build_py
creating build
creating build/lib.macosx-10.9-x86_64-3.7
creating build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/learning_curve.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/multiclass.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/kernel_approximation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/random_projection.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/isotonic.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/multioutput.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/kernel_ridge.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/naive_bayes.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/pipeline.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/grid_search.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/discriminant_analysis.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/exceptions.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/dummy.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/calibration.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
copying sklearn/cross_validation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/__check_build
copying sklearn/__check_build/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/__check_build
copying sklearn/_check_build/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/check_build
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/build_utils
copying sklearn/build_utils/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/build_utils
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance
copying sklearn/covariance/graph_lasso.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance
copying sklearn/covariance/robust_covariance.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance
copying sklearn/covariance/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance
copying sklearn/covariance/empirical_covariance.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance
copying sklearn/covariance/outlier_detection.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance
copying sklearn/covariance/shrunk_covariance.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance/tests
copying sklearn/covariance/tests/test_graph_lasso.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance/tests
copying sklearn/covariance/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance/tests
copying sklearn/covariance/tests/test_covariance.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance/tests
copying sklearn/covariance/tests/test_robust_covariance.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/covariance/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/cross_decomposition
copying sklearn/cross_decomposition/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cross_decomposition
copying sklearn/cross_decomposition/cca.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cross_decomposition
copying sklearn/cross_decomposition/pls.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cross_decomposition
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/cross_decomposition/tests
copying sklearn/cross_decomposition/tests/test_pls.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cross_decomposition/tests
copying sklearn/cross_decomposition/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cross_decomposition/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection
copying sklearn/feature_selection/rfe.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection
copying sklearn/feature_selection/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection
copying sklearn/feature_selection/variance_threshold.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection
copying sklearn/feature_selection/univariate_selection.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection
copying sklearn/feature_selection/mutual_info.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection
copying sklearn/feature_selection/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection
copying sklearn/feature_selection/from_model.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection/tests
copying sklearn/feature_selection/tests/test_variance_threshold.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection/tests
copying sklearn/feature_selection/tests/test_rfe.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection/tests
copying sklearn/feature_selection/tests/test_from_model.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection/tests
copying sklearn/feature_selection/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection/tests
copying sklearn/feature_selection/tests/test_feature_select.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection/tests
copying sklearn/feature_selection/tests/test_chi2.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection/tests
copying sklearn/feature_selection/tests/test_base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection/tests
copying sklearn/feature_selection/tests/test_mutual_info.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_selection/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process
copying sklearn/gaussian_process/gaussian_process.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process
copying sklearn/gaussian_process/gpr.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process
copying sklearn/gaussian_process/correlation_models.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process
copying sklearn/gaussian_process/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process
copying sklearn/gaussian_process/regression_models.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process
copying sklearn/gaussian_process/gpc.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process
copying sklearn/gaussian_process/kernels.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process/tests
copying sklearn/gaussian_process/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process/tests
copying sklearn/gaussian_process/tests/test_gpr.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process/tests
copying sklearn/gaussian_process/tests/test_kernels.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process/tests
copying sklearn/gaussian_process/tests/test_gpc.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process/tests
copying sklearn/gaussian_process/tests/test_gaussian_process.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/gaussian_process/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture
copying sklearn/mixture/gmm.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture
copying sklearn/mixture/dpgmm.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture
copying sklearn/mixture/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture
copying sklearn/mixture/bayesian_mixture.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture
copying sklearn/mixture/gaussian_mixture.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture
copying sklearn/mixture/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture/tests
copying sklearn/mixture/tests/test_bayesian_mixture.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture/tests
copying sklearn/mixture/tests/test_gmm.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture/tests
copying sklearn/mixture/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture/tests
copying sklearn/mixture/tests/test_gaussian_mixture.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture/tests
copying sklearn/mixture/tests/test_dpgmm.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/mixture/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection
copying sklearn/model_selection/_search.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection
copying sklearn/model_selection/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection
copying sklearn/model_selection/_validation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection
copying sklearn/model_selection/split.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection/tests
copying sklearn/model_selection/tests/test_split.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection/tests
copying sklearn/model_selection/tests/test_validation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection/tests
copying sklearn/model_selection/tests/test_search.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection/tests
copying sklearn/model_selection/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection/tests
copying sklearn/model_selection/tests/common.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/model_selection/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network
copying sklearn/neural_network/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network
copying sklearn/neural_network/multilayer_perceptron.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network
copying sklearn/neural_network/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network
copying sklearn/neural_network/stochastic_optimizers.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network
copying sklearn/neural_network/rbm.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network/tests
copying sklearn/neural_network/tests/test_mlp.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network/tests
copying sklearn/neural_network/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network/tests
copying sklearn/neural_network/tests/test_rbm.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network/tests
copying sklearn/neural_network/tests/test_stochastic_optimizers.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neural_network/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing
copying sklearn/preprocessing/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing
copying sklearn/preprocessing/imputation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing
copying sklearn/preprocessing/label.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing
copying sklearn/preprocessing/data.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing
copying sklearn/preprocessing/function_transformer.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing/tests
copying sklearn/preprocessing/tests/test_label.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing/tests
copying sklearn/preprocessing/tests/test_function_transformer.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing/tests
copying sklearn/preprocessing/tests/test_imputation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing/tests
copying sklearn/preprocessing/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing/tests
copying sklearn/preprocessing/tests/test_data.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/preprocessing/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/semi_supervised
copying sklearn/semi_supervised/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/semi_supervised
copying sklearn/semi_supervised/label_propagation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/semi_supervised
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/semi_supervised/tests
copying sklearn/semi_supervised/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/semi_supervised/tests
copying sklearn/semi_supervised/tests/test_label_propagation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/semi_supervised/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/bicluster.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/mean_shift.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/hierarchical.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/feature_agglomeration.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/k_means.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/affinity_propagation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/spectral.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/birch.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
copying sklearn/cluster/dbscan.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/test_mean_shift.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/test_k_means.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/test_dbscan.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/test_birch.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/test_affinity_propagation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/test_hierarchical.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/common.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/test_bicluster.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
copying sklearn/cluster/tests/test_spectral.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/cluster/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/kddcup99.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/olivetti_faces.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/svmlight_format.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/samples_generator.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/mlcomp.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/covtype.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/lfw.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/rcv1.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/twenty_newsgroups.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/mldata.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/species_distributions.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/california_housing.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
copying sklearn/datasets/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/test_lfw.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/test_mldata.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/test_rcv1.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/test_kddcup99.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/test_covtype.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/test_svmlight_format.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/test_20news.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/test_samples_generator.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
copying sklearn/datasets/tests/test_base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/datasets/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/dict_learning.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/factor_analysis.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/kernel_pca.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/online_lda.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/fastica.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/incremental_pca.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/sparse_pca.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/truncated_svd.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/pca.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/nmf.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
copying sklearn/decomposition/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_dict_learning.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_fastica.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_truncated_svd.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_nmf.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_online_lda.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_kernel_pca.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_sparse_pca.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_factor_analysis.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_incremental_pca.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
copying sklearn/decomposition/tests/test_pca.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/decomposition/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/bagging.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/gradient_boosting.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/voting_classifier.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/weight_boosting.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/forest.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/partial_dependence.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
copying sklearn/ensemble/iforest.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/test_partial_dependence.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/test_forest.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/test_bagging.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/test_weight_boosting.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/test_gradient_boosting.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/test_iforest.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/test_base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
copying sklearn/ensemble/tests/test_voting_classifier.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/ensemble/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/externals
copying sklearn/externals/test_externals_setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals
copying sklearn/externals/funcsigs.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals
copying sklearn/externals/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals
copying sklearn/externals/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals
copying sklearn/externals/six.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/disk.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/_memory_helpers.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/format_stack.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/memory.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/func_inspect.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/logger.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/numpy_pickle_compat.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/_multiprocessing_helpers.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/numpy_pickle_utils.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/backports.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/numpy_pickle.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/_compat.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/my_exceptions.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/parallel.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/pool.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/parallel_backends.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
copying sklearn/externals/joblib/hashing.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/externals/joblib
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction
copying sklearn/feature_extraction/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction
copying sklearn/feature_extraction/dict_vectorizer.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction
copying sklearn/feature_extraction/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction
copying sklearn/feature_extraction/text.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction
copying sklearn/feature_extraction/hashing.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction
copying sklearn/feature_extraction/image.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction
copying sklearn/feature_extraction/stop_words.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction/tests
copying sklearn/feature_extraction/tests/test_dict_vectorizer.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction/tests
copying sklearn/feature_extraction/tests/test_image.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction/tests
copying sklearn/feature_extraction/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction/tests
copying sklearn/feature_extraction/tests/test_text.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction/tests
copying sklearn/feature_extraction/tests/test_feature_hasher.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/feature_extraction/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold
copying sklearn/manifold/locally_linear.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold
copying sklearn/manifold/t_sne.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold
copying sklearn/manifold/mds.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold
copying sklearn/manifold/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold
copying sklearn/manifold/isomap.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold
copying sklearn/manifold/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold
copying sklearn/manifold/spectral_embedding.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold/tests
copying sklearn/manifold/tests/test_isomap.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold/tests
copying sklearn/manifold/tests/test_mds.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold/tests
copying sklearn/manifold/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold/tests
copying sklearn/manifold/tests/test_t_sne.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold/tests
copying sklearn/manifold/tests/test_spectral_embedding.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold/tests
copying sklearn/manifold/tests/test_locally_linear.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/manifold/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics
copying sklearn/metrics/regression.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics
copying sklearn/metrics/classification.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics
copying sklearn/metrics/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics
copying sklearn/metrics/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics
copying sklearn/metrics/pairwise.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics
copying sklearn/metrics/ranking.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics
copying sklearn/metrics/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics
copying sklearn/metrics/scorer.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/tests
copying sklearn/metrics/tests/test_common.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/tests
copying sklearn/metrics/tests/test_score_objects.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/tests
copying sklearn/metrics/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/tests
copying sklearn/metrics/tests/test_ranking.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/tests
copying sklearn/metrics/tests/test_pairwise.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/tests
copying sklearn/metrics/tests/test_classification.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/tests
copying sklearn/metrics/tests/test_regression.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster
copying sklearn/metrics/cluster/bicluster.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster
copying sklearn/metrics/cluster/unsupervised.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster
copying sklearn/metrics/cluster/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster
copying sklearn/metrics/cluster/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster
copying sklearn/metrics/cluster/supervised.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster/tests
copying sklearn/metrics/cluster/tests/test_supervised.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster/tests
copying sklearn/metrics/cluster/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster/tests
copying sklearn/metrics/cluster/tests/test_bicluster.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster/tests
copying sklearn/metrics/cluster/tests/test_unsupervised.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/metrics/cluster/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/lof.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/regression.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/approximate.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/unsupervised.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/classification.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/graph.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/kde.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/nearest_centroid.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
copying sklearn/neighbors/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/test_nearest_centroid.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/test_kde.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/test_dist_metrics.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/test_lof.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/test_kd_tree.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/test_approximate.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/test_neighbors.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/test_quad_tree.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
copying sklearn/neighbors/tests/test_ball_tree.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/neighbors/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/tree
copying sklearn/tree/tree.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tree
copying sklearn/tree/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tree
copying sklearn/tree/export.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tree
copying sklearn/tree/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tree
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/tree/tests
copying sklearn/tree/tests/test_tree.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tree/tests
copying sklearn/tree/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tree/tests
copying sklearn/tree/tests/test_export.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tree/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/svm
copying sklearn/svm/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/svm
copying sklearn/svm/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/svm
copying sklearn/svm/classes.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/svm
copying sklearn/svm/bounds.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/svm
copying sklearn/svm/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/svm
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/svm/tests
copying sklearn/svm/tests/test_sparse.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/svm/tests
copying sklearn/svm/tests/test_bounds.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/svm/tests
copying sklearn/svm/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/svm/tests
copying sklearn/svm/tests/test_svm.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/svm/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/ransac.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/perceptron.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/least_angle.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/logistic.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/coordinate_descent.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/randomized_l1.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/sag.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/bayes.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/omp.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/passive_aggressive.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/stochastic_gradient.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/huber.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/ridge.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/theil_sen.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
copying sklearn/linear_model/base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_logistic.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_sgd.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_huber.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_bayes.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_theil_sen.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_ridge.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_passive_aggressive.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_randomized_l1.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_least_angle.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_ransac.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_coordinate_descent.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_sparse_coordinate_descent.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_perceptron.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_sag.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
copying sklearn/linear_model/tests/test_omp.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/linear_model/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/optimize.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/fixes.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/deprecation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/estimator_checks.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/multiclass.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/graph.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/random.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/bench.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/scipy_sparse_lsqr_backport.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/mocking.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/stats.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/class_weight.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/metaestimators.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/extmath.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/testing.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/arpack.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/sparsefuncs.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/linear_assignment.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
copying sklearn/utils/validation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/sparsetools
copying sklearn/utils/sparsetools/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/sparsetools
copying sklearn/utils/sparsetools/setup.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/sparsetools
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/sparsetools/tests
copying sklearn/utils/sparsetools/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/sparsetools/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_deprecation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_bench.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_utils.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_metaestimators.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_seq_dataset.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_stats.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_validation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_optimize.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_shortest_path.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_fast_dict.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_class_weight.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_estimator_checks.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_multiclass.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_fixes.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_sparsefuncs.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_murmurhash.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_linear_assignment.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_graph.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_extmath.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_random.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
copying sklearn/utils/tests/test_testing.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/utils/tests
creating build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_metaestimators.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_common.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_multioutput.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_isotonic.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_cross_validation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_docstring_parameters.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_dummy.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_check_build.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_discriminant_analysis.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_multiclass.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_config.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_kernel_ridge.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_calibration.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_naive_bayes.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_learning_curve.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_base.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_init.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_grid_search.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_pipeline.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_random_projection.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
copying sklearn/tests/test_kernel_approximation.py -> build/lib.macosx-10.9-x86_64-3.7/sklearn/tests
running build_clib
customize UnixCCompiler
customize UnixCCompiler using build_clib
building 'libsvm-skl' library
compiling C++ sources
C compiler: g++ -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/zoakes/opt/anaconda3/include -arch x86_64 -I/Users/zoakes/opt/anaconda3/include -arch x86_64creating build/temp.macosx-10.9-x86_64-3.7
creating build/temp.macosx-10.9-x86_64-3.7/sklearn
creating build/temp.macosx-10.9-x86_64-3.7/sklearn/svm
creating build/temp.macosx-10.9-x86_64-3.7/sklearn/svm/src
creating build/temp.macosx-10.9-x86_64-3.7/sklearn/svm/src/libsvm
compile options: '-I/Users/zoakes/opt/anaconda3/lib/python3.7/site-packages/numpy/core/include -c'
g++: sklearn/svm/src/libsvm/libsvm_template.cpp
ar: adding 1 object files to build/temp.macosx-10.9-x86_64-3.7/liblibsvm-skl.a
ranlib:@ build/temp.macosx-10.9-x86_64-3.7/liblibsvm-skl.a
running build_ext
customize UnixCCompiler
customize UnixCCompiler using build_ext
customize UnixCCompiler
customize UnixCCompiler using build_ext
building 'sklearn.__check_build._check_build' extension
compiling C sources
C compiler: gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/zoakes/opt/anaconda3/include -arch x86_64 -I/Users/zoakes/opt/anaconda3/include -arch x86_64creating build/temp.macosx-10.9-x86_64-3.7/sklearn/__check_build
compile options: '-I/Users/zoakes/opt/anaconda3/lib/python3.7/site-packages/numpy/core/include -I/Users/zoakes/opt/anaconda3/lib/python3.7/site-packages/numpy/core/include -I/Users/zoakes/opt/anaconda3/include/python3.7m -c'
gcc: sklearn/__check_build/_check_build.c
gcc -bundle -undefined dynamic_lookup -L/Users/zoakes/opt/anaconda3/lib -arch x86_64 -L/Users/zoakes/opt/anaconda3/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.9-x86_64-3.7/sklearn/__check_build/_check_build.o -Lbuild/temp.macosx-10.9-x86_64-3.7 -o build/lib.macosx-10.9-x86_64-3.7/sklearn/__check_build/_check_build.cpython-37m-darwin.so
building 'sklearn.cluster._dbscan_inner' extension
compiling C++ sources
C compiler: g++ -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/zoakes/opt/anaconda3/include -arch x86_64 -I/Users/zoakes/opt/anaconda3/include -arch x86_64ERROR: Failed building wheel for scikit-learn
Running setup.py clean for scikit-learn
Failed to build scikit-learn
ERROR: yfinance 0.1.50 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.
ERROR: yfinance 0.1.50 has requirement pandas>=0.24, but you'll have pandas 0.23.1 which is incompatible.
Installing collected packages: seaborn, pandas, matplotlib, scikit-learn, lightgbm, feature-selector
Attempting uninstall: seaborn
Found existing installation: seaborn 0.10.0
Uninstalling seaborn-0.10.0:
Successfully uninstalled seaborn-0.10.0
Attempting uninstall: pandas
Found existing installation: pandas 0.25.3
Uninstalling pandas-0.25.3:
Successfully uninstalled pandas-0.25.3
Attempting uninstall: matplotlib
Found existing installation: matplotlib 3.1.1
Uninstalling matplotlib-3.1.1:
Successfully uninstalled matplotlib-3.1.1
Attempting uninstall: scikit-learn
Found existing installation: scikit-learn 0.22.1
Uninstalling scikit-learn-0.22.1:
Successfully uninstalled scikit-learn-0.22.1
Running setup.py install for scikit-learn ... error
ERROR: Command errored out with exit status 1:
command: /Users/zoakes/opt/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-install-oiint0vb/scikit-learn/setup.py'""'""'; file='""'""'/private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-install-oiint0vb/scikit-learn/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(file);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, file, '""'""'exec'""'""'))' install --record /private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-record-etkoh_nb/install-record.txt --single-version-externally-managed --compile --install-headers /Users/zoakes/opt/anaconda3/include/python3.7m/scikit-learn
cwd: /private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-install-oiint0vb/scikit-learn/
Complete output (752 lines):
Partial import of sklearn during the build process.
blas_opt_info:
blas_mkl_info:
customize UnixCCompiler
FOUND:
libraries = ['mkl_rt', 'pthread']
library_dirs = ['/Users/zoakes/opt/anaconda3/lib']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
include_dirs = ['/usr/local/include', '/Users/zoakes/opt/anaconda3/include']Rolling back uninstall of scikit-learn
Moving to /Users/zoakes/opt/anaconda3/lib/python3.7/site-packages/scikit_learn-0.22.1.dist-info/
from /Users/zoakes/opt/anaconda3/lib/python3.7/site-packages/~cikit_learn-0.22.1.dist-info
Moving to /Users/zoakes/opt/anaconda3/lib/python3.7/site-packages/sklearn/
from /Users/zoakes/opt/anaconda3/lib/python3.7/site-packages/~klearn
ERROR: Command errored out with exit status 1: /Users/zoakes/opt/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-install-oiint0vb/scikit-learn/setup.py'""'""'; file='""'""'/private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-install-oiint0vb/scikit-learn/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(file);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, file, '""'""'exec'""'""'))' install --record /private/var/folders/qv/4jt_sly54s3g24j4x0qj6xx00000gn/T/pip-record-etkoh_nb/install-record.txt --single-version-externally-managed --compile --install-headers /Users/zoakes/opt/anaconda3/include/python3.7m/scikit-learn Check the logs for full command output.+1I was able to install directly from github:
pip install git+https://github.com/WillKoehrsen/feature-selector.gitit works,thx!",None yet
https://github.com/ogrisel/parallel_ml_tutorial/issues/6,"Code:Gives the warning:DeprecationWarning)The error is repeated many times. The figure does load properly despite these warnings.I also faced an error while running this block.
This error was along the lines of a list not having a shape attribute.Fix:
p = clf.decision_function(np.array([x1, x2]).reshape(1, -1))",None yet
https://github.com/automl/Auto-PyTorch/issues/65,"I was able to run the classification.py but when running regression.py, the below error message showed up.well, settingtoand it is runnable now (use the same argument as in classification.py).",None yet
https://github.com/deepwel/Chinese-Annotator/issues/80,,None yet
https://github.com/scikit-learn/scikit-learn/pull/14523,"Closes #14444Respects fit_prior and class_prior parameters when computing joint log-likelihood.Complement NB (original)Training:
ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)
train time: 0.136s
test time: 0.028s
accuracy: 0.832
dimensionality: 130107
density: 1.000000
classification report: precision recall f1-score supportComplement NB (priors)Training:
ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)
train time: 0.129s
test time: 0.026s
accuracy: 0.804
dimensionality: 130107
density: 1.000000
classification report: precision recall f1-score supportThis is a breaking change, as currently formulated. Not using priors accords with the documentation, and some popular use of CNB, so we can't call it a bug fix. We need to add an option to use the priors, and possibly change the default after a deprecation period.Hi @ghost are you still interested in working on this PR? Thanks!Hi @cmarmo and @jnothman, I'm interested in working on this PR as first timer in the open source world.
Could you (@jnothman) explain me what you mean exactly by ""We need to add an option to use the priors, and possibly change the default after a deprecation period"".Cheers@yacth The current PR creates a breaking change, and therefore we cannot release it under our backwards compatibility policy.Instead, we need to make the inclusion of priors an option rather than a forced change in behaviour. Once we have added an option, we can perhaps deprecate the current behaviour to make the use of priors the default behaviour. That deprecation can be considered as a separate contribution.",Stalled help wanted module:naive_bayes
https://github.com/scikit-learn/scikit-learn/pull/18387,Partially addresses #15761added the default value for the parameter 'extra'One minor suggestion.Thank you for the PR @haiatn !,Documentation module:utils
https://github.com/Teichlab/cellphonedb/issues/205,"Hi,
When I ran CellPhoneDB on my scRNA data, I got the following error messages on some samples.[ ][APP][10/09/20-22:19:17][ERROR] Unexpected error
/nas/longleaf/apps/python/3.6.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
warnings.warn(message, FutureWarning)
Traceback (most recent call last):
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3078, in get_loc
return self._engine.get_loc(key)
File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc
File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc
File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'ENSG00000182985'During handling of the above exception, another exception occurred:Traceback (most recent call last):
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/cellphonedb/src/api_endpoints/terminal_api/method_terminal_api_endpoints/method_terminal_commands.py"", line 144, in statistical_analysis
subsampler,
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/cellphonedb/src/local_launchers/local_method_launcher.py"", line 64, in cpdb_statistical_analysis_local_method_launcher
subsampler
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/cellphonedb/src/core/methods/method_launcher.py"", line 75, in cpdb_statistical_analysis_launcher
self.separator)
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/cellphonedb/src/core/methods/cpdb_statistical_analysis_method.py"", line 35, in call
result_precision,
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/cellphonedb/src/core/methods/cpdb_statistical_analysis_simple_method.py"", line 59, in call
counts_data=counts_data)
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/cellphonedb/src/core/methods/cpdb_statistical_analysis_helper.py"", line 246, in percent_analysis
counts_data=counts_data)
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/cellphonedb/src/core/methods/cpdb_statistical_analysis_helper.py"", line 413, in cluster_interaction_percent
percent_receptor = percent_cluster_receptors[interaction['{}{}'.format(counts_data, suffixes[0])]]
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in getitem
return self._getitem_column(key)
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column
return self._get_item_cache(key)
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache
values = self._data.get(item)
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get
loc = self.items.get_loc(item)
File ""/nas/longleaf/apps/cellphonedb/2.1.4/cpdb/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc
return self._engine.get_loc(self._maybe_cast_indexer(key))
File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc
File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc
File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'ENSG00000182985'Does anyone know any solutions to this? Thanks!Hi,
I had likely the same error (for me it read KeyError: 'KLRG2') on some samples, although KLRG2 is present in the count files. turns out I had some NA values in the cell_type column, after removing them the error wasn't raised again.",None yet
https://github.com/HectorAnadon/Face-expression-and-ethnic-recognition/issues/11,"Thank you very much for providing this great code. I am trying to run "" python test_emotions.py -i"", however, I get the following error. Could you please help solve the code so make it up-to-date?/Users/yuyang01/miniconda3/envs/py37/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.svm.classes module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API. warnings.warn(message, FutureWarning) /Users/yuyang01/miniconda3/envs/py37/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator SVC from version 0.18.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) Traceback (most recent call last): File ""test_emotions.py"", line 53, in <module> result = predict_emotion(args[""image""]) File ""test_emotions.py"", line 45, in predict_emotion return clf.predict(np.array([poi])) File ""/Users/yuyang01/miniconda3/envs/py37/lib/python3.7/site-packages/sklearn/svm/_base.py"", line 585, in predict if self.break_ties and self.decision_function_shape == 'ovo': AttributeError: 'SVC' object has no attribute 'break_ties'It appears that it is a version mismatch. I have tried with scikit_learn==0.19.0. Try it with that version.",None yet
https://github.com/xeneta/LeadQualifier/pull/11,Removed n_iter in favor of the new max_iter. Performance and accuracy is consistent. The change was because n_iter is being deprecated and will be removed in sklearn 0.21 according the the deprecation warning.,None yet
https://github.com/aylliote/senti-py/issues/14,"Hi, first of all just wanted to say that I really liked your job on this project, it's seems like something really useful and well done.I just have one issue, whenever I try to import it, it tells meand I was wondering if there was any way someone could help me with this.Thanks in advance!Hi, the dependencies of package spanish_sentiment_analysis are not specified, then if you install it using pip, the last version of scikit-learn will be installed what is wrong. You must use an older compatible version of scikit-learn. Use scikit-learn 0.19.2 works for me.These are my steps to make it works in python 3.8:These are my dependencies exact versions:That worked like a charm, thank you very much!",None yet
https://github.com/yihui-he/channel-pruning/issues/124,"hello,I'm learning from your channel-pruning-master. I have a problem running the problem.Do you have any solutions? Here are my problem:
/home/linux/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/externals/joblib/init.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will beremoved in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models,you may need to re-serialize those models with scikit-learn 0.21+.
warnings.warn(msg, category=FutureWarning)
/home/linux/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is deprecated in version0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported fromsklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)
no lighting pack
using CPU caffe
[libprotobuf INFO google/protobuf/io/coded_stream.cc:610] Reading dangerously large protocol message. If the message turns out to be larger than 2147483647 bytes, parsingwill be halted for security reasons. To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 553432081temp/bn_vgg.prototxt
using CPU caffe
including last conv layer!
run for 100 batches nFeatsPerBatch 100
Extracting conv1_1 (10000, 64)
Extracting conv1_2_V (10000, 22)
Extracting conv1_2_H (10000, 22)
Extracting conv1_2_P (10000, 59)
Extracting conv2_1_V (10000, 37)
Extracting conv2_1_H (10000, 37)
Extracting conv2_1_P (10000, 118)
Extracting conv2_2_V (10000, 47)
Extracting conv2_2_H (10000, 47)
Extracting conv2_2_P (10000, 119)
Extracting conv3_1_V (10000, 83)
Extracting conv3_1_H (10000, 83)
Extracting conv3_1_P (10000, 226)
Extracting conv3_2_V (10000, 89)
Extracting conv3_2_H (10000, 89)
Extracting conv3_2_P (10000, 243)
Extracting conv3_3_V (10000, 106)
Extracting conv3_3_H (10000, 106)
Extracting conv3_3_P (10000, 256)
Extracting conv4_1_V (10000, 175)
Extracting conv4_1_H (10000, 175)
Extracting conv4_1_P (10000, 482)
Extracting conv4_2_V (10000, 192)
Extracting conv4_2_H (10000, 192)
Extracting conv4_2_P (10000, 457)
Extracting conv4_3_V (10000, 227)
Extracting conv4_3_H (10000, 227)
Extracting conv4_3_P (10000, 512)
Extracting conv5_1_V (10000, 398)
Extracting conv5_1_H (10000, 512)
Extracting conv5_2_V (10000, 390)
Extracting conv5_2_H (10000, 512)
Extracting conv5_3_V (10000, 379)
Extracting conv5_3_H (10000, 512)
Acc 0.000
wrote memory data layer to temp/mem_bn_vgg.prototxt
freezing imgs to temp/frozen100.pickleusing CPU caffe
loading imgs from temp/frozen100.pickle
loaded
Process Process-3:
Traceback (most recent call last):
File ""/home/linux/anaconda3/envs/py35/lib/python3.5/multiprocessing/process.py"", line 315, in _bootstrap
self.run()
File ""/home/linux/anaconda3/envs/py35/lib/python3.5/multiprocessing/process.py"", line 108, in run
self._target(*self._args, **self._kwargs)
File ""/home/linux/channel-pruning-master/lib/worker.py"", line 21, in job
ret = target(**kwargs)
File ""train.py"", line 75, in solve
WPQ, new_pt = net.R3()
File ""/home/linux/channel-pruning-master/lib/net.py"", line 1348, in R3
rank = rankdic[conv]
KeyError: 'conv1_2_V'",None yet
https://github.com/scikit-learn/scikit-learn/issues/13969,"Right now the common tests have two different use-cases: checking internal sklearn estimators, and checking 3rd party estimators via check_estimator.The common tests are very strict to a degree that's not really reasonable for 3rd party estimators, in particular in checking for particular error messages.
Furthermore, the required error-messages are not documented anywhere (see also #10082).I think it would be good to add a strict=False argument to check_estimator to not require exact messages, and possibly not even the exact error types. That would make it much easier for people to use check_estimator.It's true that having a way to differentiate common tests used internally for scikit-learn and those intented to run on 3rd-party packages would be very useful. Related to #6715 . Maybe using something like,in common tests would be better? For the examples you provide they should never apply outside of scikit-learn and this would avoid passing this parameter everywhere..I would rather give the user the control rather than being magic about it. It would add the parameter in a lot of places.
On the other hand, we would have to add your snippet in a lot of places as well (though not in the signatures).
Maybe someone wants to have error messages consistent with sklearn?
I'm not entirely decided but right now tending towards a parameter.I think not having the exact error message is good. Now...are we okay with not even checking the types when strict=False?Or strict could be 'message', 'type' or False? Not sure if that's necessary though.Since this seems to be the place, do we want to have a deprecation cycle for certain API changes in check_estomator? Seems reasonable to me. Certain checks can then move from non-strict to strict with a deprecation cycle (related: #14241 (comment))I'm working on this. NYC WiMLDS@hirenmayani I would recommend you start with something smaller if you haven't worked on sklearn before, you can also pick up something more tricky later.I think I can fix it. Will ask you if I need any help.@hirenmayani ok go for it then.Re-opening since we still need to make another PR to use the mechanism added in #17361 in more common tests. Also add a what's new entry.Not validating error messages is done in #18415.What else do we want to relax when strict mode is off?",API Blocker Documentation
https://github.com/jswulff/pcaflow/issues/3,"Thank you so much for sharing your code.When I run PCA_Layers, it returns 2 warnings:Do these warning affect the output?
Thank you.",None yet
https://github.com/safiqu/safi/issues/1,"I dont understand i should I do%matplotlib inline
import ipyrad
import ipyrad.analysis as ipaout put: /opt/anaconda3/envs/treemix/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.nearest_centroid module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.
warnings.warn(message, FutureWarning)data = ipyrad.load_json(""/Volumes/SSD/fina-snowfinch/montifringilla.json"")
pca = ipa.pca(data)
pca.plot()TypeError Traceback (most recent call last)
in
2 data = ipyrad.load_json(""/Volumes/SSD/fina-snowfinch/montifringilla.json"")
3 ## Create they pca object
----> 4 pca = ipa.pca(data)
5 ## Bam!
6 pca.plot()/opt/anaconda3/envs/treemix/lib/python3.6/site-packages/ipyrad/analysis/pca.py in init(self, data, impute_method, imap, minmap, mincov, quiet, topcov, niters)
109 # init attributes
110 self.quiet = quiet
--> 111 self.data = os.path.realpath(os.path.expanduser(data))
112
113 # data attributes/opt/anaconda3/envs/treemix/lib/python3.6/posixpath.py in expanduser(path)
233 """"""Expand ~ and user constructions. If user or $HOME is unknown,
234 do nothing.""""""
--> 235 path = os.fspath(path)
236 if isinstance(path, bytes):
237 tilde = b''TypeError: expected str, bytes or os.PathLike object, not Assembly",None yet
https://github.com/emdupre/me-ica/issues/10,"mdp is no longer actively maintained and importing the dependency is triggering multiple deprecation warnings. Since mdp is only really used for FastICA, it would be great to use sklearn's implementation of FastICA instead.There are a few differences between the two implementations, so ensuring consistency of results will be important.For easier comparison, the documentation for mdp's FastICANode is here.@prantikk do you have any concerns about dropping the two-stage optimization? As it stands, the default use case maintains the same cost function across both stages, so I'm not sure that much will be lost for the average user.This should probably be considered a breaking change, and only implemented in the nipype branch. Otherwise, providing the same fixed_seed will yield different results due to differing implementations across libraries.",enhancement
https://github.com/skorch-dev/skorch/issues/462,"skorch's scoring callbacks have the lower_is_better argument. @mfcabrera just informed me that sklearn's make_scorer has a greater_is_better argument. Since both have the same job, it would be helpful if they were named the same.The name could be changed in skorch to make it consistent (but that would require a deprecation period) or it is just left as is (maybe arguing that skorch leanes on PyTorch here, where lower scores are better by default).I agree this is a bit strange, since a skorch Scorer callback defaults to lower_is_better=True. It may be better to call the callback a ""Metric"", i.e. EpochMetric or BatchMetric.Hmm, I think it's a little late for renaming the callbacks themselves. And even if we rename the argument, I would still keep it so that lower is better by default, since that is the universal standard for neural networks.I was thinking of creating a new one. It’s the English that is a big strange. Normally it is better to have a higher “score”. Since we are in the context of neural networks, we want to minimize something.It’s fine as is, a little confusing for new users.",None yet
https://github.com/automl/auto-sklearn/issues/953,"I think 2deb1f0#diff-b4ef698db8ca845e5845c4618278f29a removed liac-arff from the dependency list, which is causing issues.As a side note, is there a reason why we have to curl and pip to xargs? It looks like you're populating install_requires, why the extra step? (When replacing the curl line in the following script with a pip install liac-arff, everything seems to work as expected)Here's some commands to reproduce:Install and test scriptRun with Python 3.7 Debian buster containerYeah, I am seeing the same issue and I am proposing #954 to fix this.Thanks @pdxjohnny for reporting this. I just merged @franchuterivera's fix and it'll be available in the next release (early next week).That's a leftover from times where there weren't any wheel files on pypi. Then, having the wrong installation order (pip does or did not guarantee to keep the installation order) would result in scikit-learn being installed before numpy, resulting in a compilation error.Anyway, I'm proposing to keep this open so that",None yet
https://github.com/lmcinnes/umap/issues/423,"Some tips given already in #178, although it does not solve the problem here.I'm having troubles when loading a previously saved UMAP model.
I used umap version 0.4.2
The model I construct calls umap as:
mapper = umap.UMAP(n_neighbors=150, n_components=2, metric='cosine', random_state=69, transform_queue_size=8.0, min_dist=0.0, target_weight=1, target_n_neighbors=20, verbose=True).fit(nzDense_norm,y=labels_train)nzDense_norm is 290000x4096 numpy float32 matrix
labels_train is 290000 numpy int8 vectorThe model is computed correctly and I save it using joblib:
outputFile = open(filename, 'wb')
joblib.dump(mapper, outputFile, compress=True)
outputFile.close()Then, when I tried to load it using:
mapper = joblib.load(filename)I get the error below, (which has been reported in #179 but once the model has been loaded)
Any clues or help?
In fact I performed this operation a lot of times with umap-learn 0.3.10 without any trouble. Afterwards, to get 0.4.2 running I needed to install pynndescent (which I did no thave when using 0.3.10) because I was experiencing RAM problems. But now the models are correctly done, but there is this problem of loading it once saved into disk.Traceback (most recent call last):
File """", line 1, in
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/joblib/numpy_pickle.py"", line 605, in load
obj = _unpickle(fobj, filename, mmap_mode)
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/joblib/numpy_pickle.py"", line 529, in unpickle
obj = unpickler.load()
File ""/usr/lib/python3.6/pickle.py"", line 1050, in load
dispatchkey[0]
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/joblib/numpy_pickle.py"", line 342, in load_build
Unpickler.load_build(self)
File ""/usr/lib/python3.6/pickle.py"", line 1507, in load_build
setstate(state)
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/pynndescent/pynndescent.py"", line 1026, in setstate
self._rp_forest = tuple([renumbaify_tree(tree) for tree in d[""rp_forest""]])
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/pynndescent/pynndescent.py"", line 1026, in
self._rp_forest = tuple([renumbaify_tree(tree) for tree in d[""_rp_forest""]])
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/pynndescent/rp_trees.py"", line 1178, in renumbaify_tree
hyperplanes.extend(tree.hyperplanes)
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py"", line 302, in extend
return _extend(self, iterable)
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args
error_rewrite(e, 'typing')
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/dispatcher.py"", line 344, in error_rewrite
reraise(type(e), e, None)
File ""/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/six.py"", line 668, in reraise
raise value.with_traceback(tb)
numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Invalid use of Function(<function impl_extend at 0x7f97dcd35950>) with argument(s) of type(s): (ListType[array(float64, 2d, C)], reflected list(array(float32, 1d, C)))File ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 487:
def impl(l, item):
casteditem = _cast(item, itemty)
^[1] During: lowering ""$0.4 = call $0.1(item, $0.3, func=$0.1, args=[Var(item, listobject.py:487), Var($0.3, listobject.py:487)], kws=(), vararg=None)"" at /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/listobject.py (487)
raised from /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/six.py:669
In definition 1:
LoweringError: Failed in nopython mode pipeline (step: nopython mode backend)File ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 487:
def impl(l, item):
casteditem = _cast(item, itemty)
^[1] During: lowering ""$0.4 = call $0.1(item, $0.3, func=$0.1, args=[Var(item, listobject.py:487), Var($0.3, listobject.py:487)], kws=(), vararg=None)"" at /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/listobject.py (487)
raised from /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/six.py:669
This error is usually caused by passing an argument of a type that is unsupported by the named function.
[1] During: resolving callee type: BoundFunction((<class 'numba.types.containers.ListType'>, 'append') for ListType[array(float64, 2d, C)])
[2] During: typing of call at /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/listobject.py (895)File ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 895:
def impl(l, iterable):for i in iterable:
l.append(i)
^raised from /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/typeinfer.py:985In definition 1:
TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Invalid use of Function(<function impl_append at 0x7f97dcd350d0>) with argument(s) of type(s): (ListType[array(float64, 2d, C)], array(float32, 1d, C))File ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 487:
def impl(l, item):
casteditem = _cast(item, itemty)
^[1] During: lowering ""$0.4 = call $0.1(item, $0.3, func=$0.1, args=[Var(item, listobject.py:487), Var($0.3, listobject.py:487)], kws=(), vararg=None)"" at /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/listobject.py (487)
raised from /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/six.py:669
In definition 1:
LoweringError: Failed in nopython mode pipeline (step: nopython mode backend)File ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 487:
def impl(l, item):
casteditem = _cast(item, itemty)
^[1] During: lowering ""$0.4 = call $0.1(item, $0.3, func=$0.1, args=[Var(item, listobject.py:487), Var($0.3, listobject.py:487)], kws=(), vararg=None)"" at /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/listobject.py (487)
raised from /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/six.py:669
This error is usually caused by passing an argument of a type that is unsupported by the named function.
[1] During: resolving callee type: BoundFunction((<class 'numba.types.containers.ListType'>, 'append') for ListType[array(float64, 2d, C)])
[2] During: typing of call at /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/listobject.py (895)File ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 895:
def impl(l, iterable):for i in iterable:
l.append(i)
^raised from /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/typeinfer.py:985This error is usually caused by passing an argument of a type that is unsupported by the named function.
[1] During: resolving callee type: BoundFunction((<class 'numba.types.containers.ListType'>, 'extend') for ListType[array(float64, 2d, C)])
[2] During: typing of call at /home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py (82)File ""../../Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py"", line 82:
def _extend(l, iterable):
return l.extend(iterable)
^Hi again, I've testing different options. By now I realized this error only affects data sets bigger than 4096 data points, so the issue is within the ""# Standard case"" solving in the codeHello, looking a little bit more into the code, I see the difference in the umap object on those two situations (dataset<4096 or dataset>4096) are the elements '_knn_dists', '_knn_indices' and '_rp_forest', so that could be aclue of where the issue is.I suspect the error comes from pynndescent with the '_rp_forest' object. I see that within rp_trees.py file there are two definitions as
dense_hyperplane_type = numba.float32[::1]
sparse_hyperplane_type = numba.float64[:, ::1]which could be related to that error when trying to load the umap generated model
But I do not know how to fix or overcome thisOk, I think I found the cause of that error in my first coment:In rp_trees.py lines 32-33 there are two possibilities for the hyperplanes definition:dense_hyperplane_type = numba.float32[::1]
sparse_hyperplane_type = numba.float64[:, ::1]Then in the code, depending on whether the input data is sparse or not make_sparse_tree() or make_dense_tree() functions are calledThe fact is that once a model (umap) is obtained and saved to disk, the _rp_forest object is included. But when we load that model using joblib, we go to rp_trees.py renumbaify_tree() function which has a definition like:hyperplanes = numba.typed.List.empty_list(sparse_hyperplane_type)And this is providing the error type when loading if the data for the umap model was not sparse (as it is my case)I simply tried to change this line of the function to:hyperplanes = numba.typed.List.empty_list(dense_hyperplane_type)and I could properly load the model saved (although now, a warning from numba is obtained as detailed below)I assume a conditional check for the sparsity of data should be added in renumbaify_tree() function. I'm sorry I can not do it myself as I'm not fully aware of where else will this imply changes, but hope that this analysis helps to solve the issue.The new warning:
/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py:82: NumbaPendingDeprecationWarning:
Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'iterable' of function 'impl_extend..select_impl..impl'.For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-typesFile ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 893:
else:
def impl(l, iterable):
^return l.extend(iterable)
/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/ir_utils.py:2041: NumbaPendingDeprecationWarning:
Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'iterable' of function '_extend'.For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-typesFile ""../../Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py"", line 81:
@njit
def _extend(l, iterable):
^warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))
/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py:82: NumbaPendingDeprecationWarning:
Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'iterable' of function 'impl_extend..select_impl..impl'.For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-typesFile ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 893:
else:
def impl(l, iterable):
^return l.extend(iterable)
/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/ir_utils.py:2041: NumbaPendingDeprecationWarning:
Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'iterable' of function '_extend'.For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-typesFile ""../../Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py"", line 81:
@njit
def _extend(l, iterable):
^warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))
/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py:82: NumbaPendingDeprecationWarning:
Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'iterable' of function 'impl_extend..select_impl..impl'.For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-typesFile ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 893:
else:
def impl(l, iterable):
^return l.extend(iterable)
/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py:82: NumbaTypeSafetyWarning: unsafe cast from UniTuple(int64 x 2) to UniTuple(int32 x 2). Precision may be lost.
return l.extend(iterable)
/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/ir_utils.py:2041: NumbaPendingDeprecationWarning:
Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'iterable' of function '_extend'.For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-typesFile ""../../Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py"", line 81:
@njit
def _extend(l, iterable):
^warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))
/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py:82: NumbaPendingDeprecationWarning:
Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'iterable' of function 'impl_extend..select_impl..impl'.For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-typesFile ""../../Trainer/lib/python3.6/site-packages/numba/listobject.py"", line 893:
else:
def impl(l, iterable):
^return l.extend(iterable)
/home/peppone/Feina/INLOC/Projectes/WorkDir/Trainer/lib/python3.6/site-packages/numba/ir_utils.py:2041: NumbaPendingDeprecationWarning:
Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'iterable' of function '_extend'.For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-typesFile ""../../Trainer/lib/python3.6/site-packages/numba/typed/typedlist.py"", line 81:
@njit
def _extend(l, iterable):
^warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))Just ran into this also on 0.4.4. Same error with loading after saving w/ pickle or joblib. Works fine on datasets < 4096I'll have to try and look at this soon. Things involving integrating with numba and pickle are always rather hard to get working just so.Thanks so much. Such an amazing package!Hi,
Just ran into this also on umap=0.4.5, pynndescent=0.4.8; Works fine on smaller datasets, but not on large ones. Looking forward to a fix. Thanks for all your work!In case it helps anybody: I've been able to ""bypass"" the issue for now by uninstalling pynndescent. UMAP still works without it, albeit slower, and this issue doesn't occur for me anymore.The problem with training with pynndescent is that for large number of samples, you'll have memory explosion issues mentioned in #462
If the model is already trained using pynndescent and try to load it without it installed, it will lead to import errors.Also having trouble with joblib. When dumping large file, the kernel crashes after the file* has been dumped. Then when loading file I am getting EOFerror. Also installed pynndescent as I was getting memory explosion before.*File is sklearn pipeline objectalso with pynndescent installed, transform method is slower",None yet
https://github.com/scikit-learn/scikit-learn/issues/14473,"Sometimes we return float, sometimes we return numpy array. Seems unfriendly, since numpy array is different from float in many ways.or maybe it's trivial and we can ignore it?It's not trivial, and it should preferably always be the same.
We could have a common test about it.
But changing it would require a deprecation cycle, and I am not sure it is worth the trouble (for both the developers and the users).Which one will you prefer? float seems more natural, but numpy array is consistent with our classifiers.",API
https://github.com/scikit-learn-contrib/imbalanced-learn/issues/601,"All the following classes use n_neighbors:Whereas k_neighbors is used with SMOTE and all its variants.This poses a problem with duck-typing and pipelines.SMOTE would benefit using n_neighbors to have consistent API.Darwin-18.7.0-x86_64-i386-64bit
Python 3.7.3 | packaged by conda-forge | (default, Jul 1 2019, 14:38:56)
[Clang 4.0.1 (tags/RELEASE_401/final)]
NumPy 1.17.1
SciPy 1.3.1
Scikit-Learn 0.21.3
Imbalanced-Learn 0.5.0I see. Could make sense. It would take 2 versions for the deprecation. However, you still have some other neighbors params in the smote variants as well. It could also be an issue.You could always create you grid on the fly:I would argue that the extra m_neighbors parameters in SVMSMOTE and BorderlineSMOTE have different meaning than the n/k_neighbors found in other algorithms (and themselves). The n/k_neighbors are used only for finding neighbors, whereas m_neighbors looks to me that its usage is for flagging samples as 'danger' or 'noise'.I know this is a minor issue that has simple workarounds, but I felt that it was worth marking as an issue nonetheless.We could think about modifying this in 1.X since that we will have more freedom to break the APIAdditionally, I recently noticed the inconsistency also occurs with self.nn_ vs self.nn_k_ for non-SMOTE and SMOTE repsectively.hey! come here from #680Thanks for your answer.I know it's more or less complex and need some time for this cycle (waiting for two releases) but, is it going to start?Thanks",Type: Enhancement
https://github.com/scikit-learn/scikit-learn/issues/7736,"In the manifold library, I have noticed that t-sne has n_iter as an argument for the maximum number of iterations and MDS has max_iter. I was wondering if it was an inconsistency in the API. If that's the case, I'm happy to submit a PR with the right parameters.0.18Thank you so much for the awesome work!Hm we do have n_iter in some places. I guess if there is another stopping criterion max_iter is more appropriate. Question for the other devs: should we rename?
I'm all for consistency but you know, the deprecations....
(good opportunity to use futurepast ;)We also have n_iter_max in a public helper in SpectralClustering. Yuk.doing git grep ""self.n_iter = n_iter"" | grep -v test shows:Though in _search / grid_search it's fine, cross-validation is deprecated.
That leavesThere's #5036 for SGD (omg that's still not merged?!)
For the other ones we need to check whether they are the only stopping criterion or if there are others -- I imagine there are additional stopping criteria everywhere.
Then we should rename.A quick overview gave me:I would say why not for the last 4, with deprecation and all.The ""why not"" would be ""the deprecations and all"" ;) But I think it might be worth it?I would be happy to help out to make that change happen if you have an example of deprecations.http://scikit-learn.org/dev/developers/contributing.html#deprecation@amueller Thank you, I will work on the following methods",API
https://github.com/scikit-learn/scikit-learn/issues/13396,"CI is erroring on uncaught deprecation and future warnings but that's hard to reproduce locally unless you really know what you're doing.I feel like we should make this easier, or maybe even the default when running tests locally?One way would be to do this in the makefile, which would only help if using make, but that already would be better, I think.We could also add it to the configuration, not sure if that's overkill?It's a bit annoying to have tests pass locally but then not on CI because it uses different flags [yes yes, I added these flags, I know].Maybe I just wanted to be conservative? Actually I agree with you, we should always run it.Because our CI runs with either fixed versions or the latest versions of dependencies. Developers and people packaging scikit-learn for Linux distributions can run it with arbitrary versions of dependencies where we cannot guarantee that some warning won't occur (particularly for not yet released versions of numpy/scipy). In those conditions, this option will lead to unwanted test failures.See for instance #12043So I don't think having it as default is a good idea, however +1 adding a new entry (non default) to the makefile if you find it useful.A makefile entry would make passing pytest parameters like -k or -s a bit cumbersome.Is there a way to easily bypass the flags in setup.cfg that would make the lives of those that build sklearn for arbitrary version easier?@rth lol ok yes, I was opposed to this change ;) but clearly didn't see the ping so that's on me. And I agree your reasoning is sound but the change makes running the tests locally too hard.could we by default raise on deprecation warnings only from within sklearn?I think it's weird to have something pass locally and then fail on the CI with the same versions.Yeah, that's the reason I'm not using the makefile for testing :)Last time I checked it was non-trivial, at least much harder than providing a CLI parameter to pytest.My point that one shouldn't have to bypass the flags in setup.cfg. pytest sklearn should run the tests suite (even without reading the docs) and fail only if something is broken in scikit-learn. When we handle warnings as errors, we don't test scikit-learn alone but also that none of the dependencies raise any warnings (including their not yet released versions) which is outside of the scope of unit tests IMO.For instance, I would consider a bug, a situation where tests pass for some scikit-learn release; then start failing few month later because of a new scipy release that's perfectly backward compatible but adds a new harmless deprecation warning.An option could be to suggest setting the PYTEST_ADDOPTS environement variable for developpers prefering this default.@rth my concern is more about deprecation warnings within sklearn which couldn't raise these spurious warnings. If I deprecate something in sklearn I should catch or change all occurrences. Right now the tests don't force me to do that locally, but they do on the CI.I think warnings caused by deprecations within sklearn are much more common than from scipy or numpy. If we can't filter these, then I guess another option would be to not use DeprecationWarning but only error on our own warning that inherits from DeprecationWarning?Yeah, but even a single deprecation warning from numpy / scipy / pandas breaking tests post release (for the sake of development convenience) is not OK IMO. The latest was the deprecation of scipy.sparse matrices not that long ago.We could if you think it's worth the effort.Wouldn't setting the PYTEST_ADDOPTS variable locally solve that?To summarize, I still have concerns about changing setup.cfg to error on warnings for all versions of dependencies, but if others think it's a good idea I won't argue against it. +0 overall.@rth I agree they shouldn't lead to errors post-release.",None yet
https://github.com/EpistasisLab/tpot/issues/823,"I'm using the TPOT package with Dask and I'm encountering an exception while using a remote Dask clusterI've created a Dask cluster in Google Cloud Container Engine as per the documentation, http://docs.dask.org/en/latest/setup/kubernetes-helm.htmlI've included as dependencies for the workers in the conf.yaml as follows:[describe what you would expect to have resulted from this process]After executing tpot.fit()
The client side generates a traceback:Inspecting the cluster worker logs, you'll find:No fix suggestionsHaving the same issue. Not sure, but may be a problem inside Dask and its serializer.@aleksarias This may work: try this before running tpot.fit(X_train, y_train):And then try to execute tpot.fit(X_train, y_train)@julioasotodv Also having this same issue, but using your suggestion did not help.I'm running in Docker using Python 2 -- If I had to bet, this would be the cause.@masonkirchner ... I doubt it's a Python 2 issue... my logs show I was using python 3.7 and still got the issueSo maybe my issue stems from Docker? Haven't dug too much into it, but I could see maybe some weird things arising from Dask starting up workers within the container?The error is in the dask worker.
TypeError: No dispatch for <class 'xgboost.sklearn.XGBRegressor'>
For some reason the dask_deserialize function isn't finding the XBGRegressor class in the environment.
Here's the line where the issue is arising:I went into the same issue today and fixed it by making sure both the client and workers have the same version of xgboost installed.",bug question
https://github.com/scikit-learn/scikit-learn/pull/7761,"#7736I have replaced n_iter by max_iter and added the deprecated decorators in the methods discussed in #7736.The tests are not passing yet and I have a hard time to debug without test isolation. How can I isolate the file I want to test?You can isolate tests with the command:
nosetests sklearn/tests/test_common.py:test_non_meta_estimatorsNote that this is currently breaking user's code, for example if they have BayesianRidge(n_iter=10).
You should keep a parameter n_iter in the init, with default None, and raise a warning only if n_iter is specified (i.e. not None).
You will also have to change n_iter in the tests.Thanks for the feedback.I still get a few errors in the test coming from the n_test_ assertion and the failed examples.https://travis-ci.org/scikit-learn/scikit-learn/jobs/171341699To pass the tests, you'll also have to add an attribute self.n_iter_ with the actual number of iteration done before convergence.",Stalled module:linear_model module:manifold module:neural_network
https://github.com/Avik-Jain/100-Days-Of-ML-Code/pull/40,"I have read and understood CONTRIBUTING document.
I have read and understood CODE_OF_CONDUCT document.
I have included tests for the changes in my PR. If not, I have included a rationale for why I haven't.
I understand my PR may be closed if it becomes obvious I didn't actually perform all of these steps.The changes made in this commit address the deprecation warnings occurring during the preprocessing stage. It uses the new 'sklearn.model_selection' package to use the 'train_test_split' method.",None yet
https://github.com/rapidsai/cuml/pull/2758,No description provided.Please update the changelog in order to start CI tests.View the gpuCI docs here.Resolves #2725.Just a few small comments added to the comment about the warningThanks! Updated according to those suggestions. Leaving the conversation about the warning open to make sure that the new version matches what we're looking for for behavior-change warnings.Just noticed one super minor detail,5 - Ready to Merge
https://github.com/broadinstitute/getzlab-SignatureAnalyzer/issues/14,"Hi,While trying to run the latest commit and the latest PIP version, I kept getting missing key errors:(please excuse the X server errors - I don't know why it's complaining about not having X forwarding but it doesn't seem to impact anything.)I traced it down to this line:It looks like the ""chr"" prefix gets appended to the MAF but not the reference 2bit chromosome names. I was able to successfully run by removing this line and an identical one later in the file.I'm happy to make a PR to remove these lines, but I assume they were put there for a purpose. Maybe because they need to be coerced to strings and numpy is trying to interpret them as ints?Generally I try to avoid enforcing prefixes with references/VCFs/MAFs because it's easy to get mismatches like this. Every center has their own way of resolving them, and in enforcing one way or the other I have usually created more problems than I started with.I assume this would also lead to the MAF or spectra output having the ""chr"" prefix, or are they chomped off before final output?Hello,get_spectra_from_maf should now check the hg build file for the ""chr"" prefix and only attach it if necessary.",enhancement
https://github.com/scikit-learn/scikit-learn/issues/6830,"I think the dual variable in LinearSVC and LogisticRegression should be set based on the penalty and loss parameters. (not sure if there is already an issue for that).Ill choose you for my first issue to address at the Scipy 2016 sprint!Be sure to use the deprecation mechanism: http://scikit-learn.org/dev/developers/contributing.html#deprecation@amueller could you please help clarify, if only the below 2 files need to be changed?
sklearn/svm/classes.py
sklearn/linear_model/logistic.pyYou also need to add the correct behavior for auto which would be something compatible with the loss and penalty parameters. If there's more than one possible way to set dual based on loss and penalty we need a heuristic, probably based on n_samples, n_features and maybe whether the data is sparse. That actually requires some benchmarking, I think.For now, we could say always default to dual=False and don't use a heuristic as a first step.@amueller so for now what is the scope of this Issue. If its not too complicated I could work on this.
ThanksAs I said above, I think for now it would be fine to have ""auto"" be ""dual=False"" if ""dual=False"" is supported and otherwise ""dual=True""If we do it like is it's a backward-compatible change. If we do anything else, we would need a deprecation cycle...",None yet
https://github.com/ctuning/ck-openvino/issues/22,"SSD-MobileNet OpenVINO can be installed as follows:Attempting to use SSD-MobileNet via an *ssdm* command key when it's not installed, however, causes a fantastic (or rather phantasmagoric!) chain reaction involving ImageNet calibration:",bug invalid
https://github.com/machinalis/yalign/issues/8,"I tried to create a model using yalign-issue6-response package and I am getting the following warning.DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.17. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sampleThough i got aligner.pickle and metadata is created. I am attaching the file for reference.en-es.zipI dunno whether I can use that or not. It would be grt If I get an earnest replyYes, I get this too. It has no effect on the result of the yalign-align program, it is just a warning. What I do is, suppress the annoying message (something like this):
yalign-align ......... doc_A doc_B 2>&1 > aligned-file.txt | grep -v ""DeprecationWarning""However, this is just a shell trick which sends stderr to the screen unless it contains the text ""DeprecationWarning"".It would be nice if somebody could fix this (I have tried, but I don't really know enough about it to have succeeded), because presumably yalign will fail when the next version of sklearn arrives.Thanks a lot for your earnest reply. Now it got working.I wish to develop a model for other language that has word boundary. What will be the optimal number of parallel sentences we would be needing for developing the model.Is 15k parallel sentences a fair amount?@simontite-capita-ti to the new version of sklearn(0.18.1), DeprecationWarning comes out and yalign doesn't works now. do you have any tips@luoyangen
I haven't tried this on the new version of sklearn yet, but this fixes the deprecation warning, so it should probably also fix the ValueError in 0.19:In yalign/svm.py, after line 51 add the line:
vector = vector.reshape(1, -1)The whole function now looks like this:",None yet
https://github.com/ContinuumIO/anaconda-issues/issues/11795,"When I import sklearn in Spyder or jupyter notebook I get a DLL load fail error. (see full error copied below)Steps taken to prevent this error:Note that running import sklearn from jupyter notebook or the consol causes the same error and dependencies like scipy work fine.Be able to do:
import sklearn
in the base environment of anaconda individual editionIn the base environment of a newly installed anaconda, open spyder and import sklearn.Anaconda3-2020.02-Windows-x86_64 downloaded today 09.05.2020 from the official anaconda website.Microsoft Windows 10 Home
Version 10.0.17763 Build 17763
System Type x64-based PCAfter installing anaconda on a different PC (windows 7) which had never any previously installed versions of python sklearn works! Still don't know where the problem comes from on my machine, will probably have to reboot it.",None yet
https://github.com/AshishBora/csgm/issues/9,"here is the problem, when I run the demos, each returns
Traceback (most recent call last):
File ""./src/compressed_sensing.py"", line 177, in
main(HPARAMS)
File ""./src/compressed_sensing.py"", line 19, in main
xs_dict = model_input(hparams)
File ""/home/feiht/csgm/src/mnist_input.py"", line 56, in model_input
mnist = input_data.read_data_sets('./data/mnist', one_hot=True)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
return func(*args, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 262, in read_data_sets
train_images = extract_images(f)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
return func(*args, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 62, in extract_images
magic = _read32(bytestream)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 43, in _read32
return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]
IndexError: index 0 is out of bounds for axis 0 with size 0why is it, thank you!Did you download the dataset?Dear author , when I run the instruction./quick_scripts/mnist_reconstr.sh,I was also got the error like this ,how to solve it? Thank you very much!!(/home/gamkiu/anaconda3/envs/test) gamkiu@ubuntu:~/csgm$ ./quick_scripts/mnist_reconstr.shbatch_size = 10
checkpoint_iter = 1
dataset = mnist
decay_lr = False
dloss1_weight = 0.0
dloss2_weight = 0.0
gif = False
gif_dir =
gif_iter = 1
image_matrix = 1
image_shape = (28, 28, 1)
inpaint_size = 1
input_path_pattern = ./data/celebAtest/*.jpg
input_type = full-input
lasso_solver = sklearn
learning_rate = 0.01
lmbd = 0.1
max_update_iter = 1000
measurement_type = gaussian
mloss1_weight = 0.0
mloss2_weight = 1.0
model_types = ['lasso', 'vae']
momentum = 0.9
n_input = 784
noise_std = 0.1
not_lazy = True
num_input_images = 10
num_measurements = 100
num_random_restarts = 10
optimizer_type = adam
pretrained_model_dir = ./mnist_vae/models/mnist-vae/
print_stats = True
save_images = False
save_stats = False
sparsity = 1
superres_factor = 2
zprior_weight = 0.1Extracting ./data/mnist/train-images-idx3-ubyte.gz
Traceback (most recent call last):
File ""./src/compressed_sensing.py"", line 177, in
main(HPARAMS)
File ""./src/compressed_sensing.py"", line 19, in main
xs_dict = model_input(hparams)
File ""/home/gamkiu/csgm/src/mnist_input.py"", line 56, in model_input
mnist = input_data.read_data_sets('./data/mnist', one_hot=True)
File ""/home/gamkiu/anaconda3/envs/test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 213, in read_data_sets
train_images = extract_images(f)
File ""/home/gamkiu/anaconda3/envs/test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 53, in extract_images
magic = _read32(bytestream)
File ""/home/gamkiu/anaconda3/envs/test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 35, in _read32
return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]
IndexError: index 0 is out of bounds for axis 0 with size 0
(/home/gamkiu/anaconda3/envs/test) gamkiu@ubuntu:~/csgm$",None yet
https://github.com/dmnfarrell/smallrnaseq/issues/6,"I found smallrnaseq depends on scikit-learn==0.19.1, while the new version of which is 0.22.
I can install 0.22 version without error, but unfortunately, 0.19.1 seems not compatible in my computer.
Here is some report of error informations:Do you have any suggestions?Here's the complete output. Hope it can help!",None yet
https://github.com/scikit-learn/scikit-learn/pull/17743,"Towards: #3020It deprecates 'normalize' in _base.py (LinearRegression)@rth @agramfort @glemaitre
what do you think?(problem with the docs should hopefully be fixed soon: #17745)you will also need an entry in what's new do document the deprecationbesides LTGM provided CIs are happy (doc build included)To clarify, I'm assuming that ""removing the parameter in 0.26"" means ""no normalization happens in 0.26"".If instead we decide that removing the parameter means ""still do whatever the default was"", then there is no problem.Unless I am missing something in #17743 (comment), then I think we need to first change the default from True to False and then to remove the parameterI see now what you mean @NicolasHug . Let me clarify then.Let' make the new default 'deprecated'. Now...If the default is now False:if the default is now True then in 0.24 and 0.25 you getif the default is now True then in 0.26 and 0.27 you get@maikia @NicolasHug does it sound reasonable? You can update my comment if you want.Thanks @agramfort , I think this is the right way to do things. It's a bit annoying that we're forcing users to set the value to False and then to remove the parameter, but I don't see any other way.Surely this kind of situation has happened in the past, maybe @jnothman @amueller @ogrisel @rth can confirm we should do as suggested in #17743 (comment)?Should be good to go. What do you think @NicolasHug @agramfort @rth @glemaitre ?I'll let @NicolasHug double check it does what he had expectedthx @maikia !Hi @maikia if you are still interested in working on this, do you mind fixing conflicts with upstream? Thanks!@NicolasHug @glemaitre ?this one is stuck until we make progress in #18159",module:linear_model
https://github.com/scikit-learn/scikit-learn/pull/15906,"Previously, classes with methods decorated with if_delegate_has_method still had their instances display the method via dir and code auto-completion even when the delegate did not have the method. This uses __set_name__ to dynamically adjust __dir__ for a class instance to conceal methods that would throw an AttributeError due to a delegate not implementing the method.__set_name__ was introduced in Python3.6, but if Scikit-Learn is installed with Python3.5, the __set_name__ protocol will not be invoked. This should not be an issue other than this pull request not affecting builds under that version.Note for convenience, the methods in Pipeline that are decorated with if_delegate_has_method are:",module:utils
https://github.com/scikit-learn/scikit-learn/pull/8338,"#6346Changed default param for percentile to a more reasonable default value.Continue to review full report at Codecov.This probably needs a whatsnew entry?As suggested in #6346, it's not this simple, and requires some deprecation process. Imagine, for instance, a user had relied on this default when selecting a different mode.or at the very least, as stated in #6346, we would need to document that as a breaking change.@vincentpham1991 have a look at http://scikit-learn.org/stable/developers/contributing.html#deprecation if you want to implement this.Thank you everyone for the comments and help and sorry for the delay. I addressed the comments above, please let me know what else needs to be changed.You've introduced some mess in your commit history; and for deprecation, the default must remain the same for now, and the warning should be silenced if the user enters a non-default.",Stalled module:feature_selection
https://github.com/adobs/project/issues/1,"Hi,
I get that error message when I run pip install -r requirements.txt:Could you please tell me how to solve it?
Best regardsScikit learn has dependencies: Python, Numpy, SciPy (http://scikit-learn.org/stable/install.html)Hypothesis: the requirements.txt was created in alphabetically order. In the doc, scikit-learn==0.17 comes before scipy==0.16.1.
If this is the case, in the terminal, type ""pip install scipy"". Then run pip install -r requirements.txtPlease let me know if this still does not work.Thanks for your kind help.Unfortunately the newbie I'm is still facing some issues, I hope these logs can help:Hi Alexandra,Sorry to annoy again, but I'm still facing that issue: https://paste.debian.net/hidden/14c602d0/Any idea to solve it?Best regards from France",None yet
https://github.com/gitter-lab/ml-bio-workshop/issues/7,"I converted the list to the GitHub checkbox format. We can check off the short term issues and create new issues for specific items that take more than a few days or are longer-term efforts.Done:To do:Excellent progress!This could wait for v2. It may be a less common use case.For the documentation, I suggest documenting the code instead of writing a separate Markdown file. There are Python documentation conventions that can be used to generate documentation files from the code comments. sklearn is actually a great example of this because they have strong documentation. If you inspect their source code, for example the decision tree, you can see how the functions, arguments, examples, etc. are all documented in the code. They are somehow using Circle CI to automatically build and deploy the documentation, but we wouldn't need that complexity.I believe that Sphinx is the underlying system that translates the comments to external documents. Let's explore that as an option for documentation.I added some good suggestions from @csmagnano to the list above. Thank you for the testing and great feedback.The wrapper script warnings he saw were:Both seem relatively harmless but will confuse beginners.",None yet
https://github.com/scikit-learn/scikit-learn/pull/11524,"Fixes #5545This PR creates a function sklearn.utils.init_arpack_v0(size, random_state) which goal is to be used each time eigs, eigsh or svds from scipy.sparse.linalg is called. It initializes the v0 parameter correctly with value sampled from the uniform distribution in [-1, 1] (like in ARPACK) to avoid convergence issues with another initialization. The v0 parameter is mandatory as it is the only way to render linalg functions behaviour deterministic.I put the function in __init__py as I have seen that some general utils functions are there but I'm not convinced by this choice. Maybe a utils.utils could be created to contain one shot functions which don't belong to a group ?
For now I just replaced places where randomization was correctly set using v0 parameter.
TODO :@amueller @rth : Should I change some tests or define new ones for the functions and classes that have been changed (most notably those calling svds without random_state defined) ? It seems that random_state params are not often checked by test so maybe leave it like that ?I opened an issue for scipy to add a seed parameter, I'm probably going to take care of it after this PR. Maybe there are some impacts on this work but I don't think so.There are still some stuffs bothering me but I can't find a better solution :For your TODO list, you can make a check list like this
[] todo 1
[] todo 2
...For the unit test of _init_arpack_v0, you can generate several v0 with different random_state and check that they are not all equals. You can also check that they are sampled as expected, using pytest.approx on the mean and std for example.@rth do you know what's the failure on LGTM ?lgtm is broken right now :-/LGTMAny idea what the codecov fail mean ?Test would need to be added for the lines that are currently not covered (or the corresponding if branches removed if possible).Just noticed that there is an arpack submodule sklearn/utils/arpack.py.
Although all functions inside are in deprecation cycle, I think _init_arpack_v0 should be in there.
And if you make that scipy PR, this function would eventually be deprecated one day.Are you sure about this ? Because the first line of arpack.py says that this file is to be removed in version 0.21. That's why I did not put it there. Should I do the change or do we merge like that ?Yes, feel free to remove that comment and put it there. Only the deprecated functions will then be removed.Any idea on the error ? This seems completely unrelated, should I resubmit ?The CI fails because you need to fix the following docstring,(adding the random_state to the expected output)Otherwise LGTM, thanks!Please add add .. versionadded:: 0.21 below the random_state in docstrings where this parameter was added.Also please add a what's new entry.@FollowKenny are you working on those comments?I think I have everything ready to push but I'm still not sure about the last two points you raised in your last comment.
I can push my work next week, as I'm in vacation right now.hello @rth , @jnothman , i will be taking over this issue from @FollowKenny . I am new to git so please bear with any newbie mistakes i may make :) . thanks.
Also, do i continue to work from branch 0.21.X?Hi @imnotaqtpie, sorry, I know it took a while to come back to you. Are you still interested in taking over? Do you need some guidance? Thanks and sorry again.take",Stalled Superseded
https://github.com/scikit-learn/scikit-learn/issues/14720,"Something that came up as part of the GLM PR in #14300 (comment)@rth@NicolasHugopening this issue for further discussion.@rth I edited _tree and _root to tree_ and root_To sum up my view: I think that tree_ and root_ should be private. I'm still OK with documenting these, but we should make clear that we don't guarantee backward compatibility.Overall I agree. Though I now that those are public, I wonder if it's really worth changing them.BTW, GradientBoostingClassifier.loss_, a LossFunction (will be private once the deprecation cycle finished in 0.23) is also affected.A releated question is then more on how to support custom objects as input arguments and whether this should be mentioned in some way in the docstring.For instance,Another instance that I found is the loss_function_ attribute of BaseSGD and all child classes. I'm not sure it make sense to expose these, even though they're not explicitly private, they are defined in sgd_fast.pyx. They're all undocumented classes.from #14789 (comment)",None yet
https://github.com/jakevdp/sklearn_tutorial/pull/1,,None yet
https://github.com/nilearn/nilearn/issues/1426,"Under the supervision from Bertrand Thirion, I have read the documentation of nilearn and may express some comments.General comments :
-Even though people tend not to read a documentation sequentially, the sections 4 (plotting) and 5 (manipulating brain volumes) could be put in position 2 and 3. I think it is more convenient to handle statistical tools when one knows and understands the concept of masks.-The code in the documentation is often not straightforward and it could be a plus if one could just copy and paste the lines on an iPython terminal to run it.-Some parts are redundant and there are explanations of the same subject which come many times in the doc (eg the construction and the inverse of a masker)Some warnings
The code is highly dependent to the sklearn library, and some deprecation warnings appear for :
-the cross-validation
-other deprecation warning concerning matplotlib (axisbg has to be changed to facecolor)Minor requests
Here are some subjectives remarks on the content of the documentation. In parenthesis is the related part of the documentation :
-(3.3.1.3) Isn't it possible to order the components to the best independent to the least?
-(3.3.2) The dictionary learning could be explained a bit
-(3.4) This section seems redundant with the two previous ones (3.3 and 3.2) and could be merged
-(4.2) The slice selection is not clear when automatic
-(6) This section is a recap of all the previous ones and does not add a lot, but may be volunteer.As far as I have followed, these warnings are fixed with this PRs #1291 and #1342 in master.Fixed with this PR #1395 and one PR #1423 under review.Are you able to still see these warnings despite of these fixes ?I agree with concept of masks. But, then leads to further discussion and approvals from other people before refactoring.If you have observed, we point the link to the example in each section so users can play with the example in IPython.Not so clear. May be you can point to the section.Merging things can make complex to understand for the users showing everything in one complete section. That's why we made into two parts. Region extraction is a post-processing method when someone wants to split networks into regions. I feel it is in good position in terms of ordering of the section.",None yet
https://github.com/scikit-learn/scikit-learn/issues/8737,"This runs and I think it should error.
By default f1 uses binary averaging, which makes no sense for multiclass IIRC.
I'm sure @jnothman knows more about this than me.
I'd argue this is a bug in the f1_score which should error if average='binary' but the targets are not binary.
Though I vaguely remember that we discussed that we want to enable this behavior to compute the f1-score for a single class that is selected by pos_label but that seems to be covered by
thisThe docstring seems exactly right to me but the behavior is different.I'm even more confused by this:now there is no class named 1. What's happening?@jnothman was that in 0.18.1 already?Ok yes I think it should be, a student showed me this and I don't think they have a dev install. grrrr!I reckon something is wrong in LogisticRegressionCV. GridSearchCV(LogisticRegression(), {'C': [.1, 1, 10]}, scoring='f1').fit(iris.data, y) correctly raises an error.Well, a small separate issue is that LogisticRegressionCV is using SCORERS rather than get_scorerWait. Look at the structure of lrcv.scores_. Scores are calculated per class. I think this needs documentation under scoring.LogisticRegressionCV(scoring=""f1"", multi_class='multinomial').fit(iris.data, y) on the other hand throws an error: ValueError: could not convert string to float: 'setosa'.In any case, LRCV is what needs work. I wouldn't call this a blocker.I thought it was in the scoring, not LogisticRegression - then I would have thought it was blocking.Hi @amueller @jnothman I'd like to work on this.So far what I got is that in LogisticRegressionCV, all multi-class classification will be implicitly ""casted"" to binary problem with the following code:where pos_class will iterate through all possible classes. Clearly this conversion does not care whether there's a class named 1. That's why f1 score will work in above cases.Just to check the expected behavior: Error should be raised for bothandright?I will also 1) document the fact that scores are calculated per class and 2) change LogisticRegressionCV from using SCORERS to get_scorer.Any advice/correction will be appreciated :)I think LogisticRegressionCV(scoring=""f1"", multi_class='multinomial').fit(iris.data, y) should definitely raise an error, but not the error it's raising now.Given that LogisticRegressionCV computes the binary scores, the behavior is a bit unexpected but not wrong. I think we should just document better.Arguably it would be better to convert it to computing overall scores, not per-class. That would change behavior quite a bit, though.If I understand correctly, there are two cases:In all, the behavior seems reasonable.What is the structure of scores_ when multinomial?Why is this in 0.20, @amueller ?In version 0.20, we changed to using the scoring parameter rather than the default accuracy score when LogisticRegressionCV.score was called. But scoring is being applied to the multiclass problem, while the scores calculated internally are over binarised y for each class (where multi_class='ovr')@jnothman sorry I'm not sure I follow entirely. Are you saying this is solved?the original example errors now (as I think it should be), so we might be able to close this?the structure of the scores doesn't make sense for multi_class='multinomial' because it looks like it's ovr scores but they are actually multiclass scores and not per-class.works, which makes sense, but then res.score errors, which is the right thing to do; but a bit weird.Do we want to change the structure of scores_ depending on whether it's ovr or multinomial? That also seems a bit weird but the current structure makes no sense.clearing the milestone, and it seems this is at least half resolved.",help wanted
https://github.com/scikit-learn/scikit-learn/issues/15801,"On the Utilities for Developers page, it states:If we want to provide utilities to support third-party estimators, we should treat some of these utilities as ""first class"" citizens.For example safe_indexing would be extremely useful for third parties that want to support DataFrames as input. Currently, the options for third-party developers is to build their own ""safe_indexing"" or depend on our private version which may not be stable.Another example is scikit-learn/enhancement_proposals#22, which defines a n_features_in_ contract where we will internally use private methods to cohere with the contract. Third-party estimators would need to build their own methods or functions to work with the SLEP.TLDR: Now that much of the utilities are ""private"", we can make deliberate decisions about what utilities should be public and supported by us. This would mean deprecation cycles, etc. If we support some of the utils module, it will make it easier to build estimators, which will enrich the ecosystem of scikit-learn compatible estimators.CC @scikit-learn/core-devsIn practice, that's not true. We've been treating all these utilities as public, and we've always deprecated things smoothly with the 2 versions rules. Case in point: safe_indexing, which we just deprecated in 0.22.It seems that the questions are:I'm happy to consider a proposal :)I'm +1 with that. Then, we should ensure some deprecation cycle if we have breaking changes but we could make it as with experimental to have fast adoption (and we can use DeprecationWarning to warn the end-user :)).I think that we can come with a list of utilities. Another way would be to ask third-party which tools they are using?I guess not @NicolasHug , see #14545, we change the behaviour of check_is_fitted without a deprecation cycle (You can say that there's a deprecation cycle, but I'd argue that since we no longer support the old behaviour, the deprecation cycle there is meaningless).Perhaps we should introduce a deprecation cycle if we want to change public functions in sklearn.utils.",None yet
https://github.com/scikit-learn/scikit-learn/pull/16961,"Partially fixes #11198
Based on #16392This PR replaces grid_scores_ with cv_results_ in _rfy.py.
Also adds temporary property grid_scores.I plan to change tests in a similar way (replacing grid_scores_ with code of property grid_scores_) after confirming that it is correct way to do so.
Am I mistaken or are those grid_scores a one-dimensional array?lgtmThis needs test for grid_scores_ and cv_results_.i think you test fail because you need rebase. check this great tut by my dear friend https://www.youtube.com/watch?v=Gjd44YpucEA@arka204 are you still working on this?A sphinx warning in the documentation is preventing your build from finalization.
Once the sphinx warning fixed and if you think that this PR is ready for review, do you mind changing the title from [WIP] to [MRG] as specified in the documentation? Thanks!Can I have Your opinion on this @jnothman, @thomasjpfan?We need a nontrivial test to make sure cv_results_[""mean_score""] and cv_results_[""std_score""] are computed correctly.This is looking good!We can have test_rfecv be the only one that explicitly checks for the deprecation warning.The other tests can be decorated with ignore_warnings and remove the pytest.warns. For example:The computation of cv_results_ seems a little off.Hi @arka204 , would you be able to finish this pull request? Thanks!",Needs work module:feature_selection
https://github.com/lemieuxl/pyGenClean/issues/26,"I'm encountering an error while following http://lemieuxl.github.io/pyGenClean/install_linux.html#testing-the-installationI installed pyGenClean using virtualenv as directed.Command:
run_pyGenClean --conf test.ini --bfile pyGenClean_test_data/1000G_EUR-MXL_Human610-Quad-v1_ HContent of test.ini:Tail of output:This is due to a deprecation of the scikit-learn module. Downgrading it to a version prior to 0.19 should resolve the issue. We are currently using scikit-learn version 0.18.1 in production.I also noticed that some plots generated using matplotlib version 2 are invalid. You should probably downgrade matplotlib to a version prior to 2 (we use version 1.5.3 in production).We are planning to update pyGenClean to increase efficiency, enable Python 3 compatibility and use most up-to-date modules. Since this is going to take some time, I recommend just downgrading the two modules as a quick work around.Thanks @lemieuxl that worked!One further question, is using plink 1.9 supported (or will it be)? I just tested using it with the above test run and it seems to work OK, and of course much faster (which is the main benefit of 1.9 vs 1.07)We wanted to wait for an official release of Plink 2 before testing the compatibility with pyGenClean, but I guess it should be for the most part (since the developers say that version 2 is backward compatible with version 1).I'll leave the ticket open for now, to remind me to fix the scikit-learn issue when updating pyGenClean.",None yet
https://github.com/Aifred-Health/Vulcan/pull/180,"No description provided.@nirtiac @rfratila
I will work on all the above suggestions and get back to you.Hi Divya,Can you explain the benefit of
'''elif interactive is True:
plt.draw()
plt.pause(1e-17)'''Over what was already there? I'm afraid I've forgotten by then.Also, if the solution for most of the warnings is to have an updated sklearn/numpy/whatever package, can you update the requirements.txt to include the required package versions?@divyachandran-ds can you update the requirements as discussed above? ^Hey @divyachandran-ds there's a lot of extra files on this. Can you constrain it to just the files required by Vulcan (what was in this file before) but just the updated version of these packages?",None yet
https://github.com/alegonz/baikal/issues/16,"I don't know how many people is using this library, but from now on I'll make an effort to post in advance any new features and changes that I plan to make to the API in this thread.Please be aware that baikal is still a young project and it might be subject to backwards-incompatible changes. The major version (following semver) is still zero, meaning that any changes might happen at anytime. Currently there is no deprecation policy. I don't think there is a significant user base yet, so development will be rather liberal introducing backward-incompatible changes if they are required to make the API easier to use, handle important use-cases, less error-prone, etc. That said, I'll make an effort to keep the backward-incompatible changes to a minimum.If you are using baikal (thank you!) I'd suggest doing the following:Comments and discussions are of course welcome in this thread :)(This thread was inspired by the one used by the trio project)This will be a backwards-incompatible change, necessary for the other two changes described below.The idea is that instead of doing this:you would doso that it is possible to call the same step (a shared step) with different behaviors on different inputs. So, for example, learned target transformations would be expressed as:Both compute_func and trainable would be keyword-only arguments. This is to make client code more readable and to allow baikal to change the order in the future without breaking existing code.The renaming of function to compute_func is to be consistent with the future fit_compute_func argument described below.(See Issue #11 for the original discussion.)The idea is that steps could be called an arbitrary number of times on different inputs with different behaviors at each call (e.g. trainable + transform function in the first call, non-trainable + inverse transform function in the second call).The motivation is to allow reusing steps and their learned parameters on different inputs (similar to what Keras do with shared layers). Having shared steps is particularly important for reusing learned transformations on targets like in the example above. Also, this would allow reusing steps like Lambda to apply the same computation (e.g. casting data types, dropping dimensions) on several inputs. Currently, calling a step with new inputs will override the connectivity of the first call, so this is not possible yet. One could perhaps work around this limitation by having a step with pointers to the parameters of an earlier step, but that might end up being unwieldy.(See Issue #13 for the original discussion.)The motivation is three-fold:Currently the above is not possible because Model.fit runs each step's fit and predict/transform method separately, making it impossible to control them jointly. To make this kind of training protocol possible, I plan to add a fit_compute API that allows you to have more control on the computation at fit time (*1). The idea is that, for example, in the case of a stacked classifier, you would define the method in the first-level steps like this:and Model.fit will give precedence to this method when fitting the step. This should allow defining the stacked model once and fitting it with a single call to model.fit, without having to build an train the first and second stages separately.Analogously to compute_func, a fit_compute_func argument will also be added to Step.__call__ so client code can specify arbitrary methods.fit_transform (transformers) and fit_predict (classifiers/regressors) are special cases of fit_compute and will be detected and used by Model.fit if the step implements either.",None yet
https://github.com/scikit-learn-contrib/skope-rules/pull/42,"Hi !I've added some new features for skope rules :""myfunc"" being another new SkopeRule class parameter :Some issues that have been raised (but no merged) are also answered, such as :thanks for the contribution, all of this look very exciting and very usefulcould you please split this PR into different ones, so that each of them addresses a specific enhancement or fix?",None yet
https://github.com/scikit-learn/scikit-learn/pull/14557,"As far as I can tell the sklearn.utils.graph_shortest_path is the same implementation originally in 2011 by Jake Vanderplas as scipy.sparse.csgraph.shortest_path (available since v0.11), except that the scipy version (unlike the one in scikit-learn) has received several improvements since.This removes the scikit-learn versions in favor of using the one from scipy.I have not benchmarked the run time or audited the code in detail, but given that it's used in scikit-learn only once in Isomap I think it's worth using the scipy implementation in any case.Removing without deprecation warning per #6616 (comment)Ha I figured this was related to #6616 ;)seems like there's some mismatch but I'm pretty sure dropping this is the right way to go.Were we calculating the wrong shortest path? (I don't see why else the tests would fail)This is due to two issues:Thanks for investigating this @TomDLT !So we remove explicit zeros and fix isomap?I don't know what we are supposed to do when the k-neighbors graph has disconnected components.We can either :[1]_ Nonlinear Dimensionality Reduction for Datawith Disconnected Neighborhood Graph.
Jicong Fan·Tommy W. S. Chow·Mingbo Zhao·John K. L. Ho, 2018 (5 citations)
[2]_ An Improved ISOMAP for Visualization and Classification of Multiple Manifolds
Wang. Hong-Yuan, Ding Xiu-Jie, Cheng Qi-Cai, and Chen Fu-Hua, 2013 (3 citations)I think we should raise an error for now.This PR would fix #14010 and #8352.We are hackily fixing the nearest neighbor graph in agglomerative clustering, right? Not saying that's the right thing to do, just wondering if consistency here is relevant.Indeed, in agglomerative clustering we fix the graph by linking each component to all the others, using each time the shortest possible connection. I guess that makes sense. That would be option 3.",Waiting for Reviewer module:manifold module:utils
https://github.com/scikit-learn/scikit-learn/issues/10158,"We just disabled warnings on travis in #9840.
I don't think we should do that. I've been a bit absent lately, but I think the current state of the warnings is pretty bad. Many of these seem recent changes that require cleanup.I think you are right. I look at the distribution of warnings removing --disable-pytest-warnings from setup.cfg and there you go:The top 10 offenders:The full thing: warnings.txtA lot of warnings are indeed coming from common tests:Okay. I've added hiding the ConvergenceWarnings there to the issue description.@lesteve I can see that the file you mention here has warnings of three types: UserWarning, RuntimeWarning, DeprecationWarning, also contains some ConvergenceWarning though.Do we need to modify all these three warnings to ConvergenceWarning?[Edit]: May be also contains FutureWarning.IMO the main priority is to figure out where the 3000+ warnings from test_common.py are coming from, and to silence them if we think it is fine to silence them. One example that jumps to mind: we may fit on some small data in test_common.py so that ignoring ConvergeWarning may be fine.I think @jnothman is right, we get convergence warnings because we change n_iter. We should probably just catch those.So we gota bunch of times, which is bad, since it means the code will actually break if we don't change it (but we still have 2 versions to fix it ;)#9379 added a whole bunch of new warnings...also pretty bad:looks like the common tests are clean now. looking for the worst offenders in the other tests nowSee #11608 and #11536 for the worst offenders.Updated:Some other ones are a bit more concerning I feel, though:I'm also confused by these:Shouldn't these be errors?I think we should try to avoid all that are not RuntimeWarning or ConvergenceWarning who are both numerical issues. The other warnings are UserWarnings mostly which means we made weird mistakes in setting up the tests.what needs to be done here at this point?I think someone need to do what I did in #10158 (comment) and reevaluate how many warnings are still there.From Andy's comments about worst offenders #11536 is still open (but you saw that since you commented on the issue).some new ones, yay! (this is from #8022 but master is similar)@thomasjpfan is this still relevant?@adrinjalali Yes, this feels like a never ending issue. Since we do not actively check for warnings, new warnings will always appear.We need a good way to see if a PR introduces new warnings or have a way to display the warnings a PR creates. Furthermore, on the master branch, we should have a way to see current warnings.In azure, after running pytest, we can create a separate step to just show the warnings. (It will do a little string processing of the pytest output and display only the warnings)if we could somehow save those warning as artifacts, then we could fail on the diff between the master's log and the PR's maybe?Why do we need to do string processing? Can't we just show the pytest output?We can show the pytest output, but it's the same as checking for pep8 issues on the whole codebase. We don't do it, we only fail if a PR introduces new issues.The issue is that we already do have tons of warnings, and it'd be too hard to try and remove them all, so the idea is to prevent PRs to introduce new warnings.Why is it too hard? I want to remove them all.Also I would argue we should maybe do the same for flake8, because we keep introducing unused imports. though lgtm might help us with that.I'm totally +1 on both warnings and flake8!",Easy help wanted
https://github.com/scikit-learn/scikit-learn/issues/13555,"Below is a proposal for a minor refactoring of BaseEstimator.get_params to make it easier to use in subclasses. Would like to hear your opinions :)
If it sounds like a sensible idea, I would be glad to throw a PR.Currently, invoking get_params from custom subclasses can produce an incomplete list of parameters, depending on how you define the subclass __init__.For example, if you have, say, the following class:and want to make some subclass from it. You might want to do something like:This however produces an incomplete parameter dict when calling get_params(), which can be surprising:You could work around it by copy-and-pasting the keyword arguments from the parent class and passing them via super():However, the parameters can be many, and this leads to code duplication.Refactor BaseEstimator.get_params so that client code has access to the parent class' __init__ and has more control over how to use it together with its own subclass parameters. Specifically:This would look like this:This way, client code could do something like this:I also thought of extracting the core get_params logic into _get_params_from so it could be reused easily in subclasses, e.g. feed it the parent __init__ names or the subclass __init__, or any other sequence of params that might have the same double underscore nested style.The subclass could implement its own _get_params_from but the user would need to replicate what the BaseEstimator.get_params already does, and might require keeping track of any API changes in sklearn (unlikely to happen?).Upon re-reading your question I see what you meant. I thought of separating them so you could inspect the param names with _get_param_names, but being able to process them with any logic you would like, not limited to the logic that I extracted in _get_params_from. But now that I think about it, if you want to inspect sklearn params you most likely will want to process them in the sklearn way, so there seems to be little need in separating. So, another version could be something like:@jnothman
Do you have any thoughts?@jnothman
No worries. I can ask again once the release is done :)@jnothman
Congrats on the new release :)
Sorry to bother you, but could we revisit this issue?(Pressed the wrong button when posting the comment. Sorry for the spam.)So this is mostly about allowing an estimator to delegate **kw constructor params to a super-class?This is mostly a question of whether we're happy about the use of super().__init__(**kw). I can certainly see cases where it's useful. **kw can be a big design problem if it's hard to know where and whether a param will be routed or whether, for instance, a spelling mistake will be silently ignored. As long as super classes have a listed set of params, that should be safe. Or even if super classes did something likewhich is basically what all our estimator __init__s are (or equiv kw = locals().copy(); del kw['self']; self.set_params(**kw)), set_params should guard against bad values. Anyway... that's some big picture stuff related to the error message that you have copied: ""scikit-learn estimators should always specify their parameters in the signature of their init (no varargs).""Back to your proposal: Is there any reason we should have _get_params be a method, rather than providing sklearn.utils.get_params_from_signature()?that last was a silly question. I see that a method works with inheritance.
This stuff:could even be removed if it was a classmethodYes, that's the gist.That's a good point which I didn't consider. I see why one wouldn't want sklearn's estimators to have **kw in their signature, but would you extend that restriction to custom classes of client code?Since there are some cases where **kw can be useful, I proposed to remove the classmethod implementation so you can pass to it a parent class instead of the bound class. This usage would be meant only for custom classes (if they prefer so); native sklearn classes stay the way they are with no **kw in their signatures (as they should).But isn't that what super().some_classmethod would do?Open a PR. I'd be in support of some abstraction here, particularly if it can remove some duplication between BaseEstimator and gaussian_process.kernels.Kernel... :)Sure thing. I'll see what I can do and throw a PR :)I forgot to answer your question above.Sadly, super().some_classmethod won't do that. @classmethod is essentially some_classmethod(type(obj), ...) so it will be bound to the object class. You can workaround it by first unbinding the method, but it's a bit hacky:Not that it matters now though, we now have to move _get_param_names out of the class anyway to reuse it in gaussian_process.kernels.Kernel .I'm working on the PR now.This may be a problem,
For Example, Inherient ContinuousOptimalBinning object of https://github.com/guillermo-navas-palencia/optbinning
May yield empty dict by call BaseEstimator’s get_params if set init as the first floor format.Another side effect of this API design is that it means any subclasses are heavily tied to a sklearn version. If an argument is added or removed in a sklearn version your subclass will have to implement custom logic to work out whether to pass it into the parent class if you intend to support both versions and you are forced to specify defaults which you have to manually keep in sync with the underlying sklearn library.",None yet
https://github.com/yaarith/ceph-telemetry/pull/1,"Hey @yaarith, I added the ceph upstream models to this repo, and updated the model.py script to use these by default.Marking this PR as WIP because there's one piece still missing - vendor info. Is it possible to get the ""vendor"", ""nvme_vendor"" and ""model_name"" keys from the smartctl jsons as well? These aren't used as inputs to models, but are needed to decide which model to use (models are vendor based atm). Could you please take a look? Sorry for not mentioning this requirement earlier.Edit: Addresses ceph#13@chauhankaranraj Great!!I pushed the changes you asked:
4c12cc2Please note that 'nvme_vendor' key was not added since it's the same as 'vendor'. That's because we retrieve the data from the database, where it was processed and filtered (as opposed to the raw SMART data in the client side, where 'vendor' and 'nvme_vendor' may contain different values).Let me know if you need anything :-)Thanks for the changes, @yaarith, I have updated the predictor :)@chauhankaranraj Great, many thanks!!I wish to test the integration, can you please add the updated requirements.txt?Of course, should be updated now.",None yet
https://github.com/scikit-multiflow/scikit-multiflow/pull/113,"Changes proposed in this pull request:Note that this PR also deprecate some existing classes, in order to encourage usage of streamz as the default way of handling streamingChecklistThis PR introduces new syntaxTravis fails due to the latest version of sklearn. I will update a temporary workaround as part of the other open PR. I will upload the actual fix later today.On the whole, I am impressed by how relatively easy it was to plug streamz into your workflow.I have left some comments for discussion.",None yet
https://github.com/scikit-learn/scikit-learn/issues/15994,"FutureWarning is not issued when using BaseNB as a baseclass but its __init__() is not called in the subclass, here: https://github.com/astroML/astroML/blob/master/astroML/classification/gmm_bayes.py#L15As the comment suggest in your deprecated decorator, overriding __new__ in the class decorator indeed solves this issue.I'm happy to open a PR with the fix.Also, relatedly, I wonder whether you would be interested in using a generic deprecation package instead. Basically we have the same functionality in astropy (I feel it's actually has more features e.g. this works there out of the box, it helps with arg renames/removals, etc.), there is also a deprecated decorator in matplotlib, and also a very basic one in numpy. I feel that having one for the wider ecosystem would be beneficial instead of the current system where we all roll our own.
At the numfocus summit I recall some interest from the mpl side, so I'm happy to get the ball rolling in this quoter if it's a thumb up from multiple projects.Darwin-17.7.0-x86_64-i386-64bit
Python 3.7.5 (default, Nov 1 2019, 02:16:38)
[Clang 10.0.0 (clang-1000.11.45.5)]
NumPy 1.19.0.dev0+63ef78b
SciPy 1.4.1
Scikit-Learn 0.23.dev0",module:utils
https://github.com/nilearn/nilearn/issues/471,"Y'all know numpy better than I do, but I have seen deprecation warnings in the past that make me worry about the following issue. I thought I'd post an issue to review things, because I've seen the warnings before and because the consequence can be not so good.In-place operations (-=, +=, *=, /=, maybe others?) have weird type-casting in numpy. Specifically [int array] -= [float] implicitly type-casts the float to int (and emits a warnings). This usually gives unexpected results, llke:Even thoughThe warning emitted is as follows, and should not be ignored:
DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=""unsafe"" if this is intentional.There are a number of places in the code where these operators are used, often like this:orIf signals or data could be of type int, these will do different things than expected. And again--I've seen these deprecation warnings before (but can't consistently reproduce them), so I'm unsure if this is a concern or not.Can someone confirm that this is not an issue? I just want to make sure...given that nifti files in sufficiently raw state are usually of integer
type, but more processed ones are often of type float, i see a potential
for weird/""inexplicable"" behavior cropping up from time to time. it will
become a tangible issue as soon as one of these phenomena causes a
discrepancy that is on the order of magnitude of the signal, and i don't
see why that shouldn't happen.probably a very good idea to keep a close eye on these warnings.On Sunday, March 1, 2015, Ben Cipollini notifications@github.com wrote:My (somewhat naive) recommendation would be to add a datatype assert anywhere there's this kind of in-place operation. There are only a few. They should be efficient, would potentially save some headaches, and would not be lost as an ignored error (i.e. if this is hit in tests, it's likely to be ignored).Indeed, these are a potential source of bug.In many places, the inplace operation is made to save memory, and we could have coded a non inplace operation. In these places, I wouldn't do an assert, which would raise errors when the user just wants to move forward. I would rather test if the data of float type, and convert if necessary, and for this I would use sklearn.utils.validation.as_float_array which will make sure to not cast to float64 when not necessary.In other places, maybe, the inplace is explicit: we are modifying an array that was given by the user, and the user is expecting it to be modified (typically when copy=False is given). In such places, I think we should raise an error.",None yet
https://github.com/scikit-learn/scikit-learn/issues/14953,"In sklearn.preprocessing._encoders._BaseEncoder, columns with pd.Categorical dtype are converted to arrays.If the categories ordering is explicitly specified by the user to the constructor of OneHotEncoder or OrdinalEncoder, then this is fine... but if 'auto' is used, lexicographic ordering will be assumed, disregarding the encoding order determined by the Categorical dtype.I propose that we raise a warning if:The warning might be something like UserWarning(""'auto' categories is used, but the Categorical dtype provided is not consistent with the automatic lexicographic ordering"")... or else something more intelligible.We may change this warning to a FutureWarning with ""From version 0.24 the category ordering specified by a Categorical dtype will be respected in encoders.""@jnothman Hi, I am a bit new to open source and would like to work on this issue, if available.Let me know if I got things correct,if any of the above point hold true we need to raise warning.Few queries,Few queries in general, if you don't mind answering them -Thanks.@jnothman Thanks.
Just one more thing the last pointDo we have lexicographically sorted feature list? And what is feature's dtype.categories (is it the self.categories_ list that we are finding?)Thanks.@jnothman Hey, I thought of doing it as -
In the lines above in line 85, within the if condition we can just check whether the other two conditions are true, that is whether X_list[i] is a categorical dtype and then we can check cats variable has same order as that of X_list[i]'s dtype.category.Is it a feasible approach?Thanks.I agree with the proposed behavior, but I doubt this issue will be easy ;) [also this issue is tagged easy and moderate?]@amueller Thanks for the heads up, I won't be opening any PR until I make sure that the approach is correct and get it reviewed by you guys.And from your reply I think that this approach won't be any good. Still working on it.Thanks.@jnothman I was going through the check_array function, please correct me if wrong, the function converts any type of object passed to it to an array of features with string values.If, so I think we need to verify the three conditions to raise the warning in the check_array function itself.Thanks.It would be nice to have support for this now and not wait till 0.24. Is adding option in categories a good idea? (I do not have a good idea what this new option would be called. categories=use-pandas-categories-if-avaliable?)This duplicates #12086 somewhat, there is some discussion there as well (but basically what @jnothman is proposing here to introduce a warning for it). Will close the other issue.I agree with @thomasjpfan that it would be nice to already have the new behaviour now, but I was wondering if we cannot combine that we the changes in OrdinalEncoder for strings with categories='sort' / 'auto' ? For example, if people specify categories='auto', we use the order of the categorical dtype, if categories='sort', we lexicographically sort them (the current behaviour).
That way it could also be possible to raise a warning in the default case, but having a way for users to both have the new behaviour or keep the old behaviour and silence the warning.BTW, I don't think that the actual implementation to use the categorical dtype's categories should be very hard (there is actually a PR for this: #13351), as the required preparatory work to handle a DataFrame column by column is already done (#13253).The question is also: what default behaviour do we want in the long run?
Personally, I think the default should become to use the categorical's dtype ordering/categories. If we want that, having this behaviour for categories='auto' (instead of a new categories='dtype') might be nicer.EDIT: hmm, of course what I am forgetting is that users can already explicitly do categories='auto' right now. So eg changing the actual default to categories=None (meaning the old default), so we can raise a warning if needed and point users towards specifying categories='auto'/'sort' to choose explicit behaviour will still introduce a breaking change for those who already explicitly passed that keyword ..In the long run I would want auto to mean dtype. The question is how we get there. If we want to maintain backward compatibility, we need to warn that 'auto' will now respect categorical dtypes in 0.24. With this warning, it would be nice to say that ""you can use the categorical='dtype' option now to enable this behavior now. When 'dtype' becomes the new 'auto', we would need to deprecated 'dtype' because it means the same as 'auto'.(This is a fairly long deprecation path)Yes, I was mainly trying to think if we can't do it with a shorter path, without having a new option that afterwards becomes obsolete. But yeah, as edited my comment above, it's difficult to do that in a fully backwards compatible way ..We only want to warn when the Categorical dtype is ordered right? Categories aren't necessarily ordered and this is actually pandas' default.The current PR #15396 warns when categories aren't ordered which seems wrong to me.(Sorry if this has been previously discussed)pandas uses the lexicon order for its encoding when the categorical dtype is unordered, so it so happens that this is okay. Although I agree we should not rely on this, and warn when the category is ordered.Ordinal categories are far less common than pure nominal categories, so IMHO the pandas default makes sense, and we would be warning for no good reason in most cases.Why would you want to warn something about the order when there is no order in the first place?Sure, but then:... which makes absolutely no sense unless those strings are orderedI think that #14984, #15050, and #15396 might not be blockers for 0.22 and I would move them for 0.23.I think that it could be great to have a single issue (superseded #14953, #14954) to discuss the overall behaviour for categories in OneHotEncoder and OrdinalEncoder and from there having several PRs which follows the discussed proposals.@NicolasHug I would not be so sure about this. Trees can cope with
random orders provided you make them deep enough.True but trees aren't always deep, typically in GBTo reproduce a OHE split using OE, you would need in the worst case C - 1 splits. That's not negligible when the OHE splits multiple time on the same feature during fitting. And because of the arbitrary order, you might just not ever consider such a split because the gain is too low.The right way to handle nominal categories in trees is still to use a OHE, or to natively support categories like the nocats PRs.In any case, going back to the original issue, my concern here is that the current proposal is to raise an order-related warning even when there is actually no order, which I think will just confuse / frustrate users.Also note that you can have a pandas Categorical with a specific order without having it ""ordered"" (it the sense of the ordered=False/True attribute.Eg:So when people are passing this to a OrdinalEncoder, they might actually be doing it ""correctly"" already, even though it is not an ""ordered"" categorical, it just happens to have its categories in the sensible (non lexico) order.",Moderate
https://github.com/vaexio/vaex/pull/927,"Requested in #920 .This PR adds support for obtaining probabilities and log probabilities from sklearn classifiers.
The Predictor and IncrementalPredictor classes now have a prediction_type kwarg, which can be set to predict (default), predict_proba and predict_log_proba. Each option will trigger the appropriate method in the sklearn classifier at prediction time.Note: in this PR we are also removing a deprecated class (vaex.ml.sklearn.SKLearnPredictor), as it's deprecation period has expired.Checklist:great work, nice clean commits",None yet
https://github.com/Teichlab/cellphonedb/issues/140,"Hi,
I have been trying to run cellphonedb but I am having an error.
I am adding below the command line, outputs and details of Input.
Please let me know what to change.
Thanks in advance,
Juliecellphonedb method analysis pah.subset.metada.cpdb.txt pah.subset.count.cpdb.txt --counts-data gene_name/home/jrodor2/cpdb-venv/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.
warnings.warn(message, FutureWarning)
[ ][APP][21/01/20-15:24:00][WARNING] Latest local available version is v2.0.0, using it
[ ][APP][21/01/20-15:24:00][WARNING] User selected downloaded database v2.0.0 is available, using it
[ ][CORE][21/01/20-15:24:00][INFO] Initializing SqlAlchemy CellPhoneDB Core
[ ][CORE][21/01/20-15:24:00][INFO] Using custom database at /home/jrodor2/.cpdb/releases/v2.0.0/cellphone.db
[ ][APP][21/01/20-15:24:00][INFO] Launching Method cpdb_analysis_local_method_launcher
[ ][APP][21/01/20-15:24:00][INFO] Launching Method _set_paths
[ ][APP][21/01/20-15:24:00][INFO] Launching Method _load_meta_counts
[ ][CORE][21/01/20-15:24:01][INFO] Launching Method cpdb_method_analysis_launcher
[ ][CORE][21/01/20-15:24:01][INFO] Launching Method _counts_validations
[ ][CORE][21/01/20-15:24:01][INFO] [Non Statistical Method] Threshold:0.1 Precission:3
[ ][CORE][21/01/20-15:24:01][INFO] Running Simple Prefilters
[ ][CORE][21/01/20-15:24:02][INFO] [Non Statistical Method] Threshold:0.1 Precision:3
[ ][CORE][21/01/20-15:24:02][INFO] Running Complex Prefilters
[ ][APP][21/01/20-15:24:02][ERROR] Unexpected error
Traceback (most recent call last):
File ""/home/jrodor2/cpdb-venv/lib/python3.5/site-packages/cellphonedb/src/api_endpoints/terminal_api/method_terminal_api_endpoints/method_terminal_commands.py"", line 207, in analysis
subsampler,
File ""/home/jrodor2/cpdb-venv/lib/python3.5/site-packages/cellphonedb/src/local_launchers/local_method_launcher.py"", line 98, in cpdb_analysis_local_method_launcher
subsampler)
File ""/home/jrodor2/cpdb-venv/lib/python3.5/site-packages/cellphonedb/src/core/methods/method_launcher.py"", line 113, in cpdb_method_analysis_launcher
result_precision)
File ""/home/jrodor2/cpdb-venv/lib/python3.5/site-packages/cellphonedb/src/core/methods/cpdb_analysis_method.py"", line 41, in call
deconvoluted.drop_duplicates(inplace=True)
File ""/home/jrodor2/cpdb-venv/lib/python3.5/site-packages/pandas/core/frame.py"", line 4331, in drop_duplicates
duplicated = self.duplicated(subset, keep=keep)
File ""/home/jrodor2/cpdb-venv/lib/python3.5/site-packages/pandas/core/frame.py"", line 4385, in duplicated
labels, shape = map(list, zip(*map(f, vals)))
ValueError: not enough values to unpack (expected 2, got 0)Cell cluster
pah3_CGGACACGTTGAGTTC c3
contA_CACACTCTCTGTTTGT c1
contA_CTCGTCATCCGTCAAA c7
pah3_CTCGAGGGTTTGACAC c0
pah2_TCTCATAAGCGTTCCG c0
pah2_GAGCAGATCCAAGCCG c3
pah2_GGCTGGTGTTACAGAA c0
contB_TCAGCAACATATGGTC c1
pah1_GCACATATCAAGGTAA c3Gene pah3_CGGACACGTTGAGTTC contA_CACACTCTCTGTTTGT contA_CTCGTCATCCGTCAAA pah3_CTCGAGGGTTTGACAC
Rp1 0 0 0 0
Sox17 0 0 0 2.74082443266566
Mrpl15 0 0 0 0
Lypla1 0 0 0 0
Gm37988 0 0 0 0
Tcea1 1.65068087096815 0 1.2849200801299 0
Atp6v1h 0 0 0 1.76357478287235
Rb1cc1 0 0 0 0
4732440D04Rik 0 0 0 0Hi @julierodor,Can you send me the subset files? Here it seems like you are using spaces instead off tabs as separator character.BestHello!
I also faced exactly same error, although I have used the files with tab-deliminated txt files. Do you have any further recommendations?Thanks!",None yet
https://github.com/imoscovitz/wittgenstein/issues/9,"@flamby,Just wanted to create a central checklist with the last remaining things since we're very close to finishing V0.2.Let me know what you think!catnap speed optimization #4:Provide np and iterable support for train/predict/score/recalibrateFlexible predict dataset featuresPos class namingPredict proba, recalibrate proba #2:Metaclassifier compatibility #7:Other outstanding strangeness:Update readme/description/docstringsHi @imoscovitzGood idea.Yep, predict speed could be improved. That explains why prediction is slower than training then ;-)
BTW I've tested a third-party binning library (the one built-in sklearn HistGradientBoostingClassifier) with IREP.I'll do in a couple of days a pull request demonstrating it in an unittest file.
I've already these tests based on the sklearn dataset I used last time :Most of them works like a charm with IREP, but not RIPPER, but I guess you are aware of that since recent changes were focused on IREP only (n_discretize_bins not in __init__ for instance). But I spotted several cases where those tests break with IREP, and they are probably related to the dataset I chose, which highlights the need of more warnings I guess.Yes, definitely more testing, as my current unittest creation demonstrate lots of small bugs if you somehow change default parameters here and there.That's interesting. For now, I observed only better accuracy using what we can now call indeed flexible predict dataset features. Maybe that's just your bug ;-)Thanks!Yep, testing, definitely ;-)I'll give VotingClassifier another try and keep you updated.I would vote for moving them completely. It's not a big deal to follow the new conventions.I'm glad you noticed too ;-) That puzzle me since a while.Yes, a wittgenstein.readthedocs.io site would be great, and some refreshed notebooks.
I could help on that.Hi @imoscovitz
first, thanks for your Wittgenstein Lib. Do you know when you will release the new version?",None yet
https://github.com/kr-colab/shIC/issues/1,"Dear Daniel,sorry to bother you with this, but I'm trying to run your pipeline in the shIC_pipelin.sh script and I'm running into the issue below. Does this look familiar to you?I'm hoping to use your pipeline to classify regions near color pattern genes in >30 Heliconius butterfly populations.Many thanks for any help!Stevenerror:python trainClassifier.py combinedTrainingSetsTennessenEuro/ classifiers/tennessenEuro/tennessenEuroAutosomal.p all
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
""This module will be removed in 0.20."", DeprecationWarning)
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
DeprecationWarning)
using these features: ['all'] (indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121])
training set size after balancing: 5000
Checking accuracy when distinguishing among all 5 classes
Training extraTreesClassifier
Traceback (most recent call last):
File ""trainClassifier.py"", line 102, in
grid_search.fit(X, y)
File ""/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py"", line 838, in fit
return self._fit(X, y, ParameterGrid(self.param_grid))
File ""/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py"", line 574, in _fit
for parameters in parameter_iterable
File ""/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 789, in call
self.retrieve()
File ""/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 740, in retrieve
raise exception
sklearn.externals.joblib.my_exceptions.JoblibValueError: JoblibValueErrorMultiprocessing exception:
...........................................................................
/rds/user/sv378/hpc-work/Programs/shIC/trainClassifier.py in ()
97
98 heatmap = []
99 sys.stderr.write(""Training %s\n"" %(mlType))
100 grid_search = GridSearchCV(clf,param_grid=param_grid_forest,cv=10,n_jobs=20)
101 start = time()
--> 102 grid_search.fit(X, y)
103 sys.stderr.write(""GridSearchCV took %.2f seconds for %d candidate parameter settings.\n""
104 % (time() - start, len(grid_search.grid_scores_)))
105 print ""Results for %s"" %(mlType)
106 report(grid_search.grid_scores_)...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=10, error_score='raise',
...='2n_jobs', refit=True, scoring=None, verbose=0), X=[[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], y=['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...])
833 y : array-like, shape = [n_samples] or [n_samples, n_output], optional
834 Target relative to X for classification or regression;
835 None for unsupervised learning.
836
837 """"""
--> 838 return self._fit(X, y, ParameterGrid(self.param_grid))
self._fit = <bound method GridSearchCV._fit of GridSearchCV(...'2n_jobs', refit=True, scoring=None, verbose=0)>
X = [[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...]
y = ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...]
self.param_grid = {'bootstrap': [True, False], 'criterion': ['gini', 'entropy'], 'max_depth': [3, 10, None], 'max_features': [1, 3, 11, 121], 'min_samples_leaf': [1, 3, 10], 'min_samples_split': [1, 3, 10]}
839
840
841 class RandomizedSearchCV(BaseSearchCV):
842 """"""Randomized search on hyper parameters............................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=10, error_score='raise',
...='2*n_jobs', refit=True, scoring=None, verbose=0), X=[[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], y=['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...], parameter_iterable=<sklearn.grid_search.ParameterGrid object>)
569 )(
570 delayed(fit_and_score)(clone(base_estimator), X, y, self.scorer,
571 train, test, self.verbose, parameters,
572 self.fit_params, return_parameters=True,
573 error_score=self.error_score)
--> 574 for parameters in parameter_iterable
parameters = undefined
parameter_iterable = <sklearn.grid_search.ParameterGrid object>
575 for train, test in cv)
576
577 # Out is a list of triplet: score, estimator, n_test_samples
578 n_fits = len(out)...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in call(self=Parallel(n_jobs=20), iterable=<generator object >)
784 if pre_dispatch == ""all"" or n_jobs == 1:
785 # The iterable was consumed all at once by the above for loop.
786 # No need to wait for async callbacks to trigger to
787 # consumption.
788 self._iterating = False
--> 789 self.retrieve()
self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=20)>
790 # Make sure that we get a last message telling us we are done
791 elapsed_time = time.time() - self._start_time
792 self._print('Done %3i out of %3i | elapsed: %s finished',
793 (len(self._output), len(self._output),ValueError Mon Feb 19 15:00:55 2018
PID: 233604 Python 2.7.14: /home/sv378/anaconda2/bin/python
...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in call(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
126 def init(self, iterator_slice):
127 self.items = list(iterator_slice)
128 self._size = len(self.items)
129
130 def call(self):
--> 131 return [func(*args, **kwargs) for func, args, kwargs in self.items]
func =
args = (ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), [[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...], , array([ 100, 101, 102, ..., 4997, 4998, 4999]), array([ 0, 1, 2, 3, 4, 5, 6,...4093, 4094,
4095, 4096, 4097, 4098, 4099]), 0, {'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 1}, {})
kwargs = {'error_score': 'raise', 'return_parameters': True}
self.items = [(, (ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), [[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...], , array([ 100, 101, 102, ..., 4997, 4998, 4999]), array([ 0, 1, 2, 3, 4, 5, 6,...4093, 4094,
4095, 4096, 4097, 4098, 4099]), 0, {'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 1}, {}), {'error_score': 'raise', 'return_parameters': True})]
132
133 def len(self):
134 return self._size
135...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py in _fit_and_score(estimator=ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), X=[[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], y=['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...], scorer=, train=array([ 100, 101, 102, ..., 4997, 4998, 4999]), test=array([ 0, 1, 2, 3, 4, 5, 6,...4093, 4094,
4095, 4096, 4097, 4098, 4099]), verbose=0, parameters={'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 1}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')
1670
1671 try:
1672 if y_train is None:
1673 estimator.fit(X_train, **fit_params)
1674 else:
-> 1675 estimator.fit(X_train, y_train, **fit_params)
estimator.fit =
X_train = [[0.108771151419, 0.105614197748, 0.0966494789241, 0.0866170007457, 0.0964047576502, 0.11930815248, 0.0665383993874, 0.0841112601666, 0.051475049271, 0.068146231684, 0.116364320525, 0.105090670745, 0.093292549705, 0.106620056806, 0.0873934891851, 0.0941664845969, 0.117325759231, 0.0694778239021, 0.0793095914354, 0.0747214332532, ...], [0.106617909874, 0.106357049024, 0.0765518196138, 0.147184749999, 0.0858705240986, 0.0847635617398, 0.0850937259626, 0.111091967911, 0.0773122555785, 0.0487853243806, 0.0703711118193, 0.0942956926659, 0.108265424913, 0.0791618160652, 0.136204889406, 0.0977881257276, 0.0849825378347, 0.0861466821886, 0.0966239813737, 0.0791618160652, ...], [0.185416168109, 0.100101261529, 0.133528111996, 0.101465662428, 0.0703716746745, 0.102758077764, 0.0812441134372, 0.0988210729473, 0.0435087624933, 0.0382121103195, 0.0445729843021, 0.141348497157, 0.103980503656, 0.10804224208, 0.0909829406986, 0.0601137286759, 0.0999187652315, 0.072298943948, 0.116978066613, 0.0674248578392, ...], [0.0963637088631, 0.121625351917, 0.0956768527573, 0.189534202703, 0.10684804642, 0.0715540664797, 0.123586319996, 0.0487971734638, 0.0425811912908, 0.0475143202527, 0.055918765857, 0.106206438765, 0.108861599734, 0.0932625290408, 0.138400265516, 0.0945901095254, 0.081978094922, 0.103883172917, 0.0640557583804, 0.0640557583804, ...], [0.0801096133575, 0.071895449447, 0.0685534174348, 0.0896877135845, 0.108408700517, 0.0759894373889, 0.111760084293, 0.114561218587, 0.0846313318665, 0.0917518547716, 0.102651178752, 0.0948412698413, 0.0845238095238, 0.068253968254, 0.0936507936508, 0.1, 0.0845238095238, 0.109920634921, 0.1, 0.0809523809524, ...], [0.0382561737825, 0.0315757277579, 0.0376576988282, 0.0861285452859, 0.143679577183, 0.0924431361215, 0.086804250971, 0.105428327609, 0.117157492386, 0.134981244139, 0.125887825935, 0.0738862730895, 0.0590365809489, 0.0684534588917, 0.102136906918, 0.114813473379, 0.0804056501268, 0.0843897138718, 0.0876494023904, 0.105396595436, ...], [0.0784746826308, 0.1006413305, 0.105294687454, 0.0729785868519, 0.0657225365574, 0.110274033891, 0.109100079816, 0.0715028124597, 0.116002698888, 0.0796512495891, 0.0903573013613, 0.0795935647756, 0.0897544453853, 0.106689246401, 0.0838272650296, 0.0804403048264, 0.104149026249, 0.106689246401, 0.0762066045724, 0.0973751058425, ...], [0.0920064369969, 0.0767766409783, 0.088577875781, 0.103323636964, 0.0708319990492, 0.102821897842, 0.0771736055954, 0.0911898776385, 0.115921754556, 0.113968901938, 0.0674073726602, 0.10502283105, 0.0896118721461, 0.0816210045662, 0.087899543379, 0.0810502283105, 0.0958904109589, 0.074200913242, 0.0816210045662, 0.101598173516, ...], [0.113920327585, 0.104225241968, 0.121645767516, 0.072213302433, 0.130757972587, 0.089292624932, 0.0656439314145, 0.101428447263, 0.0763251337636, 0.0631624875531, 0.0613847629852, 0.0898410504492, 0.109882515549, 0.112646855563, 0.0822391154112, 0.108500345543, 0.0746371803732, 0.0711817553559, 0.0981340704907, 0.0988251554941, ...], [0.10219007675, 0.0723602246041, 0.0961970951707, 0.116814267704, 0.148062298638, 0.0827725938389, 0.125494140016, 0.0514917907593, 0.0651889033509, 0.05285931811, 0.0865692910589, 0.0786445012788, 0.0620204603581, 0.0895140664962, 0.105498721228, 0.128516624041, 0.0773657289003, 0.106138107417, 0.0818414322251, 0.0914322250639, ...], [0.080177548404, 0.0931795868809, 0.0999111444437, 0.118483345352, 0.091940873078, 0.083312789863, 0.0522401079175, 0.106566192871, 0.0585826455211, 0.107876963363, 0.107728802306, 0.0743801652893, 0.0914256198347, 0.107438016529, 0.114669421488, 0.0847107438017, 0.0888429752066, 0.0712809917355, 0.0976239669421, 0.0686983471074, ...], [0.113891144206, 0.0866807008977, 0.0548496521406, 0.0900785333581, 0.0950678237314, 0.0814450026369, 0.0838627610583, 0.098519913327, 0.140039732117, 0.0894909660789, 0.0660737704478, 0.117040630685, 0.0824742268041, 0.0651910248636, 0.0779260157671, 0.0921770770164, 0.089144936325, 0.0876288659794, 0.0885385081868, 0.11522134627, ...], [0.0381077645694, 0.0974157216961, 0.0950881655299, 0.092574894648, 0.130234317646, 0.108731554031, 0.105156453529, 0.0993667783061, 0.0648689912665, 0.106401315243, 0.0620540435341, 0.076511861009, 0.0935516204477, 0.0898763782158, 0.0968927497494, 0.11693952556, 0.0992315402606, 0.0928833945874, 0.0838623454728, 0.0825258937521, ...], [0.0805225473236, 0.0825788214013, 0.0774580674909, 0.0505955135195, 0.0852763226211, 0.093171130758, 0.14737804487, 0.0900577192716, 0.0697143905999, 0.108670714035, 0.114576728109, 0.0880964866282, 0.0980597797588, 0.0880964866282, 0.0608285264814, 0.0844257996854, 0.102254850551, 0.126900891453, 0.0859989512323, 0.0676455165181, ...], [0.0545096490945, 0.0854380096443, 0.0590723053012, 0.11268847762, 0.11237593458, 0.148140575669, 0.0777969797628, 0.0578052877306, 0.114498669096, 0.0769023412346, 0.100771770267, 0.100886162236, 0.0804362644853, 0.0688479890934, 0.091342876619, 0.0940695296524, 0.12406271302, 0.105657805044, 0.0743012951602, 0.0879345603272, ...], [0.0793137451288, 0.0971837591699, 0.112239651189, 0.0960395146984, 0.107311937923, 0.12176252675, 0.0855251726555, 0.0610570054797, 0.0502425432012, 0.0688468299037, 0.120477313901, 0.0880999342538, 0.0878807801885, 0.104098181021, 0.0973044049967, 0.10650887574, 0.115713346483, 0.0913872452334, 0.0721016874863, 0.0600482138944, ...], [0.098928448476, 0.137305549245, 0.0753791252916, 0.0990566805978, 0.0822124178289, 0.0910533865423, 0.0892351234243, 0.1018692375, 0.107928239779, 0.0444253340597, 0.0726064572547, 0.108493310064, 0.112274578243, 0.086678301338, 0.0933682373473, 0.0796974985457, 0.087550901687, 0.0767888307155, 0.0983129726585, 0.108493310064, ...], [0.123070473061, 0.0578929142681, 0.0464503220138, 0.201737887342, 0.0855492777249, 0.0536180031335, 0.111344533931, 0.10607667333, 0.0519348097508, 0.0775904494795, 0.084734655966, 0.118683901293, 0.0708969839405, 0.0642381511947, 0.142969056013, 0.087348217783, 0.0708969839405, 0.104582843713, 0.0967489228359, 0.0771641206424, ...], [0.0908112608184, 0.086731479801, 0.106727761089, 0.116224958469, 0.0746198778934, 0.100064341346, 0.0903571333521, 0.0540778404843, 0.0883506852995, 0.0824550664772, 0.109579594971, 0.0942684766214, 0.10407239819, 0.0942684766214, 0.0972850678733, 0.077677224736, 0.0852187028658, 0.0980392156863, 0.0761689291101, 0.0897435897436, ...], [0.0946723497621, 0.102614036567, 0.104068459189, 0.104233398688, 0.0692101472815, 0.10136195871, 0.0758868780611, 0.100250906356, 0.0760499812908, 0.0793837453837, 0.092268138711, 0.0808823529412, 0.104411764706, 0.105882352941, 0.107352941176, 0.0801470588235, 0.0911764705882, 0.0727941176471, 0.0948529411765, 0.0860294117647, ...], ...]
y_train = ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...]
fit_params = {}
1676
1677 except Exception as e:
1678 if error_score == 'raise':
1679 raise...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.py in fit(self=ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), X=array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), y=array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), sample_weight=None)
323 trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
324 backend=""threading"")(
325 delayed(parallel_build_trees)(
326 t, self, X, y, sample_weight, i, len(trees),
327 verbose=self.verbose, class_weight=self.class_weight)
--> 328 for i, t in enumerate(trees))
i = 99
329
330 # Collect newly grown trees
331 self.estimators.extend(trees)
332...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in call(self=Parallel(n_jobs=1), iterable=<generator object >)
774 self.n_completed_tasks = 0
775 try:
776 # Only set self._iterating to True if at least a batch
777 # was dispatched. In particular this covers the edge
778 # case of Parallel used with an exhausted iterator.
--> 779 while self.dispatch_one_batch(iterator):
self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
iterator = <generator object >
780 self._iterating = True
781 else:
782 self._iterating = False
783...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object >)
620 tasks = BatchedCalls(itertools.islice(iterator, batch_size))
621 if len(tasks) == 0:
622 # No more tasks available in the iterator: tell caller to stop.
623 return False
624 else:
--> 625 self._dispatch(tasks)
self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
626 return True
627
628 def _print(self, msg, msg_args):
629 """"""Display the message on stout or stderr depending on verbosity""""""...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
583 self.n_dispatched_tasks += len(batch)
584 self.n_dispatched_batches += 1
585
586 dispatch_timestamp = time.time()
587 cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588 job = self._backend.apply_async(batch, callback=cb)
job = undefined
self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
589 self._jobs.append(job)
590
591 def dispatch_next(self):
592 """"""Dispatch more data for parallel processing...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
106 raise ValueError('n_jobs == 0 in Parallel has no meaning')
107 return 1
108
109 def apply_async(self, func, callback=None):
110 """"""Schedule a func to be run""""""
--> 111 result = ImmediateResult(func)
result = undefined
func = <sklearn.externals.joblib.parallel.BatchedCalls object>
112 if callback:
113 callback(result)
114 return result
115...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py in init(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
327
328 class ImmediateResult(object):
329 def init(self, batch):
330 # Don't delay the application, to avoid keeping the input
331 # arguments in memory
--> 332 self.results = batch()
self.results = undefined
batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
333
334 def get(self):
335 return self.results
336...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in call(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
126 def init(self, iterator_slice):
127 self.items = list(iterator_slice)
128 self._size = len(self.items)
129
130 def call(self):
--> 131 return [func(*args, **kwargs) for func, args, kwargs in self.items]
func =
args = (ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), None, 0, 100)
kwargs = {'class_weight': None, 'verbose': 0}
self.items = [(, (ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]
132
133 def len(self):
134 return self._size
135...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), forest=ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), X=array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), y=array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)
116 warnings.simplefilter('ignore', DeprecationWarning)
117 curr_sample_weight *= compute_sample_weight('auto', y, indices)
118 elif class_weight == 'balanced_subsample':
119 curr_sample_weight *= compute_sample_weight('balanced', y, indices)
120
--> 121 tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
tree.fit = <bound method ExtraTreeClassifier.fit of ExtraTr...om_state=820678124,
splitter='random')>
X = array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32)
y = array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]])
sample_weight = None
curr_sample_weight = array([ 2., 0., 2., ..., 1., 1., 2.])
122 else:
123 tree.fit(X, y, sample_weight=sample_weight, check_input=False)
124
125 return tree...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.py in fit(self=ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), X=array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), y=array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), sample_weight=array([ 2., 0., 2., ..., 1., 1., 2.]), check_input=False, X_idx_sorted=None)
785
786 super(DecisionTreeClassifier, self).fit(
787 X, y,
788 sample_weight=sample_weight,
789 check_input=check_input,
--> 790 X_idx_sorted=X_idx_sorted)
X_idx_sorted = None
791 return self
792
793 def predict_proba(self, X, check_input=True):
794 """"""Predict class probabilities of the input samples X............................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.py in fit(self=ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), X=array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), y=array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), sample_weight=array([ 2., 0., 2., ..., 1., 1., 2.]), check_input=False, X_idx_sorted=None)
189 if isinstance(self.min_samples_split, (numbers.Integral, np.integer)):
190 if not 2 <= self.min_samples_split:
191 raise ValueError(""min_samples_split must be an integer ""
192 ""greater than 1 or a float in (0.0, 1.0]; ""
193 ""got the integer %s""
--> 194 % self.min_samples_split)
self.min_samples_split = 1
195 min_samples_split = self.min_samples_split
196 else: # float
197 if not 0. < self.min_samples_split <= 1.:
198 raise ValueError(""min_samples_split must be an integer ""ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1python trainClassifier.py combinedTrainingSetsTennessenEuro/ classifiers/tennessenEuro/tennessenEuroAutosomal.p all
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
""This module will be removed in 0.20."", DeprecationWarning)
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
DeprecationWarning)
using these features: ['all'] (indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121])
training set size after balancing: 5000
Checking accuracy when distinguishing among all 5 classes
Training extraTreesClassifier
Traceback (most recent call last):
File ""trainClassifier.py"", line 102, in
grid_search.fit(X, y)
File ""/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py"", line 838, in fit
return self._fit(X, y, ParameterGrid(self.param_grid))
File ""/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py"", line 574, in _fit
for parameters in parameter_iterable
File ""/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 789, in call
self.retrieve()
File ""/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 740, in retrieve
raise exception
sklearn.externals.joblib.my_exceptions.JoblibValueError: JoblibValueErrorMultiprocessing exception:
...........................................................................
/rds/user/sv378/hpc-work/Programs/shIC/trainClassifier.py in ()
97
98 heatmap = []
99 sys.stderr.write(""Training %s\n"" %(mlType))
100 grid_search = GridSearchCV(clf,param_grid=param_grid_forest,cv=10,n_jobs=20)
101 start = time()
--> 102 grid_search.fit(X, y)
103 sys.stderr.write(""GridSearchCV took %.2f seconds for %d candidate parameter settings.\n""
104 % (time() - start, len(grid_search.grid_scores_)))
105 print ""Results for %s"" %(mlType)
106 report(grid_search.grid_scores_)...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=10, error_score='raise',
...='2n_jobs', refit=True, scoring=None, verbose=0), X=[[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], y=['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...])
833 y : array-like, shape = [n_samples] or [n_samples, n_output], optional
834 Target relative to X for classification or regression;
835 None for unsupervised learning.
836
837 """"""
--> 838 return self._fit(X, y, ParameterGrid(self.param_grid))
self._fit = <bound method GridSearchCV._fit of GridSearchCV(...'2n_jobs', refit=True, scoring=None, verbose=0)>
X = [[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...]
y = ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...]
self.param_grid = {'bootstrap': [True, False], 'criterion': ['gini', 'entropy'], 'max_depth': [3, 10, None], 'max_features': [1, 3, 11, 121], 'min_samples_leaf': [1, 3, 10], 'min_samples_split': [1, 3, 10]}
839
840
841 class RandomizedSearchCV(BaseSearchCV):
842 """"""Randomized search on hyper parameters............................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=10, error_score='raise',
...='2*n_jobs', refit=True, scoring=None, verbose=0), X=[[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], y=['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...], parameter_iterable=<sklearn.grid_search.ParameterGrid object>)
569 )(
570 delayed(fit_and_score)(clone(base_estimator), X, y, self.scorer,
571 train, test, self.verbose, parameters,
572 self.fit_params, return_parameters=True,
573 error_score=self.error_score)
--> 574 for parameters in parameter_iterable
parameters = undefined
parameter_iterable = <sklearn.grid_search.ParameterGrid object>
575 for train, test in cv)
576
577 # Out is a list of triplet: score, estimator, n_test_samples
578 n_fits = len(out)...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in call(self=Parallel(n_jobs=20), iterable=<generator object >)
784 if pre_dispatch == ""all"" or n_jobs == 1:
785 # The iterable was consumed all at once by the above for loop.
786 # No need to wait for async callbacks to trigger to
787 # consumption.
788 self._iterating = False
--> 789 self.retrieve()
self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=20)>
790 # Make sure that we get a last message telling us we are done
791 elapsed_time = time.time() - self._start_time
792 self._print('Done %3i out of %3i | elapsed: %s finished',
793 (len(self._output), len(self._output),ValueError Mon Feb 19 15:00:55 2018
PID: 233604 Python 2.7.14: /home/sv378/anaconda2/bin/python
...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in call(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
126 def init(self, iterator_slice):
127 self.items = list(iterator_slice)
128 self._size = len(self.items)
129
130 def call(self):
--> 131 return [func(*args, **kwargs) for func, args, kwargs in self.items]
func =
args = (ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), [[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...], , array([ 100, 101, 102, ..., 4997, 4998, 4999]), array([ 0, 1, 2, 3, 4, 5, 6,...4093, 4094,
4095, 4096, 4097, 4098, 4099]), 0, {'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 1}, {})
kwargs = {'error_score': 'raise', 'return_parameters': True}
self.items = [(, (ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), [[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...], , array([ 100, 101, 102, ..., 4997, 4998, 4999]), array([ 0, 1, 2, 3, 4, 5, 6,...4093, 4094,
4095, 4096, 4097, 4098, 4099]), 0, {'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 1}, {}), {'error_score': 'raise', 'return_parameters': True})]
132
133 def len(self):
134 return self._size
135...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py in _fit_and_score(estimator=ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), X=[[0.0785765713796, 0.0711224669757, 0.0534461846636, 0.11298676124, 0.107917943941, 0.101909772587, 0.0901051202811, 0.0651605926738, 0.107920962989, 0.10713828059, 0.10371534268, 0.0868028279654, 0.0738413197172, 0.0652003142184, 0.106834249804, 0.106441476826, 0.0856245090338, 0.0934799685782, 0.0777690494894, 0.0926944226237, ...], [0.0973934121939, 0.103414595071, 0.0582038016651, 0.0722162925787, 0.101465160151, 0.096078447283, 0.0956982685818, 0.124639279001, 0.0882124475236, 0.0822871796707, 0.08039111628, 0.0980584910297, 0.11231260752, 0.0646350454657, 0.078397640698, 0.0857704595724, 0.0916687146719, 0.0948636028508, 0.107151634308, 0.0941263209634, ...], [0.0946038612071, 0.0628274293759, 0.0635114153467, 0.0821922661427, 0.117514683178, 0.0857874299633, 0.116179998728, 0.034174352587, 0.148609061614, 0.0954130467951, 0.0991864550625, 0.0895716140199, 0.0876244050195, 0.0748593682389, 0.111639982691, 0.115750757248, 0.0822154911294, 0.0993076590221, 0.0577672003462, 0.11813067936, ...], [0.0763012937959, 0.0822720272276, 0.0875805475829, 0.151841581049, 0.105441813407, 0.125292033663, 0.0557865414393, 0.129616497127, 0.0623685496556, 0.0592295925092, 0.0642695225445, 0.0799347471452, 0.0872756933116, 0.10277324633, 0.142740619902, 0.119086460033, 0.07911908646, 0.0668841761827, 0.108482871126, 0.0807504078303, ...], [0.0780317791643, 0.088050455936, 0.112721027998, 0.0815720859807, 0.0932859144498, 0.120855127775, 0.0889598922566, 0.0732143452073, 0.0712786931909, 0.097072670274, 0.094958007768, 0.0838767042087, 0.0853586247777, 0.10106698281, 0.0820983995258, 0.092768227623, 0.109069353883, 0.0856550088915, 0.081802015412, 0.0755779490219, ...], [0.0460941718576, 0.0463637200459, 0.0447542856168, 0.0554310536586, 0.0860398274601, 0.0982856948517, 0.0724855559341, 0.0878971694772, 0.127892013132, 0.18569688579, 0.149059622177, 0.0534351145038, 0.0490730643402, 0.0610687022901, 0.062159214831, 0.0741548527808, 0.0905125408942, 0.0959651035987, 0.100327153762, 0.121046892039, ...], [0.0422460985659, 0.0712603176271, 0.0667774319623, 0.0680146080701, 0.0772870708642, 0.09369433366, 0.100985932355, 0.0810389503239, 0.144340071979, 0.18983764455, 0.0645175400425, 0.0465949820789, 0.0716845878136, 0.0878136200717, 0.078853046595, 0.078853046595, 0.094982078853, 0.0913978494624, 0.0931899641577, 0.123655913978, ...], [0.0747003438046, 0.0709113975128, 0.0702564509068, 0.125195449018, 0.148396843503, 0.133330655143, 0.0947256169447, 0.0949676302822, 0.0777737196361, 0.0431480248604, 0.0665938683875, 0.0801005747126, 0.0757902298851, 0.0765086206897, 0.122126436782, 0.136494252874, 0.123204022989, 0.0908764367816, 0.0897988505747, 0.0818965517241, ...], [0.0935852681206, 0.0799515986482, 0.084809124865, 0.104350286807, 0.0861143667554, 0.075640690841, 0.0950895431933, 0.114404264643, 0.0916468738272, 0.0717119718634, 0.102696010436, 0.0822147651007, 0.0922818791946, 0.0939597315436, 0.0939597315436, 0.0989932885906, 0.0687919463087, 0.0838926174497, 0.110738255034, 0.0922818791946, ...], [0.143762118287, 0.0752115450379, 0.0759428491362, 0.0611558822459, 0.0635945153654, 0.083381655377, 0.0903466057737, 0.131923852967, 0.091660407882, 0.11440327318, 0.0686172947486, 0.129200312581, 0.0854389163845, 0.0705912998177, 0.065121125293, 0.0742380828341, 0.0854389163845, 0.0914300599114, 0.116697056525, 0.0901276374056, ...], [0.0842953852787, 0.0518660645361, 0.0688425675496, 0.0916964785564, 0.128847625977, 0.0910237753652, 0.0844987920911, 0.105917309246, 0.0900695619193, 0.0900814476277, 0.112860991853, 0.0926732673267, 0.0756435643564, 0.0784158415842, 0.0906930693069, 0.107722772277, 0.0807920792079, 0.0879207920792, 0.090297029703, 0.0974257425743, ...], [0.0995419487358, 0.0566932747575, 0.103139830659, 0.0873756781862, 0.0832312774641, 0.106317109993, 0.0877466909196, 0.120839247202, 0.0976638256343, 0.0766085628385, 0.0808425536102, 0.126052441665, 0.0726485446235, 0.0959826798172, 0.0849170074573, 0.0897281693529, 0.0995910512389, 0.078662496993, 0.0969449121963, 0.0793841712774, ...], [0.117801081687, 0.0943130009469, 0.0515135902269, 0.10545249186, 0.0944253408009, 0.105271822002, 0.0954933114009, 0.0651543340649, 0.0383074807107, 0.0792520463759, 0.153015499925, 0.113686051596, 0.0944468736336, 0.0592479230433, 0.0988194140796, 0.101005684303, 0.096195889812, 0.096195889812, 0.0686488850022, 0.0623087013555, ...], [0.0731776490501, 0.0712100001219, 0.109737568554, 0.0595921444108, 0.0638344453193, 0.0762462053669, 0.0733110101932, 0.115271113309, 0.13139444359, 0.0774666998622, 0.148758720222, 0.0855430020752, 0.0680193682269, 0.0973022826839, 0.0924602259626, 0.0880793175006, 0.0855430020752, 0.101913765276, 0.0949965413881, 0.104680654831, ...], [0.117620328396, 0.0842790397159, 0.0983482855473, 0.117148215074, 0.115752347151, 0.0485812707264, 0.0839024378383, 0.11263994789, 0.0678342126373, 0.108253114853, 0.0456408001713, 0.105845181675, 0.0789889415482, 0.0932069510269, 0.102685624013, 0.11532385466, 0.0789889415482, 0.0932069510269, 0.0821484992101, 0.0947867298578, ...], [0.0918138876293, 0.0812197731926, 0.113558463822, 0.0960908501524, 0.0587890624809, 0.0860386597311, 0.0703068770504, 0.0406429035279, 0.111694359724, 0.0943436315527, 0.155501531136, 0.0813758389262, 0.101510067114, 0.109060402685, 0.0813758389262, 0.0654362416107, 0.0788590604027, 0.0864093959732, 0.0755033557047, 0.0922818791946, ...], [0.104746288118, 0.0533569318189, 0.0523897695476, 0.0583000851884, 0.0787301255592, 0.138440837761, 0.152733227314, 0.0913592568827, 0.0881518274414, 0.0870679262584, 0.0947237241109, 0.0914583333333, 0.0704166666667, 0.075, 0.0670833333333, 0.0875, 0.126041666667, 0.132708333333, 0.079375, 0.0945833333333, ...], [0.160915056666, 0.0832245998739, 0.0506360528275, 0.12658458999, 0.057762563961, 0.0887716391952, 0.0760406217988, 0.0659714677172, 0.107550168555, 0.0871408097759, 0.0954024296403, 0.143191116306, 0.0870835768556, 0.0686732904734, 0.112507305669, 0.0660432495617, 0.0812390414962, 0.0683810637054, 0.0698421975453, 0.107831677382, ...], [0.0924159560849, 0.10902799519, 0.109129648161, 0.0700583522686, 0.112748816779, 0.0828274141212, 0.0620371134711, 0.0644140134551, 0.0817596239787, 0.0777387537625, 0.137842312728, 0.0886105860113, 0.0827032136106, 0.102788279773, 0.086011342155, 0.110349716446, 0.112712665406, 0.0824669187146, 0.0637996219282, 0.0718336483932, ...], [0.0232773173295, 0.102554932777, 0.0548014820573, 0.0231733059686, 0.0752955163473, 0.0949925915621, 0.0783216012158, 0.140890408259, 0.15839854271, 0.152253506105, 0.0960407956685, 0.0374826469227, 0.0869967607589, 0.0684868116613, 0.047200370199, 0.0763535400278, 0.0795927811199, 0.0758907913003, 0.142989356779, 0.167977788061, ...], ...], y=['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...], scorer=, train=array([ 100, 101, 102, ..., 4997, 4998, 4999]), test=array([ 0, 1, 2, 3, 4, 5, 6,...4093, 4094,
4095, 4096, 4097, 4098, 4099]), verbose=0, parameters={'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 1}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')
1670
1671 try:
1672 if y_train is None:
1673 estimator.fit(X_train, **fit_params)
1674 else:
-> 1675 estimator.fit(X_train, y_train, **fit_params)
estimator.fit =
X_train = [[0.108771151419, 0.105614197748, 0.0966494789241, 0.0866170007457, 0.0964047576502, 0.11930815248, 0.0665383993874, 0.0841112601666, 0.051475049271, 0.068146231684, 0.116364320525, 0.105090670745, 0.093292549705, 0.106620056806, 0.0873934891851, 0.0941664845969, 0.117325759231, 0.0694778239021, 0.0793095914354, 0.0747214332532, ...], [0.106617909874, 0.106357049024, 0.0765518196138, 0.147184749999, 0.0858705240986, 0.0847635617398, 0.0850937259626, 0.111091967911, 0.0773122555785, 0.0487853243806, 0.0703711118193, 0.0942956926659, 0.108265424913, 0.0791618160652, 0.136204889406, 0.0977881257276, 0.0849825378347, 0.0861466821886, 0.0966239813737, 0.0791618160652, ...], [0.185416168109, 0.100101261529, 0.133528111996, 0.101465662428, 0.0703716746745, 0.102758077764, 0.0812441134372, 0.0988210729473, 0.0435087624933, 0.0382121103195, 0.0445729843021, 0.141348497157, 0.103980503656, 0.10804224208, 0.0909829406986, 0.0601137286759, 0.0999187652315, 0.072298943948, 0.116978066613, 0.0674248578392, ...], [0.0963637088631, 0.121625351917, 0.0956768527573, 0.189534202703, 0.10684804642, 0.0715540664797, 0.123586319996, 0.0487971734638, 0.0425811912908, 0.0475143202527, 0.055918765857, 0.106206438765, 0.108861599734, 0.0932625290408, 0.138400265516, 0.0945901095254, 0.081978094922, 0.103883172917, 0.0640557583804, 0.0640557583804, ...], [0.0801096133575, 0.071895449447, 0.0685534174348, 0.0896877135845, 0.108408700517, 0.0759894373889, 0.111760084293, 0.114561218587, 0.0846313318665, 0.0917518547716, 0.102651178752, 0.0948412698413, 0.0845238095238, 0.068253968254, 0.0936507936508, 0.1, 0.0845238095238, 0.109920634921, 0.1, 0.0809523809524, ...], [0.0382561737825, 0.0315757277579, 0.0376576988282, 0.0861285452859, 0.143679577183, 0.0924431361215, 0.086804250971, 0.105428327609, 0.117157492386, 0.134981244139, 0.125887825935, 0.0738862730895, 0.0590365809489, 0.0684534588917, 0.102136906918, 0.114813473379, 0.0804056501268, 0.0843897138718, 0.0876494023904, 0.105396595436, ...], [0.0784746826308, 0.1006413305, 0.105294687454, 0.0729785868519, 0.0657225365574, 0.110274033891, 0.109100079816, 0.0715028124597, 0.116002698888, 0.0796512495891, 0.0903573013613, 0.0795935647756, 0.0897544453853, 0.106689246401, 0.0838272650296, 0.0804403048264, 0.104149026249, 0.106689246401, 0.0762066045724, 0.0973751058425, ...], [0.0920064369969, 0.0767766409783, 0.088577875781, 0.103323636964, 0.0708319990492, 0.102821897842, 0.0771736055954, 0.0911898776385, 0.115921754556, 0.113968901938, 0.0674073726602, 0.10502283105, 0.0896118721461, 0.0816210045662, 0.087899543379, 0.0810502283105, 0.0958904109589, 0.074200913242, 0.0816210045662, 0.101598173516, ...], [0.113920327585, 0.104225241968, 0.121645767516, 0.072213302433, 0.130757972587, 0.089292624932, 0.0656439314145, 0.101428447263, 0.0763251337636, 0.0631624875531, 0.0613847629852, 0.0898410504492, 0.109882515549, 0.112646855563, 0.0822391154112, 0.108500345543, 0.0746371803732, 0.0711817553559, 0.0981340704907, 0.0988251554941, ...], [0.10219007675, 0.0723602246041, 0.0961970951707, 0.116814267704, 0.148062298638, 0.0827725938389, 0.125494140016, 0.0514917907593, 0.0651889033509, 0.05285931811, 0.0865692910589, 0.0786445012788, 0.0620204603581, 0.0895140664962, 0.105498721228, 0.128516624041, 0.0773657289003, 0.106138107417, 0.0818414322251, 0.0914322250639, ...], [0.080177548404, 0.0931795868809, 0.0999111444437, 0.118483345352, 0.091940873078, 0.083312789863, 0.0522401079175, 0.106566192871, 0.0585826455211, 0.107876963363, 0.107728802306, 0.0743801652893, 0.0914256198347, 0.107438016529, 0.114669421488, 0.0847107438017, 0.0888429752066, 0.0712809917355, 0.0976239669421, 0.0686983471074, ...], [0.113891144206, 0.0866807008977, 0.0548496521406, 0.0900785333581, 0.0950678237314, 0.0814450026369, 0.0838627610583, 0.098519913327, 0.140039732117, 0.0894909660789, 0.0660737704478, 0.117040630685, 0.0824742268041, 0.0651910248636, 0.0779260157671, 0.0921770770164, 0.089144936325, 0.0876288659794, 0.0885385081868, 0.11522134627, ...], [0.0381077645694, 0.0974157216961, 0.0950881655299, 0.092574894648, 0.130234317646, 0.108731554031, 0.105156453529, 0.0993667783061, 0.0648689912665, 0.106401315243, 0.0620540435341, 0.076511861009, 0.0935516204477, 0.0898763782158, 0.0968927497494, 0.11693952556, 0.0992315402606, 0.0928833945874, 0.0838623454728, 0.0825258937521, ...], [0.0805225473236, 0.0825788214013, 0.0774580674909, 0.0505955135195, 0.0852763226211, 0.093171130758, 0.14737804487, 0.0900577192716, 0.0697143905999, 0.108670714035, 0.114576728109, 0.0880964866282, 0.0980597797588, 0.0880964866282, 0.0608285264814, 0.0844257996854, 0.102254850551, 0.126900891453, 0.0859989512323, 0.0676455165181, ...], [0.0545096490945, 0.0854380096443, 0.0590723053012, 0.11268847762, 0.11237593458, 0.148140575669, 0.0777969797628, 0.0578052877306, 0.114498669096, 0.0769023412346, 0.100771770267, 0.100886162236, 0.0804362644853, 0.0688479890934, 0.091342876619, 0.0940695296524, 0.12406271302, 0.105657805044, 0.0743012951602, 0.0879345603272, ...], [0.0793137451288, 0.0971837591699, 0.112239651189, 0.0960395146984, 0.107311937923, 0.12176252675, 0.0855251726555, 0.0610570054797, 0.0502425432012, 0.0688468299037, 0.120477313901, 0.0880999342538, 0.0878807801885, 0.104098181021, 0.0973044049967, 0.10650887574, 0.115713346483, 0.0913872452334, 0.0721016874863, 0.0600482138944, ...], [0.098928448476, 0.137305549245, 0.0753791252916, 0.0990566805978, 0.0822124178289, 0.0910533865423, 0.0892351234243, 0.1018692375, 0.107928239779, 0.0444253340597, 0.0726064572547, 0.108493310064, 0.112274578243, 0.086678301338, 0.0933682373473, 0.0796974985457, 0.087550901687, 0.0767888307155, 0.0983129726585, 0.108493310064, ...], [0.123070473061, 0.0578929142681, 0.0464503220138, 0.201737887342, 0.0855492777249, 0.0536180031335, 0.111344533931, 0.10607667333, 0.0519348097508, 0.0775904494795, 0.084734655966, 0.118683901293, 0.0708969839405, 0.0642381511947, 0.142969056013, 0.087348217783, 0.0708969839405, 0.104582843713, 0.0967489228359, 0.0771641206424, ...], [0.0908112608184, 0.086731479801, 0.106727761089, 0.116224958469, 0.0746198778934, 0.100064341346, 0.0903571333521, 0.0540778404843, 0.0883506852995, 0.0824550664772, 0.109579594971, 0.0942684766214, 0.10407239819, 0.0942684766214, 0.0972850678733, 0.077677224736, 0.0852187028658, 0.0980392156863, 0.0761689291101, 0.0897435897436, ...], [0.0946723497621, 0.102614036567, 0.104068459189, 0.104233398688, 0.0692101472815, 0.10136195871, 0.0758868780611, 0.100250906356, 0.0760499812908, 0.0793837453837, 0.092268138711, 0.0808823529412, 0.104411764706, 0.105882352941, 0.107352941176, 0.0801470588235, 0.0911764705882, 0.0727941176471, 0.0948529411765, 0.0860294117647, ...], ...]
y_train = ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', ...]
fit_params = {}
1676
1677 except Exception as e:
1678 if error_score == 'raise':
1679 raise...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.py in fit(self=ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), X=array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), y=array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), sample_weight=None)
323 trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
324 backend=""threading"")(
325 delayed(parallel_build_trees)(
326 t, self, X, y, sample_weight, i, len(trees),
327 verbose=self.verbose, class_weight=self.class_weight)
--> 328 for i, t in enumerate(trees))
i = 99
329
330 # Collect newly grown trees
331 self.estimators.extend(trees)
332...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in call(self=Parallel(n_jobs=1), iterable=<generator object >)
774 self.n_completed_tasks = 0
775 try:
776 # Only set self._iterating to True if at least a batch
777 # was dispatched. In particular this covers the edge
778 # case of Parallel used with an exhausted iterator.
--> 779 while self.dispatch_one_batch(iterator):
self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
iterator = <generator object >
780 self._iterating = True
781 else:
782 self._iterating = False
783...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object >)
620 tasks = BatchedCalls(itertools.islice(iterator, batch_size))
621 if len(tasks) == 0:
622 # No more tasks available in the iterator: tell caller to stop.
623 return False
624 else:
--> 625 self._dispatch(tasks)
self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
626 return True
627
628 def _print(self, msg, msg_args):
629 """"""Display the message on stout or stderr depending on verbosity""""""...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
583 self.n_dispatched_tasks += len(batch)
584 self.n_dispatched_batches += 1
585
586 dispatch_timestamp = time.time()
587 cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588 job = self._backend.apply_async(batch, callback=cb)
job = undefined
self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
589 self._jobs.append(job)
590
591 def dispatch_next(self):
592 """"""Dispatch more data for parallel processing...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
106 raise ValueError('n_jobs == 0 in Parallel has no meaning')
107 return 1
108
109 def apply_async(self, func, callback=None):
110 """"""Schedule a func to be run""""""
--> 111 result = ImmediateResult(func)
result = undefined
func = <sklearn.externals.joblib.parallel.BatchedCalls object>
112 if callback:
113 callback(result)
114 return result
115...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py in init(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
327
328 class ImmediateResult(object):
329 def init(self, batch):
330 # Don't delay the application, to avoid keeping the input
331 # arguments in memory
--> 332 self.results = batch()
self.results = undefined
batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
333
334 def get(self):
335 return self.results
336...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in call(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
126 def init(self, iterator_slice):
127 self.items = list(iterator_slice)
128 self._size = len(self.items)
129
130 def call(self):
--> 131 return [func(*args, **kwargs) for func, args, kwargs in self.items]
func =
args = (ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), None, 0, 100)
kwargs = {'class_weight': None, 'verbose': 0}
self.items = [(, (ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]
132
133 def len(self):
134 return self._size
135...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), forest=ExtraTreesClassifier(bootstrap=True, class_weigh..., random_state=None, verbose=0, warm_start=False), X=array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), y=array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)
116 warnings.simplefilter('ignore', DeprecationWarning)
117 curr_sample_weight *= compute_sample_weight('auto', y, indices)
118 elif class_weight == 'balanced_subsample':
119 curr_sample_weight *= compute_sample_weight('balanced', y, indices)
120
--> 121 tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
tree.fit = <bound method ExtraTreeClassifier.fit of ExtraTr...om_state=820678124,
splitter='random')>
X = array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32)
y = array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]])
sample_weight = None
curr_sample_weight = array([ 2., 0., 2., ..., 1., 1., 2.])
122 else:
123 tree.fit(X, y, sample_weight=sample_weight, check_input=False)
124
125 return tree...........................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.py in fit(self=ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), X=array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), y=array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), sample_weight=array([ 2., 0., 2., ..., 1., 1., 2.]), check_input=False, X_idx_sorted=None)
785
786 super(DecisionTreeClassifier, self).fit(
787 X, y,
788 sample_weight=sample_weight,
789 check_input=check_input,
--> 790 X_idx_sorted=X_idx_sorted)
X_idx_sorted = None
791 return self
792
793 def predict_proba(self, X, check_input=True):
794 """"""Predict class probabilities of the input samples X............................................................................
/home/sv378/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.py in fit(self=ExtraTreeClassifier(class_weight=None, criterion...dom_state=820678124,
splitter='random'), X=array([[ 0.10877115, 0.1056142 , 0.09664948, .... 0.26129046, 0.23792571]], dtype=float32), y=array([[ 0.],
[ 0.],
[ 0.],
...,
[ 4.],
[ 4.],
[ 4.]]), sample_weight=array([ 2., 0., 2., ..., 1., 1., 2.]), check_input=False, X_idx_sorted=None)
189 if isinstance(self.min_samples_split, (numbers.Integral, np.integer)):
190 if not 2 <= self.min_samples_split:
191 raise ValueError(""min_samples_split must be an integer ""
192 ""greater than 1 or a float in (0.0, 1.0]; ""
193 ""got the integer %s""
--> 194 % self.min_samples_split)
self.min_samples_split = 1
195 min_samples_split = self.min_samples_split
196 else: # float
197 if not 0. < self.min_samples_split <= 1.:
198 raise ValueError(""min_samples_split must be an integer ""ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1Hello,I get the similar error on the latest master commit from October, 26 2018. I run it on the test dataset using ./shIC_pipeline.sh script. The system Python version is 2.7.12. Can it be related to the Deprecation Warnings concerning cross_validation.py and grid_search.py modules?
Are there any ways to get this pipeline running?Thank you!",None yet
https://github.com/snipsco/snips-nlu/issues/756,"According to instructions, I installed setuptools_rust and rust$python3 -V
Python 3.6.7setuptools_rust were installed by
$python3 -m pip install setuptools_rust
It gives me output as, Installing collected packages: toml, semantic-version, setuptools-rust
Successfully installed semantic-version-2.6.0 setuptools-rust-0.10.6 toml-0.10.0$ rustc --version
rustc 1.32.0 (9fda7c223 2019-01-16)However when I try setting up snips-nlu using following instruction:
$python3 -m pip install snips-nluCollecting snips-nlu
Using cached https://files.pythonhosted.org/packages/2e/d5/a928f703640f69992277f95f2b95075e7f3f02e9199ef253d9d78a9c6e2b/snips_nlu-0.19.2-py2.py3-none-any.whl
Collecting semantic-version<3.0,>=2.6 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/28/be/3a7241d731ba89063780279a5433f5971c1cf41735b64a9f874b7c3ff995/semantic_version-2.6.0-py3-none-any.whl
Collecting pyaml<18,>=17 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/17/c1/5892f756109e54ed53c753129b0da4acf6b6add8dff5a85b18667553b16d/pyaml-17.12.1-py2.py3-none-any.whl
Collecting plac<1.0,>=0.9.6 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl
Collecting numpy<1.16,>=1.15 (from snips-nlu)
Collecting future<0.17,>=0.16 (from snips-nlu)
Collecting sklearn-crfsuite<0.4,>=0.3.6 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl
Collecting snips-nlu-parsers<0.2,>=0.1 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/c5/b3/c4e2060b11df89d70e782b368557db0fe7af656115c54702b120750dc0a2/snips_nlu_parsers-0.1.2.tar.gz
Collecting num2words<0.6,>=0.5.6 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/97/59/daea20448b3bf3cc07694f393a2a9d06439e4c9a96f83734d9d2c5231703/num2words-0.5.9-py3-none-any.whl
Collecting deprecation<3,>=2 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/32/e9/01ffbaf3540ad54476cd7066439d629f1dd73b851cc5c0993ce2c12e1cdd/deprecation-2.0.6-py2.py3-none-any.whl
Collecting snips-nlu-utils<0.8,>=0.7 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/75/54/2b6aa49f784f6bc470a6d45bd4c9ae9ffe673b5be6b556c1ef42f30aee17/snips_nlu_utils-0.7.1.tar.gz
Collecting scipy<2.0,>=1.0 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/a9/b4/5598a706697d1e2929eaf7fe68898ef4bea76e4950b9efbe1ef396b8813a/scipy-1.2.1.tar.gz
Collecting requests<3.0,>=2.0 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl
Collecting scikit-learn<0.20,>=0.19 (from snips-nlu)
Using cached https://files.pythonhosted.org/packages/25/b6/454cf208be93efa3db50ce06b732328c57ede005d1dcfa71d9a1548530b0/scikit-learn-0.19.2.tar.gz
Collecting PyYAML (from pyaml<18,>=17->snips-nlu)
Collecting six (from sklearn-crfsuite<0.4,>=0.3.6->snips-nlu)
Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl
Collecting tqdm>=2.0 (from sklearn-crfsuite<0.4,>=0.3.6->snips-nlu)
Using cached https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl
Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite<0.4,>=0.3.6->snips-nlu)
Collecting tabulate (from sklearn-crfsuite<0.4,>=0.3.6->snips-nlu)
Collecting docopt>=0.6.2 (from num2words<0.6,>=0.5.6->snips-nlu)
Collecting packaging (from deprecation<3,>=2->snips-nlu)
Using cached https://files.pythonhosted.org/packages/91/32/58bc30e646e55eab8b21abf89e353f59c0cc02c417e42929f4a9546e1b1d/packaging-19.0-py2.py3-none-any.whl
Collecting urllib3<1.25,>=1.21.1 (from requests<3.0,>=2.0->snips-nlu)
Using cached https://files.pythonhosted.org/packages/62/00/ee1d7de624db8ba7090d1226aebefab96a2c71cd5cfa7629d6ad3f61b79e/urllib3-1.24.1-py2.py3-none-any.whl
Collecting idna<2.9,>=2.5 (from requests<3.0,>=2.0->snips-nlu)
Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl
Collecting chardet<3.1.0,>=3.0.2 (from requests<3.0,>=2.0->snips-nlu)
Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl
Collecting certifi>=2017.4.17 (from requests<3.0,>=2.0->snips-nlu)
Using cached https://files.pythonhosted.org/packages/9f/e0/accfc1b56b57e9750eba272e24c4dddeac86852c2bebd1236674d7887e8a/certifi-2018.11.29-py2.py3-none-any.whl
Collecting pyparsing>=2.0.2 (from packaging->deprecation<3,>=2->snips-nlu)
Using cached https://files.pythonhosted.org/packages/de/0a/001be530836743d8be6c2d85069f46fecf84ac6c18c7f5fb8125ee11d854/pyparsing-2.3.1-py2.py3-none-any.whl
Building wheels for collected packages: snips-nlu-parsers, snips-nlu-utils, scipy, scikit-learn
Running setup.py bdist_wheel for snips-nlu-parsers ... error
Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;file='/tmp/pip-build-3k63cejl/snips-nlu-parsers/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, file, 'exec'))"" bdist_wheel -d /tmp/tmpu7h4di4ppip-wheel- --python-tag cp36:
/usr/lib/python3/dist-packages/setuptools/dist.py:397: UserWarning: Normalizing'0.1.2
' to '0.1.2'
normalized_version,
running bdist_wheel
running build
running build_py
creating build
creating build/lib
creating build/lib/snips_nlu_parsers
copying snips_nlu_parsers/builtin_entities.py -> build/lib/snips_nlu_parsers
copying snips_nlu_parsers/gazetteer_entity_parser.py -> build/lib/snips_nlu_parsers
copying snips_nlu_parsers/utils.py -> build/lib/snips_nlu_parsers
copying snips_nlu_parsers/init.py -> build/lib/snips_nlu_parsers
copying snips_nlu_parsers/builtin_entity_parser.py -> build/lib/snips_nlu_parsers
running egg_info
writing snips_nlu_parsers.egg-info/PKG-INFO
writing dependency_links to snips_nlu_parsers.egg-info/dependency_links.txt
writing requirements to snips_nlu_parsers.egg-info/requires.txt
writing top-level names to snips_nlu_parsers.egg-info/top_level.txt
reading manifest file 'snips_nlu_parsers.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no previously-included files matching '' found under directory 'ffi/target/'
warning: no previously-included files matching 'pycache' found anywhere in distribution
warning: no previously-included files matching '.py[cod]' found anywhere in distribution
writing manifest file 'snips_nlu_parsers.egg-info/SOURCES.txt'
copying snips_nlu_parsers/version -> build/lib/snips_nlu_parsers
creating build/lib/snips_nlu_parsers/dylib
copying snips_nlu_parsers/dylib/.gitignore -> build/lib/snips_nlu_parsers/dylib
running build_ext
running build_rust
Updating crates.io index
Updating git repository https://github.com/snipsco/snips-utils-rs
Updating git repository https://github.com/snipsco/snips-nlu-ontology
Updating git repository https://github.com/snipsco/snips-nlu-parsers
Updating git repository https://github.com/snipsco/gazetteer-entity-parser
Updating git repository https://github.com/snipsco/rustling-ontology
Updating git repository https://github.com/snipsco/snips-nlu-utils
Updating git repository https://github.com/snipsco/rustling
Downloading crates ...
Downloaded rustc-demangle v0.1.13
Downloaded cc v1.0.29
Downloaded winapi v0.3.6
Downloaded vec_map v0.8.1
Downloaded num-traits v0.1.43
Downloaded winapi-i686-pc-windows-gnu v0.4.0
error: failed to unpack package winapi-i686-pc-windows-gnu v0.4.0Caused by:
failed to unpack entry at winapi-i686-pc-windows-gnu-0.4.0/lib/libwinapi_onecoreuap-api-ms-win-core-url-l1-1-0.aCaused by:
failed to unpack /home/nvidia/.cargo/registry/src/github.com-1ecc6299db9ec823/winapi-i686-pc-windows-gnu-0.4.0/lib/libwinapi_onecoreuap-api-ms-win-core-url-l1-1-0.a
Traceback (most recent call last):
File """", line 1, in
File ""/tmp/pip-build-3k63cejl/snips-nlu-parsers/setup.py"", line 60, in
zip_safe=False)
File ""/usr/lib/python3/dist-packages/setuptools/init.py"", line 129, in setup
return distutils.core.setup(**attrs)
File ""/usr/lib/python3.6/distutils/core.py"", line 148, in setup
dist.run_commands()
File ""/usr/lib/python3.6/distutils/dist.py"", line 955, in run_commands
self.run_command(cmd)
File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
cmd_obj.run()
File ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 204, in run
self.run_command('build')
File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
self.distribution.run_command(command)
File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
cmd_obj.run()
File ""/usr/lib/python3.6/distutils/command/build.py"", line 135, in run
self.run_command(cmd_name)
File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
self.distribution.run_command(command)
File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
cmd_obj.run()
File ""/home/nvidia/.local/lib/python3.6/site-packages/setuptools_rust/build_ext.py"", line 26, in run
build_rust.run()
File ""/home/nvidia/.local/lib/python3.6/site-packages/setuptools_rust/build.py"", line 313, in run
self.build_extension(ext)
File ""/home/nvidia/.local/lib/python3.6/site-packages/setuptools_rust/build.py"", line 95, in build_extension
metadata = json.loads(check_output(metadata_command).decode(""utf-8""))
File ""/usr/lib/python3.6/subprocess.py"", line 336, in check_output
**kwargs).stdout
File ""/usr/lib/python3.6/subprocess.py"", line 418, in run
output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command '['cargo', 'metadata', '--manifest-path', 'ffi/Cargo.toml', '--format-version', '1']' returned non-zero exit status 101.Failed building wheel for snips-nlu-parsersHi @ak-nv sorry for the late reply.
I'm looking at your issue, we'll come back to you shortly.Hey @ak-nv ,
Can you confirm that you have enough space left on your device?
Thanks@adrienball I have about 16GB space on my device.@ClemDoum and @adrienball Any updates on this? I would like to test out on my Jetson board. Please let me know.@ak-nv,
It's quite hard to debug this issue on our side as we cannot reproduce your build environment (no Jetson board here), and the stacktrace suggests that the issues is not directly linked to snips-nlu-rs. Have you tried to build other rust crates on your board?Also, latest rust version is now 1.34.1 so it may be worth updating.",None yet
https://github.com/scikit-learn/scikit-learn/issues/9650,"Right now, the content of classes_ and the shape of the output of predict_proba and decision_function are inconsistent between estimators.
If an estimator supports multioutput multiclass, classes_ will be [[0, 1]] * n_outputs and predict_proba will return an object array of length n_outputs where each is (n_samples, 2).
If the estimator supports multi-label classification, but not multioutput multiclass, classes_ will be np.arange(n_outputs) and predict_proba will be (n_samples, n_outputs).That leads to issues such as #8773, but also runs counter to the sklearn promise of a consistent API.
Given that even basic API like the score method is broken on multioutput multiclass (see #9414), I'm not sure it's worth keeping this maintenance burden.Alternatively we could detect in the multioutput multiclass classifiers that a problem is actually multilabel and use the appropriate shapes for that.Either way, it's an incompatible change that's tricky to do with a deprecation cycle :-/See also https://gist.github.com/jnothman/4807b1b0266613c20ba4d1f88d0f8cf5 (though I'm yet to add a classes_ check there).Basically, the estimators that report a multilabel decision function with a single 2d array are OneVsRestClassifier and MLPClassifier.I find the multiclass-multioutput better for its non-ambiguity... but it is not as easily usable. And given how recent MLPClassifier was merged, this perhaps comes down to something quirky in OneVsRestClassifier which is a bit weird for rolling multiclass and multilabel into one thing anyway... (It's engendered weird behaviour in LabelBinarizer, etc.)moving to 0.24",API Bug
https://github.com/scikit-learn/scikit-learn/issues/12228,"The graphical lasso is quite unstable on very ill-conditioned matrix. It is a well known issue that arises from the fact that it is a very optimization problem.Adding a small amount of l2-regularization helps a lot. On a problem in my lab, it ended being the only way to estimate a sparse inverse covariance. It is actually a simple modification. If people agree, I can try to find the time to contribute this.What do people think?I would like to work on this issueSounds good but please include a plot of how this impacts the existing examples in the PR.Have you exchanged about this l2 penalty with the authors of the glasso package? As far as I understand they do not include l2 penalty either but it would be great to have their opinion on this.Four our package http://github.com/metric-learn/metric-learn, we would be really interested in having a more robust Graphical Lasso too, as we use it for the SDML algorithm. Right now, we also have an issue with graphical lasso, but skggm seems to fix it (see PR scikit-learn-contrib/metric-learn#162). Note that in our case we sometimes need to find a sparse (SPD) precision matrix so that its inverse is the closest to a non SPD ""covariance-like"" matrix (i.e. the ""covariance"" matrix we pass to graphical lasso can be non SPD). sklearn's graphical lasso seems to not work for such matrices, contrary to skggm (though we need to put have an SPD matrix as initialization which seems normal).
Also, I have noticed that current scikit-learn's graphical lasso does not seem to work even if I put an SPD matrix to reconstruct, whereas it worked with some older versions of scikit-learn.I don't know if all of this is related to L2 regularization though, but here seemed a good place to add this comment.I can provide examples if needed, just tell me which one could help: for instance first I could provide a snippet with the new scikit-learn version not working and the old one working ?Here is a first snippet to show that with an SPD input matrix, the new version of sklearn's graphical_lasso does not seem to work contrary to an older version:Maybe it is good to add skggm's quic for comparison?I agree, I just update my comment with skggm's quic (only for the new sklearn's and python version, because I didn't manage to install skggm on the older environment (see skggm/skggm#119). I chose cov_init=np.eye(A.shape[0]) (identity) which is the default for skggm, so that we can compare both (I use identity and not covariance because for the non SPD case I'll use this too (and my ""covariance"" will be non SPD)) I also added the estimated covariance so that we can see the results. We can see that in the end in the current version even if it says it doesn't converge the estimate is coherent with skggm's one.
Now I'll add some snippets for the non SPD caseActually not, the current sklearn version finds 6 instead of 1 for the lower right valueAh yes that's right, I looked too fastHere is an example with a non SPD matrix given as input:It's as if scikit-learn's graphical lasso would estimate a covariance matrix that is similar to the original matrix except the right bottom coeff, which is the same as the upper left one (like in the last case)Here is an example a bit different that confirms the above (I just switched the sign on the diagonal):We see that scikit-learn estimated covariance is not SPD (which is problematic for SDML's case but for a real covariance case I agree the input should be SPD)Also, in both last cases (non SPD input matrix), old scikit-learn would return:For the example in the code snippet #12228 (comment), sklearn v0.19.2 converges correctly while v0.20rc1 does not (iit returns the same output as the current v0.21.dev0 tried in the above comment).This holds for Python 2 and 3 and for both solvers (CD and LARS), using numpy 0.15.4.",Enhancement
https://github.com/scikit-learn/scikit-learn/issues/18198,"I'm attempting to run a bayesian search for randomforest classifier hyperparameters, but every time i try to fit the function, it returns a FutureWarning. Attached is code that reproduces the problem I face on my local machineError Code:System:
python: 3.8.3 (default, Jul 2 2020, 11:26:31) [Clang 10.0.0 ]
executable: /Users/local/opt/anaconda3/bin/python
machine: macOS-10.15.6-x86_64-i386-64bitPython dependencies:
pip: 20.1.1
setuptools: 49.2.0.post20200714
sklearn: 0.23.1
numpy: 1.18.5
scipy: 1.5.0
Cython: 0.29.21
pandas: 1.0.5
matplotlib: 3.2.2
joblib: 0.16.0
threadpoolctl: 2.1.0
scikit-optimize: 0.7.4Built with OpenMP: Truesomething went wrong in our deprecation of MaskedArray (@deprecated doesn't support __new__ or something like that)See scikit-optimize/scikit-optimize#902 for workaroundsRe the keyword warnings, the skopt snippet should be updated.",Bug: triage
https://github.com/geopandas/geopandas/issues/972,"edit (20/03/04) by @martinfleis: See the discussion below for the context.Result:
0 True
1 False
2 False
3 False
4 False
5 False
6 False
7 False
8 False
9 False
10 False
11 False
12 False
13 False
14 False
15 False
16 False
17 False
18 False
19 False
20 False
21 False
22 False
dtype: bool
0 False
1 False
2 False
3 False
4 False
5 False
6 False
7 False
8 False
9 False
10 False
11 False
12 False
13 False
14 False
15 False
16 False
17 False
18 False
19 False
20 False
21 False
22 False
23 False
dtype: bool
0 False
dtype: bool
0 False
1 False
2 False
3 False
4 False
5 False
6 False
7 False
8 False
9 False
10 False
11 False
12 False
13 False
14 False
15 False
16 False
17 False
18 False
19 False
20 False
21 False
22 False
23 False
24 False
25 False
26 False
dtype: bool
0 True
1 False
2 False
3 False
4 False
5 False
6 False
7 False
8 False
9 False
10 False
11 False
12 False
13 False
14 False
15 False
16 False
17 False
18 False
19 False
dtype: boolreason :
why not true, only return false ,in fact, exits true@jiezhengxu without more context it is very difficult to understand your issue or question. Can you explain what you are trying to do? And can you also try to provide a reproducible example? (some code that we can run)@jorisvandenbossche I think i met the same issue when intersecting two GeoDataFrames after reprojecting using Pandas 1.0.1 and GeoPandas 0.7. Here is an example:@b4l gdf1.intersects(gdf2) does pair-wise intersection. Rows are matched and only geometries within the same rows are checked against each other. But I agree that it is not what one might expect. The solution might be to add a new keyword to have gdf.intersects(gdf2, any=True) which would do what you expected (and could even use spatial index under the hood to do so).Thank you for the insights @martinfleis. I think the spatial predicates should be handled by default as set operations, whereas the elementwise variant could be accessed through an optional keyword. What do you think?I feel the same about it, but to changing this behaviour to default on .any() would be backward incompatible and might break a lot of code if implemented. Moreover, this element wise logic is something we inherit from pandas and having some functions working like that and other not is something I'd like to avoid.As far as I know, there are many functions in pandas that do not operate elementwise, like all the merge, join, append etc. The 1.0 Milestone would probably be a good opportunity to introduce these changes. It might be painfull, though even more justifying a counterintuitive api to the user.For a bit of context, the align-then-relate pattern used in geopandas is a common rough edge I think in part because it's not how PostGIS works for spatial predicates (in addition to being different from the DE9IM semantics). This is why it's all-pairs by default in R's sf, too.I understand how we got here, but I can see how it's a confusing behavior for users, so an all-pairs relates between geoseries by default would definitely make sense for a 1.0 release, but it's a big ask... like @martinfleis, am concerned that this is a huge breakage.If so, a keyword argument is definitely preferred (though I wouldn't want any(), I'd want pairwise=True. Users should explicitly use df.intersects(df2, pairwise=True).any(axis=1) forAlternatively, a more explicit separation could put fast index-enabled pairwise predicates under a specific namespace in the dataframe, like df.pairwise.intersects(df2). This is used elsewhere say, sklearn to separate the functions that might apply to a single coordinate pair versus applying the function to all pairs between two matrices of coordinates.I would prefer df.intersects(df2, pairwise=True) with the default value set to False for the set operation. Alternatively the default could be set to True and changed later to avoid breakage and time the transition.I think we could do df.intersects(df2, pairwise=False) with a deprecation warning that default will change. I would keep it there for a few releases and then change it. It may be 1.0.0 or 0.11.0 or whatever will fit to give enough time for a transition.If we use pairwise=True to mean row-by-row, that's different from the semantics of sklearn.metrics.pairwise as well as the implied meaning of scipy.spatial.distance.pdist. I don't think that's a good idea... am I understanding your comment about the ""default value set to False for the set operation."" correctly @b4l?If so & folks find pairwise=True confusing to mean the relation for all pairs, What about all_pairs=True?What about paired=True relatet to paired_distance in sklearn? scipy has cdist which seems odd.Good points. I like paired as it is a bit more explanatory than all_pairs.Perhaps it might be useful to take other comparison methods into account as well in designing the interface, like for example bbox intersection. The latter could be achieved by another parameter or combined in a string typed mode parameter for example.I have recently played with this and came up with a possible implementation. See https://gist.github.com/martinfleis/abc7cdbf9f9266bf9ed369080eec7ceaI am not sure about the API yet, so there are two main questions to resolve (from an implementation point of view):Note that there might be some temporary workarounds as pygeos does not support all predicates yet (pygeos/pygeos#157).And then there's the question of change of default...",enhancement
https://github.com/Hironsan/anago/issues/113,"I try to run pretrained models as shown in README, namely:and receive the following error (future warnings have been silenced):Could you help me understand what i am doing wrong?Thanks in advance,
NikolaiHi. I think I have found the cause. The same cause is also preventing the training on a fresh model.So... the reason that pip install anago (that installs anago==1.0.8) installs too new libraries of Keras and tensorflow. In fact, I ended up with:Which is expected given that the file setup.py (https://github.com/Hironsan/anago/blob/master/setup.py) specifies versions of these libraries in a broad, in fact open-ended, fashion:Assuming that anago was tested with at least the lowest versions, I downgraded Keras, tensorflow and seqeval libraries.",None yet
https://github.com/scikit-learn/scikit-learn/pull/13988,"Partially addresses #13986 and #13923This avoids one memory copy in linear models when copy_X=True (default) and fit_intercept=False.For instance, this makes Ridge(fit_intercept=False) around 10% faster on the following dataset,Maybe we should change the default value of copy_X from True to None but I'm not sure it's worth a deprecation cycle.I have checked that this doesn't break anything with the following common test,but of course this only checks the default solver and parameters (and we don't yet have a way to programmatically check all solvers I think?). I'm not sure it's worth adding it in this PR.@rth CIs are green so I guess problems are fixed?",Needs work module:linear_model
https://github.com/jupyter-attic/declarativewidgets/issues/521,"I'm having trouble getting declarative widgets to work. I'm running in a python3 virtual env on Ubuntu.I have also run jupyter nbextension enable --py --sys-prefix widgetsnbextension and jupyter declarativewidgets quick-setup --sys-prefix.I have then setup a notebook and run:andworks. As in the slider shows up. However if I runthe result is (in text): datarows='[[""a"",8,5],[""b"",2,6],[""c"",5,7]]' columns='[""Index"",""Series 1"",""Series 2""]'> which is not what I would have expected.Its much the same even if I switch it to urth-viz-table.I the chrome dev console I can see these two errors:I would think that there is maybe some problem with ipywidgets getting enabled. When I switch on debug logging on jupyter I can see it is loading config from /home/adam/python3/venv/etc/jupyter/jupyter_notebook_config.json which looks like:I also see this warning in the logs and I'm not sure if it is relevent:I am aware of the other issues such as 297 but they but issue seems slightly different.Looks like a syntax error. datarows and columns should be attributes on urth-viz-bar You've got a > closing the tag with those attributes as inner text.Ok my bad but when I remove it it is just blank output still with the errors.
*edit: I have tried this on another computer and still have the problem. (essentially the same setup).The Urth deprecation warning is not relevant. Could you please provide full logs from the notebook server?@adamjoshuagray Still having issues?",None yet
https://github.com/ListerLab/HOME/issues/32,"I have installed HOME after creating an environment for python2.7 as described in the github installation instructions. Upon downloading the testdata (or trying to use my own), I get this error log while running HOME from the testdata directory:HOME-pairwise -t CG -i ./sample_file_CG.txt -o ./CG -npp 24
/home/pgontarz/HOME_env/local/bin/HOME-pairwise: line 6: $'\nCreated on Mon Apr 16 12:15:36 2018\n\n@author: akanksha\n': command not found
import: unable to open X server ' @ error/import.c/ImportImageCommand/364. import: unable to open X server ' @ error/import.c/ImportImageCommand/364.
import: unable to open X server ' @ error/import.c/ImportImageCommand/364. from: can't read /var/mail/collections import: unable to open X server ' @ error/import.c/ImportImageCommand/364.
import: unable to open X server ' @ error/import.c/ImportImageCommand/364. import: unable to open X server ' @ error/import.c/ImportImageCommand/364.
import: unable to open X server ' @ error/import.c/ImportImageCommand/364. from: can't read /var/mail/collections import: unable to open X server ' @ error/import.c/ImportImageCommand/364.
import: unable to open X server ' @ error/import.c/ImportImageCommand/364. import: unable to open X server ' @ error/import.c/ImportImageCommand/364.
import: unable to open X server ' @ error/import.c/ImportImageCommand/364. from: can't read /var/mail/HOME from: can't read /var/mail/os.path /home/pgontarz/HOME_env/local/bin/HOME-pairwise: line 25: syntax error near unexpected token ('
/home/pgontarz/HOME_env/local/bin/HOME-pairwise: line 25: `def remEmptyDir(mypath):'I am wondering if there is some issue with my installation of HOME? Building the virutalenv ran with no errors reported as did source <HOME_env>/bin/activate.ERROR: Failed building wheel for scikit-learn
Running setup.py clean for scikit-learn
Successfully built HOME
Failed to build scikit-learn
Installing collected packages: numpy, pytz, six, python-dateutil, pandas, scipy, scikit-learn, patsy, statsmodels, HOME
Running setup.py install for scikit-learn ... done
DEPRECATION: scikit-learn was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at pypa/pip#8368.
Successfully installed HOME-1.0.0 numpy-1.16.6 pandas-0.17.1 patsy-0.5.1 python-dateutil-2.8.1 pytz-2020.1 scikit-learn-0.16.1 scipy-0.16.0 six-1.15.0 statsmodels-0.6.1",None yet
https://github.com/scikit-learn/scikit-learn/issues/9285,"With a LabelEncoder fitted with both string and numeric values, the inverse transform of that LabelEncoder will include only strings.I understand that numpy is not ideal for dealing with non-numeric data, so I don't know what dtype the output SHOULD be, but I know that if the dtype is simply ""object"", then it will differentiate between the strings and numbers.The array is no longer mixed type.Windows-8.1-6.3.9600-SP0
Python 3.5.3 |Anaconda 4.4.0 (64-bit)| (default, May 15 2017, 10:43:23) [MSC v.1900 64 bit (AMD64)]
NumPy 1.12.1
SciPy 0.19.0
Scikit-Learn 0.18.1It's also worth noting that LabelEncoder().fit(np.array([1, 2, 'a', 'b'], dtype=object)) will also throw TypeError: unorderable types: str() > int() from np.unique, so this is not an issue limited to regular lists. Would it be an acceptable solution to add a parameter for dtype?I see the issue here, and it is entirely with how np.unique can't handle mixed type arrays. Interestingly, in 2.7 numpy did not have this issue. Anyway, it seems that this issue can't be easily solved unless np.unique is updated. LabelEncoder already doesn't handle mixed types (implicitly converts to string via numpy), so yeah it would probably be a good idea to raise an error or a warning.Link to known issue in numpySince mixed types already convert to strings by default, and I believe checking for mixed types would be a relatively slow operation, wouldn't it make more sense to just make a note of it in the documentation rather than update the code?@jnothman @raghavrv Hi all, can I try to work on this one? And what's your plan, raise a warning or an error for mixed types?@jnothman I tried LabelEncoder on my own dev, and the result is a bit different.For Python 2.7.13 and 3.6.1
le = LabelEncoder().fit([1, 2, 'a', 'b'])
le.inverse_transform([0, 1, 2, 3])
Result: array(['1', '2', 'a', 'b'], dtype='<U21')le = LabelEncoder().fit(np.array([1, 2, 'a', 'b'], dtype=object))
le.inverse_transform([0, 1, 2, 3])
Result for Python 2.7.13: array([1, 2, 'a', 'b'], dtype=object)
The result seems what @goldsmitha want to get. And I didn't have any exception here in Python 2.7.13.
Result for Python 3.6.1: TypeError: '>' not supported between instances of 'str' and 'int', in numpy.unique.Versions:
macOS Sierra Version 10.12.5
NumPy 1.13.1
SciPy 0.19.1
Cython 0.25.2As the mixed type runs well in Python 2.7.13 with dtype=object, do we still stick to raise a warning or an error here?@stephenweiwang as far as I can understand this is what was supposed to happen with python2.7. Therefore you need to raise a warning.
Ofc if you aren't working on it I can take it up as well.Is there a way something to handle mixed types could be added? I find myself having to layer transformation classes on top of one another to handle this case (somewhat common for me). I imagine something like the hash table that Pandas uses. I haven't contributed before, but I could potentially take a stab at this.It's not that it's useful, it's just sometimes how data comes in. Pandas handles mixed types with get_dummies, I guess I was just hoping to swap in one of sklearn's encoders in place of that operation.But, on second thought, I could see how it makes sense to just keep things as-is. It still seems like a more informative warning would be useful. I can look into that, if that's still needed.#DataUmbrella Can I pick this up for 6th June sprint? Is the acceptable solution raise a warning or an error for mixed types?Hi @amy12xx , this is a quite old PR, I think that checking the behavior of the code snippet with the current version will be already really helpful! Then, you could probably start with raising a warning and see what core-devs will say. Thank you for tackling this!ok I can do thattake sprint table 14 working on this.@cmarmo we found the same issue persists.Python 3.8.3 (default, May 19 2020, 06:50:17) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32from sklearn.preprocessing import LabelEncoder
le=LabelEncoder().fit([1, 2, 'a', 'b'])
le.inverse_transform([0, 1, 2, 3])
array(['1', '2', 'a', 'b'], #dtype='<U11')Shall we go ahead and add a warning message to it?Feel free to go with the warning... be ready to be asked for some changes... :)... that's life... Thanks!The problem is that the dtype information is not available after initially converting the list to a numpy array. Generally we ignore this issue in sklearn and assume that it's fine to interpret whatever the user gave us as np.asarray(X). What we could do is call np.asarray here and if that returns a string array instead explicitly convert to dtype object. That would allow us to raise an error here. That would be backward incompatible and so we should raise a deprecation warning for now.I'm not entirely sure this is worth it. Do we have a protection against casting to string in check_array? I doesn't look like that to me.also see #17294",Needs Decision
https://github.com/spokenlanguage/platalea/issues/13,"When running e.g. basic-default, I now get the following warnings:",None yet
https://github.com/thehawkgriffith/Captcha-Decoder/issues/1,,None yet
https://github.com/ClimbsRocks/auto_ml/issues/232,"'ml_for_analytics=False' or I got error:Thanks for help!
It was necesarry to build xgboost from scratch (pip version is not fresh enough).
I have auto_ml 2.1.9.
Xgboost update solves problem with feature_importances_ for ml_for_analytics=True but for 'optimize_final_model=True' I've got different error than before:There is a similar issue, but without solution:
https://stackoverflow.com/questions/37646034/scikit-learn-cannot-clone-object-as-the-constructor-does-not-seem-to-set-parLooks like XGBoost made a change that sklearn's GSCV isn't liking. I'm hoping @gaw89 might have an easy fix in XGBoost for this. In the meantime, you can try checking out the code before this commit and using that.Thanks for letting me know about this @mglowacki100 !I've attached my code and dataset:
my_dataset.tar.gz",None yet
https://github.com/scikit-learn/scikit-learn/issues/18184,"It does not install on python 3.7 AND 3.8pip3.8 / pip3 install scikit-learn sklearnIt needs to be installed properlyThe error is expected if you try to build scipy from sources,The best way is toor if you really want to build from sources, follow https://docs.scipy.org/doc/scipy/reference/building/linux.html#debian-ubuntu but it will take a while to compile on RPi.@rth how can i install them for python3.8 with apt? Because it still tries to collect scipyCould you try withand install dependencies manually
?@rth it gives the same errorAfter updating setuptools, i got this:python3-scipy is already the newest version (1.1.0-7).You should try miniforge (link above). Does /usr/local/bin/python3.8 -c ""import scipy"" work? If not it means that the scipy is getting installed for a different python version..",Bug: triage
https://github.com/statsmodels/statsmodels/issues/411,"I think fittedvalues in discrete should be the default predict, i.e. np.exp(x_b) instead of x_b.(Just took me half an hour to figure out fittedvalues is not inherited.)genmod GLM also has fittedvalues = mu which is predict linear=False, AFAICSI don't really have a strong opinion on this, though it seemed natural to be that the fittedvalues is linear XB.What if the model doesn't use a log link?The default values of the predict method of the various discrete choice models do the ""right thing"" with the linear prediction XB. There aren't any link functions (yet) in predict. Adding transforms is on my todo list for a more general predict (a la stata).Yes, I just meant that if you're estimating a negative binomial model in the canonical form, then hardcoding exp(Xb) will give you the wrong answer. A first step in the direction of your general predict might be to define an inverse_link() method for each model. If method exists, then predict applies the function to the linear predictions, if it doesn't then predict just returns XbRight. My point was I don't think Josef was proposing to hard code exp. He's proposing that the fittedvalues attribute be the default predict for the model, which is something sensible depending on the model. Right now we just have a linear keyword. If linear = True, it returns XB. If not, the default, it returns something reasonable. Eg., the binary choice and multivariate binary choice models return probabilities of events by default. The poisson and negative binomial return expected counts (number of events). There's no concept of a link function here, though when you do marginal effects you can sort of specify a link function by asking for the marginal effects in the form dy/dx, dy/ex, ey/dx, or ey/ex and I think this can be made more generic. What I would also like is to extend these to other sorts of things like incidence rates or probability ranges similar to how I wrote the Tobit model predict.https://github.com/statsmodels/statsmodels/blob/tobit-model-rebase/statsmodels/regression/censored_model.py#L693OK, thanks for clarifying. I'll try to refrain from barging into conversations when I don't know basic background :)No, no it's totally fine. I think this is part of the disconnect between GLM and the discrete choice via maximum likelihood that needs to be cleared up.Vincent, It's better to ask the questions or state your view. It makes it easier to find you way around statsmodels, and we can see what is not clear.As Skipper said, all I would like to have currently is that fittedvalues is the same as default predict, and that both should specify the conditional expectation E(y|x), not an intermediate result (X*b).
Still open if we predict more than the mean, e.g. mean and variance in heteroscedastic (GARCH) models, maybe fittedalues should be 2d and have the expected values for both.Tangentially related. I'm starting to think that maybe predict should also return an object like we've done with get_outlier, get_influence, and particularly get_margeff. That way we can have prediction, confidence intervals, transformations, etc. all lazily and in the same place.Thoughts? Too heavy?Yes, I also thought that we need extra features for predict, for discrete for example returning a distribution instance would be useful.The problem might be that for cross-validation and similar the extra overhead will be unnecessary.(It's a similar problem with what statistical tests should return)Extra-overhead could be nice (maybe) and the other results can be lazy.I've been toying with the idea of using sklearn pipeline for doing gradient boosting on an, say, an ARIMA model just to see how far we are from something like this (nothing good yet, very preliminary). Someone asked me about how well we play with sklearn at scipy. If we have more of a set framework here, we might be able to take advantage of it for things like this and cross-val etc. Just very vague and general ideas right now.For discrete I thought of creating a second predict method, predict_dist (or something like that) that creates a class with extra features or maybe just a scipy distribution instance.I don't know the details or how the pipelin is used in sklearn. But in an example I had some time ago it was easy to do a cross validation loop with predict and some distance measures for the prediction error. I have no idea how their automatic tuning parameter selection works.sklearn has a more consistent (or rigid) structure. But even if automatic compatibility with sklearn is not very high, examples like you said would show how to do it, or how difficult it would be.How would we change this?Deprecate fittedvalues and replace with linear_values and add what you propose as predicted_values? It breaks convention a little bit, but since it's ambiguous here, I think we should be clear rather than try to shoehorn everything into fittedvalues. Thoughts?I would really like to keep consistent attributes across models.
I think, the conditional expectation of endog/y based on the estimate, or the mle of yhat should be a general enough principle to get consistency across models, and it would be (almost ?) always a useful statistic.
We can point users to predict for explanations and other predictions which might be useful in specific cases.GLM also has mu (expected value, link.inverse(eta)) as fittedvaluesBackwards compatibility is a bit of a problem. There is not really a clean way to do it.My preference would be for 2)
Warnings are annoying but unavoidable.
But I have no idea at all how much user code might be affected by this.I tend to agree with 2), but staying on the bleeding edge with a lot of packages that I'm using everyday for research has led me to greatly appreciate deprecation cycles. That said, I don't know how many people are using discrete yet and we're still not near 1.0.At least Logit is very popular (based on blogs), maybe also MNLogitfor example http://blog.yhathq.com/posts/logistic-regression-and-python.html
uses predict not fittedvalues
I guess fittedvalues in discrete might not be very commonly used.
Our doc example also doesn't use fittedvalues. http://statsmodels.sourceforge.net/devel/examples/generated/example_discrete.htmlIf we double our downloads again next year, then 2) would get it out of the way faster. But I also have a very ambiguous feeling to changing parts that are pretty close to the core without considerable deprecation.going back to the description of the ticket: it's not just exp(x * beta) (that was for poisson IIRC)
Binary models have the predicted probability (with endog coded as 0,1 variable)
Logit for example 1/(1+np.exp(-X))related: discussion/enhancements on residuals for discrete #752Added a 0.6.0 milestone for this. Need to sort it out.This has come up again for me while working on the panel stuff bc there are even more options here. I'm almost inclined to do away with fittedvalues entirely and make users use predict, so you always know what you're getting.I like fitted values as the mean prediction by default, where fitted values are our insample prediction/expectation for endog, y. I added it also for GMM in IVGMMResults where it makes sense y - f(x) fittedvalues = f(x, theta_hat)resid = endog - fittedvalues
ssr, scale and similar can be calculated from this.This definition of resid doesn't extend to discrete models and GLM though. Also see latest results class with panel.only looked at GLMit has self._endog-self.mu in several of the other resid_xxx.
resid might not be so interesting, if we need wresid (resid_xxx) for the statistics.But after looking at a lot of GMM it makes sense as a default (even if nothing applies for every model)
for GEE
https://github.com/statsmodels/statsmodels/pull/928/files#diff-1535b76fc844ea73fbe14e973545b14bR979GLM defines fittedvalues as muAnother thought: As a user I don't know why I should be interested in the linear prediction.
I always use the fittedvalues/predicted mean to check how well my model is doing. (That's why I ran into this in the first place.)Given that it means / can mean different things across different models, and that we can't have interactive documentation with the cached properties, I still think y - predict() with default as the mean is better, but I don't really care if there's a shortcut.Just make a PR with a warning?And document somewhere that fittedvalues will always be the mean.Yes,The difference between fittedvalues and predict() is caching. Since we need to use fittedvalues in several places, it's better to calculate it just once.
Use fittedvalues if we want the default, use predict if we need to use the options.
(BTW: that's one of the patterns for the KDEUnivariate refactoring in PR #973 to gain speed for some standard use cases.)Ok, I just ran into this again.
Assuming Poisson fittedvalues are the mean (conditional expectation and conditional variance), I was surprised that I got negative numbers.I thought we already unified this.I gave up on this for 0.6. I don't like breaking backwards compatibility in a basic attribute, without being able to do a transition period.My current plan: replace fittedvalues by fitted_values in 0.7, and keep fittedvalues for some (a long) time.
(at the beginning I always tried to add the underscore in the middle of fitted_values)postponed again, we should fix this early in 0.8 milestoneprio-high, and still open after 6 years ?seven years and still openanother idea
add future warning that definition will change in two versions and add linpred as an alternative for users that don't want to use predict(linear=True) directly.AFAIR we avoid discrete fittedvalues in our internal code and won't have problemsAside, just checking source.
In count GSOC PR, we completely ignored fittedvalues, e.g. it is inherited by ZeroInflatedPoissonResults and similar but has no unit tests.
AFAICS from browsing some code, the current inherited fittedvalues is the linpred of the main model which is only one part/exog of the zero-inflated model fittedvalues should be predict(which='mean'), i.e. the conditional expectation of endog, which is the default of predict (endogandexog in ZI models are for main model, endog_infl, exog_infl` are for the binary model)",comp-discrete design prio-high
https://github.com/scikit-learn/scikit-learn/issues/18159,"Currently Pipeline doesn't support the sample_weight fit param. It fails with,passing the sample weight as logisticregression__sample_weight is annoying because it's too strongly coupled with the pipeline step names. This means that this code would break if a different estimator is used in the pipeline or else one need to use pipeline steps independent of the estimator used. In any case pipelines constructed with make_pipeline would break.I haven't followed the sample props discussion in detail #4497 but is there anything preventing from adding a sample_weight parameter to the pipeline and doing one of the following?The case 1. would already likely address most use cases. Are there drawbacks in doing this?To clarify, given an arbitrary classification or regression pipeline, currently to fit it with sample weights one has to do something like,This is not very user friendly or readable.The only example I could find that needs sample weights passed to an intermediary step is https://github.com/scikit-learn/scikit-learn/blob/master/examples/text/plot_document_classification_20newsgroups.py#L294-L297.I would say that for a user expecting this behaviour, it will be fine. For the one expecting, sample_weigth to be dispatch everywhere, it will make them angry. At least now, we just annoy everyone.I would think that you have cases that it could be wrong: you use null weight to discard samples and if you have a decomposition in your pipeline then your space will be different taking into account the sample_weight or not. I am just afraid to make it works without a user knowing what happens (and writing it in the doc does not count, who read the doc :))Right, I understand SLEP006 is much more general but so far I don't understand how it solves this immediate problem. Say there is a black box estimator and you want to pass sample weight to it. I would expect to be able to do,If that estimator happened to be a pipeline, then one of the two options above (with a personal preference for 2.) should still happen without additional information. SLEP006 can make it nicer to specify which estimator gets sample_weight (now one can specify '*__sample_weight' to some extent), but I would still want a reasonable default behavior that addresses most use case when using just the sample_weight fit param and building the pipeline with make_pipeline. Having as only option to use the props fit param (or having to call set_props_request) breaks the API expectations IMO (it's fine to have it as one of possible options). For instance, we can't deprecate normalize in Ridge (#17772) saying that using pipe = make_pipeline(StandardScaler(), Ridge()) is the same thing until we have sample_weight support in the Pipeline.Assuming one chooses option 2, can one already of a lot of it with checking if sample_weight is in the fit signature and if so passing it. I guess this corresponds to the solution 1 in https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep006/proposal.html#solution-1-pass-everything but raising an error if none of the steps accept ""sample_weight'. The question is why can't this already done for the the pipeline (i.e. handle pipeline as any other meta-estimator with sample weights) without waiting for the general implementation of SLEP006 ? What would be the downside of doing this?What I would personally expect is neither 1. or 2., but rather 3. that pipeline passes sample_weight to all steps and raises an error if any of the estimator doesn't support it (i.e. require all steps to have SW support).It seems that the lack of clear good default behavior is the reason we're forcing users to explicitly address each individual step. This avoids unexpected results.A lot of users have different expectations, and most definitely don't read the docs. I remember a Twitter shitstorm about people lecturing us because logistic regression is regularized by default...+1 sorry, I meant that it should have been the option 2. I wrote raises a warning if no estimator supports it there, but I meant raises an error.I guess, I'm questioning this statement that there is no good default behavior (e.g. use option 2 /3). For instance, even if say StandardScaler supports sample_weight one day, whether we pass sample weight to it before the classifier, either way it will not be incorrect. It's a (debatable) choice that we can document that will be good for 95% of use cases, and for the remaining ones users can pass estimator__sample_weight explicitly or use sample props for more control.Stretching this argument of ""no good default"", one could say that there is no good default for hyper-parameters and therefore they should be always passed explicitly. We don't do that because usability matters. Default are not about always providing the ideal solution, but about a reasonable value that works for most frequent use case and that might provide sub-optimal but not incorrect results otherwise.Or at least I'm looking for concrete counter-examples to this argument among frequent use cases of pipelines.fair pointJust to clarify, I think 3) is still different from the newly updated 2). In 3), all steps are required to support SW while 2) is happy with just one of them supporting SW. I updated my comment to make it clearerOr it might not be when going back to the definition of sample weights. If setting some of them to zero should be equivalent to removing those samples (#15657) or more practically passing check_sample_weights_invariance(..., kind=""zeros"") check.For instance Ridge passes that check, but a pipeline of StandardScaler + Ridge fails when sample_weight are only passed to the classifier,producesso approach 1) is likely not correct. In this example, the correct approach would be indeed option 3 (i.e. pass sample weight everywhere), but I'm not sure if it's a general property. However until all estimators support samples weight (e.g. #15601 for StandardScaler), I feel the most pragmatic/useful solution would still be option 2 (i.g. pass to all estimators that support them at fit). This is also what the TPOT library does for ML pipelines BTW.Difficulties with option 2:Difficulties with option 1:I don't have any difficulties with option 3 except that it doesn't solve your issue.a good first step for me is: if sample_weight is passed to fit in pipeline then all steps need to support it.then we can allow estimator__sample_weights in a next PR to specify to which step you want to pass.to me this ensures backwards compatibility. It cannot silently behave differently.wdyt?I agree. If we want a sample_weight param without explicitly providing the routing information I also don't see a way around this. I would add: all steps need to steps need to support it, or explicitly indicate e.g. via estimator tags that each sample is processed independently (for instance in Normalizer) and therefore sample_weight do not apply.In practice this means that we would need to add sample weight to preprocessors and ColumnTransformer to make this useful.Just a note that the if all steps support sample_weight is not trivial since a meta estimator may accept **kwargs and we don't know if sample_weight is really supported there or not.Yes, we would need some way to determine if an estimator supports sample weight irrespective of the fit **kwargs. For instance an estimator tag fit_sample_weight with one of: True (supports), False (doesn't) and None (not applicable, operates on samples independently). This could probably be mostly auto-determined, and might just need to be fixed manually in a few cases.As long as we don't remove sample_weight support from estimator, it would be backward compatible: a pipeline that supported sample weight at one versions will also support it for following versions.For ref, I tried that in #13565. It wasn't that easy, but some of the issues I encountered about inheritance should be solved now. This one is however still relevant:Interesting.One solution could be to set it to has_fit_parameter('sample_weight') by default in BaseEstimator, that way it would also work for third party estimators. We could even use a helper function that fallbacks to has_fit_parameter('sample_weight') if that estimator tag is not defined.I recall that @jnothman did not like to use has_fit_parameter for sample_weight: #9041 (comment)
@jnothman do you have other arguments that I don't recall?",New Feature
https://github.com/ShipSoft/FairShip/issues/347,"Here my experience using a fesh ISO image.run_simScript works. Continue now checking reco and event display.alpaca issue fixed.does anybody know why my ROOT build takes more than 15GB? all in interpreter/llvm 14.9GB!-Dvmc=ON looks like a red hering, we already have:
-Dbuiltin_vdt=ON
-Dvmc=ON
${PYTHON_ROOT:+-DPYTHON_EXECUTABLE=$PYTHONHOME/bin/python} \ALICE doesn't have a VMC aliBuild package yet, so we will probably have to create our own based on the installation instructions: https://vmc-project.github.io/installation/vmc/Could we enable shipdist issues to track tasks like this?Concerning the ROOT size, no idea...This might be a bug. I'll ask on the ROOT mattermost... The release-notes for 6.20 don't mention VMC.Error in TStreamerInfo::Build: UpstreamTaggerHit, discarding: const double* mom, no [dimension]
???studying the million lines long root build output:
CMake Deprecation Warning at cmake/modules/RootBuildOptions.cmake:383 (message):On my system ROOT v6.20.4 has vmc (only minor changes to the recipe which should not be related, see ShipSoft/shipdist#45), so this seems to be some Ubuntu specific problem. Which CMake do you use?I suggest we make a new issue for this. When does this occur?About 3.1 GB for me.on vmc: Yes, ROOT v6-20-04 has vmc, but its use is disabled. It is only possible to switch it on by changing the cmake file.on TStreamerInfo: this appears when running ShipReco. Since it was not reported earlier, thought it is related to new ubuntu, new root.on root size: I think in the past, this was related to using debug option. However, unable to find out if it is build with debug option, in principle, should not.Sorry if I was unclear: without changing the CMake file, vmc is built as it should be for me (on Fedora 30 and 31) with ROOT 6.20.04 when setting -Dvmc=ON, so there has to be more to this issue than just the ROOT version.At what point during build does vmc problem arise? What package could not compile without it?It's currently a sub-package of ROOT, so part of the ROOT build. You can check whether it was built by looking at root-config --features.I confirm that on my Ubuntu 20.04 vmc was build by default, without changing CmakeList
cxx11 asimage builtin_afterimage builtin_clang builtin_davix builtin_ftgl builtin_gl2ps builtin_glew builtin_llvm builtin_lz4 builtin_pcre builtin_tbb builtin_vdt builtin_xxhash builtin_zstd clad dataframe davix exceptions gdml http imt mathmore mlp minuit2 opengl pyroot pythia6 pythia8 roofit rpath runtime_cxxmodules shared soversion ssl tmva tmva-cpu tmva-pymva spectrum vmc vdt x11 xml xrootdMaybe it is an issue with CMake. What version do you have? I have 3.16.3. And do you also get many warnings and errors in the root log file?CMake Deprecation Warning at cmake/modules/CheckCompiler.cmake:106 (message):
Options cxx11/14/17 are deprecated. Please use CMAKE_CXX_STANDARD instead.
Call Stack (most recent call first):
CMakeLists.txt:123 (include)** Environment variable GSL_ROOT is set to:For compatibility, CMake is ignoring the variable.
Call Stack (most recent call first):
cmake/modules/SearchInstalledSoftware.cmake:454 (find_package)
CMakeLists.txt:168 (include)
....adding to ROOT CMakeLists.txt: cmake_policy(SET CMP0074 OLD) removes many of the above warnings/errors.any idea why this fails:2020-05-05@08:57:05:DEBUG:FairShip:ROOT:0: -- Performing Test found_stdstringview
2020-05-05@08:57:05:DEBUG:FairShip:ROOT:0: -- Performing Test found_stdstringview - FailedCHECK_CXX_SOURCE_COMPILES(""#include <string_view>
int main() { char arr[3] = {'B', 'a', 'r'}; std::string_view strv(arr, sizeof(arr)); return 0;}"" found_stdstringview)Because std::string_view is only available from C++17 onwards.
what c++ are we using? do we need to specify -std=c++17 in the build options?going to try with, package: defaults-fairship
env:
CXXFLAGS: ""-fPIC -g -O2 -std=c++17""I use the the CMake version from shipdist.This I fixed in ShipSoft/shipdist#45Currently we use 11, but would prefer if we could us the newest standard that doesn't break packages, ideally 17.string_view should be optional, but should lead to performance improvements.@ThomasRuf When running with --MuonBack do you have this error
Error in <TStreamerInfo::WriteBuffer>: The element FairMCPoint::fLength type 209 (double) is not supported yet?
I have tones of lines of errors for <TStreamerInfo::WriteBuffer> and different classes. Although the scripts finishes OK, I guess data may be corrupted.I am not so far yet with running MuonBack. However, observed a similar message when running reconstruction, see above:Error in TStreamerInfo::Build: UpstreamTaggerHit, discarding: const double* mom, no [dimension]Here my experience using a fesh ISO image and
CXXFLAGS: ""-fPIC -g -O2 -std=c++17"", CMAKE_CXX_STANDARD: 17 (default-fairship.sh)autotools.sh
+tag: v1.6.3git clone https://github.com/vmc-project/geant4_vmc-diff --git a/protobuf.sh b/protobuf.sh
index e613463..e84e254 100644
--- a/protobuf.sh
+++ b/protobuf.sh
@@ -1,18 +1,20 @@
package: protobuf
-version: v2.6.1
+version: v3.11.4
source: https://github.com/google/protobuf
build_requires:-rsync -av --delete --exclude=""**/.git"" $SOURCEDIR/ .
-autoreconf -ivf
-./configure --prefix=""$INSTALLROOT""
+cmake $SOURCEDIR/cmake \diff --git a/openssl.sh b/openssl.sh
index ec6d930..a806a4d 100644
--- a/openssl.sh
+++ b/openssl.sh
@@ -2,9 +2,8 @@ package: OpenSSL
version: v1.0.2o
tag: OpenSSL_1_0_2o
source: https://github.com/openssl/openssl
-prefer_system: (?!slc5|slc6)
prefer_system_check: |Could you please use code formatting? That would make it much easier to read.Here I think we can just force our lhapdf5 recipe to use python2. As we only use the python bindings during the build to download the pdf-sets (which we could even do without the python bindings using curl), we aren't constrained to match the python version used later.The only issue could be that slc6 python2 might be too old, so using curl might be the most robust solution.Sorry, forgot about the geant4 issue:
setenv G4PARTICLEXSDATA /media/disk0/SHiPBuild/sw/ubuntu2004_x86-64/GEANT4/geant4-10.6-release-1/share/Geant4-10.6.1/data/G4PARTICLEXS2.1",None yet
https://github.com/sjvasquez/handwriting-synthesis/issues/17,"Any idea about this error? I get it when running python demo.py
18.04 Ubuntu running stock Python 2.7.15rc1
Have installed:
apt install python-tk
pip install numpy svgwrite matplotlib scipy tensorflow pandas sklearnFull error:
Traceback (most recent call last): File ""demo.py"", line 153, in <module> hand = Hand() File ""demo.py"", line 37, in __init__ attention_mixture_components=10 File ""/home/john/handwriting-synthesis/rnn.py"", line 85, in __init__ super(rnn, self).__init__(**kwargs) File ""/home/john/handwriting-synthesis/tf_base_model.py"", line 108, in __init__ self.graph = self.build_graph() File ""/home/john/handwriting-synthesis/tf_base_model.py"", line 399, in build_graph self.loss = self.calculate_loss() File ""/home/john/handwriting-synthesis/rnn.py"", line 204, in calculate_loss lambda: self.sample(cell) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func return func(*args, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2063, in cond orig_res_t, res_t = context_t.BuildCondBranch(true_fn) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1913, in BuildCondBranch original_result = fn() File ""/home/john/handwriting-synthesis/rnn.py"", line 203, in <lambda> lambda: self.primed_sample(cell), File ""/home/john/handwriting-synthesis/rnn.py"", line 162, in primed_sample scope='rnn' File ""/home/john/handwriting-synthesis/rnn_ops.py"", line 246, in rnn_free_run states, outputs, final_state = raw_rnn(cell, loop_fn, scope=scope) File ""/home/john/handwriting-synthesis/rnn_ops.py"", line 40, in raw_rnn if context.in_graph_mode(): AttributeError: 'module' object has no attribute 'in_graph_mode'I had the same issue and udnaan's commit fixed it for me.System:Hi @ssell , please let me know how did u solve the issue.
I was getting the error: #22. but, when i removed the lines 9, 29 and 30 as suggested by you, i am getting the 'in_graph_mode' error. Kindly help me.I got it! ChenXiaoTemp's pull request has the solution.",None yet
https://github.com/PythonOT/POT/pull/112,"Hello,I am sending this PR to correct the variable names. For now on, you will find. ot/stochastic.py
. test/test_stochastic.py
. example/ plot_stochastic.pywhich have been cleaned.What about variables such as ""a"", ""b"" and ""M""? Don't we want to make those more readable as well? Especially ""M"" which I understand is the cost?Hello,In order to keep compatibility you shoudl use this decorator (should be added in utils.py wit propoer documentation and exmaple)on this exampleyou can call the function func with the new (param2) or old (param2_old) and it will work on both but print the following message if calling with the old parameter :The decorator can handle any number of parameter name switches. it should provide the proper documentation for the function also.Note that there is no need to do decoration if you change the name of non keyword variables since the code will still work.Hello everyone,I pushed a new version for variable and argument names. I included the decorator from @rflamary in utils.py and used it for ot.stochastic. You can see that it takes numItermax and returns maxiter. Furthermore, I came back to old test (with numItermax as argument) and extended them with the new argument names + new variable name.The variable name for the maximum number of iterations is max_iter in sklearn and not maxiter, but I don't want to be too nitpicky. You can change it or not, your choice, I just thought it is goot to know since we used it to decide the name.suggestion: perhaps gamma -> plan or plan_ot ?@ngayraud I also prefer max_iter but I thought we were using maxiter. Anyway, this is the right time to decide which one we use. I also like the way to short log_stoc.@hichamjanati gamma variable name comes from the documentation where we defined the OT plan as gamma. I would prefer to have a consistency between the documentation and the variable names.To be honest, gamma only ""speaks"" to people that are very familiar with the annotation. something like ot_plan would be more appropriate IMO. Also, if we can change maxiter to max_iter, it would b following the variable naming convention we are aiminng for.I agree with max_iter, I will update the PR. I am also fine with 'ot_plan', my point was that it needs to be consistent with the documentation and we will need to change it as well. @rflamary, @ncourty what do you think ?Hello all,I have added new stuffs.
\gamma -> OT_{\text{plan}}
maxiter -> max_iter
correction of a bug for sphynx library.",None yet
https://github.com/bstriner/keras-adversarial/issues/17,"Hi,I just installed keras-adversarial, and tried to run the MNIST example. I got this error:Do you know what this graph_editor module is?Running on Bash on Windows 10.What TF version are you on? Should be able to just print tensorflow.__version__. That module is in the right place in current 1.1 but I don't know if it has moved around.It is in the contrib module, so maybe that wasn't included in whatever version you are running.https://www.tensorflow.org/api_guides/python/contrib.graph_editorGraph editing is required for lots of advanced optimization. It is built-in to Theano. It was added to TF to try to compete with Theano but that might only have happened recently.BTW, how is TF on bash on Windows working for you? I've been stuck using only Theano on Windows. Performance and CUDA all works out OK?Cheers",None yet
https://github.com/oasis-forever/nlp_tutorial/issues/4,,None yet
https://github.com/scikit-learn/scikit-learn/issues/15880,"The signature of plot_confusion_matrix is currently:sklearn.metrics.plot_confusion_matrix(estimator, X, y_true, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None)The function takes an estimator and raw data and can not be used with already predicted labels. This has some downsides:Suggestion: allow passing predicted labels y_pred to plot_confusion_matrix that will be used instead of estimator and X. In my opinion the cleanest solution would be to remove the prediction step from the function and use a signature similar to that of accuracy_score, e.g. (y_true, y_pred, labels=None, sample_weight=None, ...). However in order to maintain backwards compatibility, y_pred can be added as an optional keyword argument.We should definitely stay backward compatible, but adding a y_pred keyword arg sounds reasonable to me. We should raise an error if y_pred is passed but X or estimator are also passed.Would you want to submit a PR @jhennrich ?I submitted a PR, but I think there is currently a problem with the CI so it has not passed yet.I agree that we should support plot_XXX(y_true, y_pred) to avoid calculating the prediction for multiple times.
We also have similar issues in plot_roc_curve and plot_precision_recall_curve.
Adding y_pred seems acceptable, but honestly I don't think it's a good solution.
For those functions which accept **kwargs (e.g., plot_precision_recall_curve), seems that it's impossible to keep backward compatible?Why is it impossible to keep the backward compatibility? It seems to me that the proposal in #15883 is OKbecause we do not support **kwargs in plot_confusion_matrix. @NicolasHugWhy is kwargs a problem?Hmm, so there's another annoying thing, we support **kwargs in plot_roc_curve and plot_precision_recall_curve (and plot_partial_dependence), but we do not support it in plot_confusion_matrixif we add the new parameter before **kwargs, we can keep backward compatibility, right?The changes in my PR are backwards compatible and **kwargs can still be added. But I agree with @qinhanmin2014, a much much cleaner solution would be to throw out estimator and X and use positional arguments (y_true, y_pred, ...) that are consistent with most of the other sklearn stuff.yesUnfortunately that would require a deprecation cycle (unless we make it very fast in the bugfix release but I doubt it...)@thomasjpfan , any reason to pass the estimator as input instead of the predictions?Thanks, let's add y_pred first, **kwags is another issue.This seems impossible, sighI agree that we need to reconsider our API design. also try to ping @amuellerIf a user wants to provide their own plotting part and provide their own confusion matrix:This can similar be done for the other metric plotting functions.The plot_confusion_matrix is kind of designed like the scorers which are able to handle the output of estimators nicely. In other words, it is a convenience wrapper for interacting with ConfusionMatrixDisplay and the estimator.By accepting the estimator first, there is a uniform interface for the plotting functions. For example, the plot_partial_dependence does all the computation needed for creating the partial dependence plots and passes it to PartialDependenceDisplay. A user can still create the PartialDependenceDisplay themselves, but in that case it would be more invovled.Although, I am open to having a ""fast path"", allowing for y_pred to be passed into the metrics related plotting functions, which will be passed directly to confusion_matrix and let it deal with validation.The computation of the predictions needed to build a PDPs are quite complex. Also, these predictions are typically unusable in e.g. a scorer or a metric. They're only useful for plotting the PDP. So it makes sense in this case to only accept the estimator in plot_partial_dependence.OTOH for confusion matrix, the predictions are really just est.predict(X).I don't think we want a uniform interface here. These are 2 very different input use-casesEDIT: In addition, the tree-based PDPs don't even need predictions at allThere are other things we will run into without the estimator. For example if plot_precision_recall_curve were to accept y_pred, it will need pos_label because it can not be inferred anymore. In this case, I would prefer to use PrecisionRecallDisplay directly and have the user calculate the parameters needed to reconstruct the plot.This comes down to what kind of question we are answering with this API. The current interface revolves around evaluating an estimator, thus using the estimator as an argument. It is motivated by answering ""how does this trained model behave with this input data?""If we accept y_pred, y_true, now the question becomes ""how does this metric behave with this data?"" This data may or may not be generated by a model.It's true that in this specific case, @jhennrich you could directly be using the ConfusionMatrixDisplay.One drawback is that you need to specify display_labels since it has no default.@thomasjpfan do you think we could in general provide sensible defaults for the Display objects, thus still making the direct use of the Display objects practical?For some parameters, like display_labels, there is a reasonable default. The other Display object parameters can have reasonable defaults as well. Some parameters must be provided tho. For example, confusion_matrix must be provided for ConfusionMatrixDisplay or precision and recall for PrecisionRecallDisplay.One classic pattern for this kind of thing is defining:but this is not very idiomatic to scikit-learn.I start to get confused. The goal of current API is to avoid calculating for multiple times if users want to plot for multiple times, but if we accept y_true and y_pred, users still don't need to calculate for multiple times? (I know that things are different in PDP)@jnothman That API is pretty nice looking!@qinhanmin2014 Passing an estimator, X, y or y_true, y_pred works in satisfying the ""do not calculate multiple times"" API. In both cases, the confusion matrix is computed and stored into the Display object.The difference between them is where the calculation of confusion matrix starts. One can think of pass y_pred as the ""precomputed"" value of the estimator.So I think y_true, y_pred is better than estimator, X, y (not in PDP of course), because sometimes (often?) users not only want to plot the predictions, they also want to analysis the predictions. With current API, they'll need to calculate the predictions for multiple times.For metrics, I can see the preference toward using y_true, y_pred over estimator, X, y. Imagine if the plotting for metrics support only y_true, y_predCurrently the API looks like:I would prefer to have an API that supports both options (somehow).Yes, this is what I mean.I think this is a practical solution. An annoying thing is that we can only add y_pred at the end (i.e., plot_confusion_matrix(estimator, X, y_true, ..., y_pred))I think it would be great to resolve this in some way @adrinjalali do you want to discuss it in the next meeting maybe?I sometimes have nightmares about this issue. Maybe we can add a static method that takes the output of the metric directly:For roc curve:On a side note, from looking at codebases, I think more users are familiar with the metrics interface than the score interface.Oh no :(I think this is definitely true. But I'm also quite certain that people use y_pred when they should be using y_score and are getting wrong results because the interface doesn't tell you that you need to do something different and no-one ever reads the docs.I'm not sure how the static method you propose is different from the constructor but maybe I'm overlooking something.Hi, I've just up-voted the issue - as a long-time sklearn user, I found the current API for plot_confusion_matrix very... well, confusing. I really like its addition (less copy-pasting), but the metrics functions always used the (y_true, y_pred) scheme which is just more flexible and what I have already been used to.In my case it doesn't make sense to pass an estimator in, as it's a very slow model and I'd rather load the predictions from a file than re-run it every time I want to analyze the results. I'm happy to have found out in this thread there's a work-around using the *Display object, but its discoverability is not great - I would suggest at least adding that to plot_confusion_matrix documentation or maybe confusion matrix user guide?Thanks for your input. If the current API is confusing, it would increasing make more sense to move to a more metrics-API like interface and go through a painful deprecation cycle.The biggest concern we have with using the metrics interface is:@pzelasko What are your thoughts on this matter?@thomasjpfan I understand the issue, it's a tough one. Maybe a reasonable compromise would be to allow only keyword arguments for this function (now that you don't have to support Python 2 anymore)? Like: def plot_confusion_matrix(*, y_true, y_pred, ...). It's still different from the rest of the metrics, but 1) it has a good reason for that, 2) it is at least using the same type of inputs as the other functions.Anyway, I know why you're hesitant to make any API changes, that's why I suggested to at least mention the work-around in docs. (I've actually read them numerous times and I really appreciate them!)The current way to use y_true and y_pred is shown here: https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#create-confusionmatrixdisplayI know I am stretching here but what about this:where the second position accepts y_true as predictions if estimator='precomputed.if you want to stretch even more I would prefer plot_confusion_matrix((estimator, X, y), ...) or plot_confusion_matrix((y_true, y_pred), ...) but I am not sure it is solving the issues raised by @amueller regarding the metric-like APIThere are a few new plotting utilities where allowing a metric API would really make sense:I understand the issue that @amueller mentioned about needing to pass pos_label etc., but this isn't an issue for any of the aforementionned functions.Are we OK to support both scorer and metrics API for these two? We don't need to worry about backward compatibility there.I am still for my suggestion of using precomputed, which we commonly use in our estimators. In this case, the signature would be:I'll put together PR to see what this looks like.I'm not really discussing the API yet, I'm only asking whether we're OK to support both options for new PRs.(But regarding the API, I don't think 'precomputed' helps much: what do we do about X? I think we should just keep (y_pred) and (estimator, X) mutually exclusive, by properly erroring. Also what does it mean for an estimator to be precomputed?)Or estimator='none', estimator='predictions', estimator='precomputed_predictions', and then X becomes y_pred or y_score. It's almost like how we handle precomputed distances with X in estimators.How are we going to support both options? With two functions?I would have also liked:which would be two methods.Guillaume's suggestion of using tuples #15880 (comment) is one option. I think it would have been the best option if we had started from there from the beginning. But I'm afraid using tuples breaks consistency with the existing utilities.plot_XYZ(estimator=None, X=None, y=None, y_pred=None) with mutual exclusion is another option, and it's the one I'm advocating for, for now.I like CalibrationDisplay.from_estimator(...), but as Andy noted, we'd need to remove the plot_XYZ functions then. It might be worth considering.I think we can move to tuples and deprecate the current behavior. (As long as we agree to use tuples)So this seems like discussing the namespaces, right?
Whether we have one function and one constructor, or two classmethods, or two functions, it's exactly the same functionality and basically the same code.@pzelasko @jhennrich how do you feel about having two classmethods or two functions? Or would you prefer a single function, which is a bit messy in python.And if you prefer two functions or two classmethods, do you see any benefit despite discoverability? Discoverability might be enough of a reason to do classmethods though, I don't see a strong argument for having two functions.Could we add the blocker label here? It seems that it is preventing progress on #18020 and #17443 (cc @cmarmo)The blocker label is for release blockers (things that absolutely need to be fixed before a release), not for PR blockersAhh good to know.I like the two-classmethods approach the most, especially the from_xxx pattern - sth like @thomasjpfan proposed:",module:model_selection
https://github.com/scikit-learn/scikit-learn/issues/14094,"Today, sklearn.feature_selection.VarianceThreshold.inverse_transform method fills in zeros to features which were removed for having too-small variance.
This is certainly predictable, easy to implement and easy to explain.However, filling in zeros without respect to the data passed to fit means that the reconstruction error can become arbitrarily large. For example, suppose that one of the features in your data always takes the value 10**6. This clearly has zero variance, since it always takes the same value; however, filling in zeros for that feature when the data via transform and inverse_transform will result in an output which dramatically differs from the input.Instead, I think it would make sense to fill in the mean of the columns when using inverse_transform. The means can be computed and stored from the data passed to fit. This will make the reconstruction of the transformed data via inverse_transform more closely reflect the data that was passed to fit because any columns which are removed for having variance less than threshold must, by definition, be tightly grouped about the sample mean.Naturally, in the special cases where the sample means of input features passed to fit are already zero, the proposed inverse_transform method will function the same way it does today, as it will fill in zero values for those features.In terms of code, this just means keeping an array of column means in addition to the indices of the removed columns.Of course, it's possible that I have missed an important subtlety, or there is a competing concern which outweighs the argument that I've outlined here. If that's the case, I'd like to know what I've missed!That inverse_transform is shared with other feature selector, but I'm okay with this being specialised. To maintain backwards compatibility this might need to be controlled by a parameter which would change from ""zero"" to ""mean"" over a deprecation period. PR welcome.@jnothman I'd like to work on this if that is okay.@theoptips and I want to work on this oneWorking with @MDouriez adding myself as a participant. #scikitlearnsprintSo I'm not sure if this change will be beneficial. Why should VarianceThreshold have different behavior from other feature selection methods?
You could argue the same for other feature selectors, right?I'm also not sure I understand the semantics of looking at reconstruction error in feature selection. @Sycor4x can you explain why you're doing this or what the interpretation of this is?@jnothman did you mean adding a parameter to the method or the class? Do we want to later deprecate the parameter or keep it?",Enhancement Needs Decision
https://github.com/scikit-learn/scikit-learn/pull/15744,"Fixes issue #14398By default, sklearn's GMM is initiated by k-means. When the features are of very different scales, this can lead to a poor choice of initial conditions for the GMM parameters and, as a result, a poor fit once trained. Have added:I see what you mean, but hopefully they're complimentary... Maybe...I think the current PR is a small change that should hopefully improve the default GMM implementation. From what I can gather, #11101 is suggesting more advanced options that can be used instead of the default?I'm not sure if we can change this without ensuring backwards compatibility. Are there cases where this change in behaviour will be detrimental, such as with heteroscedasticity?Hmmm... I think we'd have to try quite hard to come up with an example where scaling before applying k-means is a bad idea. The only one I can think of is the situation where the clusters are circular (before scaling), and are really far apart, so that the scaling distorts them. Even in that case though, the clusters are so separable that k-means works fine. Here are before and after scaling pictures from an example I ran this morning:Either way, I think it's fair to say that scaling data before applying k-means is good practice. See:https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clusteringfor example. I suppose a light version of this solution would be to change the docs to say something like 'Although GMMs don't need scaled data, our code is initiated by k-means so we recommend scaling it anyway' ?PS. Apologies - not sure what you meant by heteroscedastic. I know this to mean 'input dependent noise variance' in the context of regression, but haven't seen it in a GMM context before?I meant for instance if you have a gaussian mixture, but your gaussians have very different variances.OK that breaks it :)Can't guarantee that it'll be better every time, just that it's best practice. Like I said, I'd be happy to make a change to the docs as a lighter option if you're worried about back compatibility.Well if we had a sense of the risks, we could make a conservative change like adding a kmeans-scaled initialisation approach, and perhaps change the default through a deprecation cycle.Do you find that the random data approach previously proposed works for your problem?Yes sounds sensible. Sorry, what do you mean by the 'random data approach'. Do you mean the solution proposed in #11101?OK you'll have to help me out... How can I fetch their branch? I'm at the edge of my git expertise :)Hi guys, I've had a few emails asking about this so I thought I'd try to move it on a bit. I merged with upstream master but now the tests don't work? I get this:Sorry if I'm being a bit dim...That looks like that the module (from Cython) has not been build. You can look at the building from source guide to get more information about building.Thank you @thomasjpfan :)",module:mixture
https://github.com/r9y9/nnmnkwii/issues/99,"/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.data module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.
warnings.warn(message, FutureWarning)This of course does not affect the functionality of the package, but it would be optimal to fix it before it is too late.Thank you very much!Could you show which part of nnmnkwii causes the warning?@r9y9 It shows when I execute from nnmnkwii import preprocessing as P. It also shows when training your wavenet_vocoder.Thanks! This should be fixed by #100.@r9y9 Thanks for your fast response! If I install nnmnkwii using pip, how soon will the fix be reflected?Could you install nnmnkwii by the following command?This should install the latest development version of nnmnkwii.The fix is landed on the latest release (v0.0.21), so we can install it with just pip install nnmnkwii. Sorry for the inconvenience again!",bug
https://github.com/r9y9/wavenet_vocoder/issues/203,"I am trying synthesize wave from some predicted mel(.npy) file i got from tts. I follow the read file that synthesis.py may not work (in factlly I also get error when using this ) So I use the evalutae.py. But errors occur:~/AI_/wavenet_vocoder# python evaluate.py --data-root=./output_mel/ ./wavenet_premodel/20180510_mixture_lj_checkpoint_step000320000_ema.pth generated
/root/anaconda3/envs/keras/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.preprocessing.data module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.
warnings.warn(message, FutureWarning)
Using TensorFlow backend.
Command line args:
{'--data-root': './output_mel/',
'--file-name-suffix': '',
'--help': False,
'--hparams': '',
'--initial-value': None,
'--length': '32000',
'--num-utterances': '-1',
'--output-html': False,
'--speaker-id': None,
'': './wavenet_premodel/20180510_mixture_lj_checkpoint_step000320000_ema.pth',
'<dst_dir>': 'generated'}
Traceback (most recent call last):
File ""evaluate.py"", line 70, in
test_data_loader = get_data_loaders(data_root, speaker_id, test_shuffle=False)[""test""]
File ""/root/AI_/wavenet_vocoder/train.py"", line 861, in get_data_loaders
random_state=hparams.random_state))
File ""/root/anaconda3/envs/keras/lib/python3.6/site-packages/nnmnkwii/datasets/init.py"", line 111, in init
collected_files = self.file_data_source.collect_files()
File ""/root/AI_/wavenet_vocoder/train.py"", line 127, in collect_files
with open(meta, ""rb"") as f:
FileNotFoundError: [Errno 2] No such file or directory: './output_mel/train.txt'Why it needs train.txt? I mean 'wavenet_vocoder ' should generate the wav if I just give the mel files input?If you see this isues please help me about this ,Thank you.",None yet
https://github.com/scikit-learn/scikit-learn/pull/16079,"Taking a stab at proposal 4 of the sample prop proposal (scikit-learn/enhancement_proposals#16).Clear WIP, just putting it here for now for us to see how it'd look.The first change I've made compared to the proposal is that the dict
storing the required props, includes a key as which method requires
which props. This way score and fit and predict can all have different
requirements.Here's an example with a pipeline. @jnothman would you please check if it's [almost] what you had in mind?@NicolasHug you were interested in this one as well IIRC.That example looks nice! I think I had not envisaged that all requested props were required, i.e. an estimator could request sample_weight by default but only receive it if available. What are your thoughts on that?What I'm not sure about is whether we need to specify that the request is for fit. I suppose that makes things more flexible.I have looked at your modification to clone. Okay. But I've not yet looked at much elseI also first thought of having it as ""please give me that prop if you have it"" rather than ""I really need it"". The reason I have it as a mandatory requirement, is that it kinda removes the chances of a meta estimator not passing a required prop by mistake. My gut feeling was that it removes some of those bugs.I think your other proposals had a more flexible system for routing the props around. Especially when it comes to fit/score/split distinction. Also, I thought this makes it easier to have some props going to transform/predict as well.But this would be a mistake in meta-estimator implementation which should not be the user's problem?This looks good!Edit: previous comment was irrelevant after I finally read all of #4497 and #SLEP006. My initial conception (and hacky implementation in personal project) looked very similar to the single argument, no routing approach.Now for some comments (sorry if these are still uninformed):I guess my main question/point is: can this be done in two phases? Phase 1 would be implementing the basic infrastructure (i.e. changing a lot of signatures thought the API) so that sample_props gets passed correctly. Changes would also have to be made to fold in sample_weight and other existing sample properties. Phase 2 would be routing, requesting/requiring, etc.That said, how can I be of use to get this done?This is good, but it needs work :)Happy to work through this more or assist with the changesI think the consensus at the last meeting was to rename the prop request to a metadata request.How is this going? Is there something I can help with here?The code is now a bit simpler, tests on sklearn.metrics pass, and test_props.py also passes.I need to support the old behavior in grid search and pipeline somehow as well.What do you think of the idea above, to get rid of set_metatdata_request and instead just add request_sample_weight parameters:With this approach, the keys supported by a consumer become explicit, giving us:Disadvantages include needing to add a parameter to every estimator that supports weighting, and hence increased friction if we want to add more user-configurable requests.I don't like adding these parameters to the constructor. I'm still hoping we'd use this as a starting point for not having all parameters as constructor parameters.Other than that, the API you propose also doesn't have the flexibility for the routing we have right now. You can't route different sample weights to different estimators or different metadata to different verbs (fit, transform, etc), which the user might want to control and not leave it to the developer of the estimator.I'm happy to introduce helper functions such as set_sampleweight_request etc to make it easier for people to set the routing for the usual cases though.I also tried another way to have an easy way to handle common fit parameters like sample_weight. You can see the remnants of the work here.I thought of having a Mixin for each parameter, like FitRequestsSampleWeight which would set a parameter such as _metadata_request__sample_weight, and the get_metadata_request would merge all such attributes together. The issue I encountered was how the user would change the default settings. If they do it through the _metadata_request, then some sort of overriding mechanism should exist, which is tricky if we allow users to pass a given prop to multiple ones, which they can do now: a single sample_weight can be passed to fancy_sample_weight and nonfancy_sample_weight for example, if the estimator would accept both.Yes, the assumption is that the verbs relevant to a parameter is determined by the meaning of the parameter to that consumer. So that parameter knows how to consume sample_weight in fit, it will consume it in fit. You're right that this might be a problem for sample_weight in score.A possibility: A method called request_sample_weight could more explicitly define which verbs sample_weight can be requested for ...?I'm not entirely sure what the issue is here. The consumer has a notion of sample_weight. if it had a notion of multiple different weights, like fitting_weight and calibration_weight or something, then each of them is a separate request. The semantics of something like _metadata_request__sample_weight are that the consumer knows about something called sample_weight, but the request instructs a metaestimator where to get it from.should we validate expected props in set_metadata_request? we validate param names in set_params. We could, a bit like set params, check against the signature of the targeted method if **kw is not used.I'm not loving how _get_props_from_objs is used in *SearchCV. The logic doesn't appear very clear. Is the following a helpful tool to make this more explicit and less error-prone?Also:
Just thought of a tricky case that we probably won't handle for a while: a grid search over two different estimators where one of the estimators requests sample_weight and the other does not:Apologies for my patchwork reviews.Here I'm trying to make the tests more complete.I still think the set_metadata_request API needs some work. I find it funny that a for an estimator not supporting sample_weight, I can still run .set_metadata_request({'fit': {'sample_weight': 'sample_weight'}}) without error and that for an estimator supporting sample_weight, I can run .set_metadata_request({'fit': {'sample_weight': 'ample_weight'}}) without error. (Hence my suggestion of request_sample_weight instead. I'd accept another solution, but am not really happy with the present one.)The rest is shaping up very nicely!Apologies for my patchwork reviews.Here I'm trying to make the tests more complete.I still think the set_metadata_request API needs some work. I find it funny that a for an estimator not supporting sample_weight, I can still run .set_metadata_request({'fit': {'sample_weight': 'sample_weight'}}) without error and that for an estimator supporting sample_weight, I can run .set_metadata_request({'fit': {'sample_weight': 'ample_weight'}}) without error. (Hence my suggestion of request_sample_weight instead. I'd accept another solution, but am not really happy with the present one.)The rest is shaping up very nicely!How about this kind of thing for consumers?:I think we need that deep parameter for clone. set_metadata_request needs to only pertain to the needs of that estimator as consumer, not as router, and this is what should be copied in clone. However, for the purpose of actually doing the routing, there needs to be a way to get the ""deep"" metadata request.On the topic of request_* methods:I think we should keep set_metadata_request, if only because of its use in clone, but we might emphasise that most users should not use it as request_* methods should be a simpler, more explicit way of accessing the same functionality. But I'm struggling to work out how to neatly make request_sample_weight appear in only the consumers of sample_weight.I'd now tweak the example in the previous comment such that neither fit nor score defaults to True or False. Both should default to None, and no change should occur.",None yet
https://github.com/scikit-learn/scikit-learn/issues/6240,"The SGD docs mention that ""Binary probability estimates for loss=”modified_huber” are given by (clip(decision_function(X), -1, 1) + 1) / 2."".I'm not sure what the contract is between predict() vs predict_proba(), but it's a little unintuitive that on the same input, predict() can work (return the correct classes) but predict_proba() return a uniform distribution over all classes (no differentiation between classes at all). This happens when all scores are clipped to the same value in predict_proba, including the best score.Is it worth making the documentation more explicit about this clipping and the predict/predict_proba divergence? Or is it understood from sklearn's API contracts?For binary classification, predict should return the positive class whenever decision_function is positive or predict_proba is larger than 0.5.For multiclass classification, the contract is that predict_proba should be (n_samples, n_classes), that probas are positive and sum to one for each sample and that model.predict(X_test) should return model.classes_[predict_proba(X_test).argmax(axis=1)] (up to ties).Probability estimates can be bad (approximate). If you want better calibrated probabilities, ensure that model is directly optimizing p(y|x) for instance using the log loss instead of the hinge loss or use a calibration wrapper:http://scikit-learn.org/stable/modules/calibration.htmlI think it's worth improving the docstring of this specific predict_proba implementation stating that for some choices of the loss, the built-in naive clipping mechanism can cause ties and that the user should either better use the log loss or instead calibration.Another fix would be to implement a less violent normalization that would not introduce ties while preserving the decision threshold at 0.5 for each sample.This would require recording per-class min and max decision values at fit time on the training set in order to rescale at prediction time but we would still need some clipping so I am not sure this is worth the extra code complexity as this would still be a hack. People really insteaded in predict_proba should use the log loss. People interested in ranking should use the raw decision_function output.Thanks! This is how I intuitively understood the contract too, which is why modified_huber tripped me up.Would be great if these two sentences could make it into the docs.I think we should actually not implement predict_proba if we don't train with log-loss and raise an AttributeError instead. Of course this requires a deprecation now.@piskvorky @ogrisel @amueller I used this probability estimate with modified_huber in the past too -- after reading about it in a paper by Tong Zhang or one of his students (~2003).
Did anybody check the reference in the docstring? https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/stochastic_gradient.py#L760I'm sure @mblondel has something to add here too.@amueller I think @piskvorky argument is solely about how we handle ties in predict_proba and not about the way we compute probabilies for modified huber, right?",Needs Decision
https://github.com/snipsco/snips-nlu/issues/861,"First, I ran the rust installer via rustup:I then relogged (to fill my environment).I then installed the other things required for compilation:I then tried to install snips-nluWhich results in it being unable to find the rust compiler, as required for multiple packages:It took me hours to build numpy on this platform, and now I can't even get snips working :(I clearly have the rust installer working, as I am able to compile the rust ""Hello World"" as it were:More system informationHi @DamonBlais ,
I think the sudo in sudo pip3 install snips-nlu may be causing the issue.
Can you try running this command without the sudo? (If you are using virtualenv, you shouldn't have to use sudo)
What does sudo rustc --version output ?Interesting.sudo rustc --version confirmed that rust was not a system package (command not found).I have re-run rustup as root (with sudo) now,The build still fails:I need the package to be available to an application running as root, so a user-local installation won't suffice here. Note that I'm not using a virtualenv, but that python-virtualenv was a dependancy of another portion of the application, I apologize for adding that confusion.Attempts to compile ""for my user"" pip3 install -U snips-nlu have resulted in hours of pip compiling scipy again... much slower for my user than as root (no idea why.)@DamonBlais it should work when you add it to your /root/.bashrcGuess you ignored the last line of the rust setup script
To configure your current shell run source $HOME/.cargo/env",None yet
https://github.com/scikit-learn/scikit-learn/pull/9059,"Fixes #8057Added support for sparse multilabel y for nearest neighbor classifiers. Firstly, it checks if the input to fit is sparse + multilabel and converts it to a dense one for storing. Also, it stores another parameter indicating whether the original input was sparse + multilabel or not. Now, in predict, if this stored value is true, then it converts y_pred to sparse CSC.
Also, added tests for the same.This PR wraps up @dalmia 's work in #8096without checking the tests for the momentWhen addressing this comment by @glemaitre, I stumble into some problems. I don't really know how to post a concrete question, so I collected my thoughts here as a PR to this PR and I would love to get some feedback. Thx.cc: @jnothman, @lesteve@massich Could you solve the conflict and address the remaining comments@dorcoh feel free to take over this one! let me know if you need anything",Stalled help wanted module:neighbors module:utils
https://github.com/scikit-learn/scikit-learn/issues/9453,"Transforming on y doesn't conform with our API. Also it returns X, y which again doesn't conform with our API right?Copying @bthirion's comment here -@agramfort, I think that Y could be avoided in the transform for PLSRegression, not for PLS Canonical -- that is more like CCA. However, this probably implies a thorough refactoring of the code.(Making an issue from a stalled PR #9160)+1 on refactoring PLS*. Other issues with requests include mine (#8934) and #9460.Who maintains PLS* ?I'll be happy to help indeed, but cannot invest much time in the refactoring.
Best.Would it be really problematic to keep the transform on y? I would say the ability to transform on y is really important for the cross_decomposition module - in fact it would be close to useless to not have it for PLSCanonical and CCA. I would also prefer to have it on PLSRegression, as the difference in the algorithm and rationale from PLSCanonical is small (but very important though) and there are still projections of the Y matrix, etc which are relevant, especially when using PLSRegression for multi-Y prediction (which is different from doing the same with PLSCanonical!)@Gscorreia89 I agree, but it's unclear on how to fit that into the current sklearn API.@Gscorreia89 Keeping the transform on y makes it impossible to use PLS in a pipeline, which is problematic when you want to use these methods as dimensionality reduction techniques.But I do agree that transforming y is also essential in other cases.Would it be acceptable to update the transform method to not transform y, so to stay in line with the API, but additionally add a new method that does the transformation on y? Something like a decompose method that maintains the current behaviour.@IvoMerchiers I understand, and I think its perfectly acceptable. Synthax-wise I am neutral, and a specific decompose (maybe even an inverse_decompose(scores) -> estimate ?) for this class of methods looks perfectly reasonable.@IvoMerchiers yes that is probably useful, but also requires a deprecation cycle. Also, I think we should really support 1d y, which we currently don't.",None yet
https://github.com/scikit-learn/scikit-learn/issues/10573,"In macro-averaged multiclass precision, and recall, the default set of labels should come only from the target outputs (y_true) not from the predicted outputs (y_pred).The target outputs come from the test set.
I believe it is a solid assumption is that the classes we are interested in (at all) are the classes present in the the test set.
We don't want to average over classes that we are not interested in.
In the case where a label occurs only in the predicted argumentTrying to calculate recall on classes that only occur in the test set is (as the warning say) ill defined anyway.
This would make the behavior consistent with average=weighted , which only averages over true classes.More consideration at the bottom, though it is a bit long winded even by my standards.Possibly with a ""Warning: labels ['c'] occurs in predictions but not in targets. It is not counted. To count it, define labels argument manually. However, this will cause Recall and F-Score to be ill-defined.""Also reasonable would be for it to throw an error,
and require the user to specify the labels one way or the other.
Either as labels=unique_labels(y_true, y_pred) (current behavior),
or as labels=unique_labels(y_true) (what I suggest is the more commonly desired behavior)or as a final option,
not changing the behavior but adding to the current warning message: you may wish to only consider labels from y_true. To do this use ""labels=labels=unique_labels(y_true)There are matching changes that would come to average=None from this change as well.I first encounter this in Scikit-Learn 0.14.1'.
(turned out I was using an old version by mistake),
but it is unchanged when I updated to 0.19.1Now, in normal supervised multi-class ML circumstances you will not have a label that appears in the predicted output but not the target output.There are times when you can, (I am working with one in https://github.com/oxinabox/NovelPerspective)
(though the question then becomes is Macro Averaged P/R/F1 a good metric for these? Maybe not)Anyway, we must assert that we are in such a circumstance, as otherwise, why have we bothered to search both target and predicted output.Consider:
These measures come from information extraction.
They are binary measures.
The precision is ""Of all the times I found LABEL, how many of them were where they should have been "" Prec = tp/(tp+fp)
The Recall is precision is ""Of all times I should have found LABEL, how often did I actually find it?"" Recall = tp/(tp+fn)When define a test set (i.e. the target set), the classes in it must be the ones that matter. It is the ground truth.We wouldn't make a test set that didn't have a class that matters (or would we?)When we same Macro averaging,
we are thinking:
For each class rewrite the outputs so that it is binary as to (Found, Not Found) for the class in question.
And calculate the information extraction precision/recall for each class.
Then average those.In that binary view, you never see the classes from the predicted output.
Just ""Not Found"" for every class in the target output.Because it is the target output classes that you are asking questions about -- that is why they are in the test set.Now consider the math
for some class that is only in the predicted outputlets say it occurs N times.
then tp=0 and fn=0 since it has no true positives or false negatives since it never occurs.and fp=N since any time it occurs is a false positive.Its (binary) precision will be 0. tp/(tp +fp)=0/(0+N)its (binary) recall will be Undefined:
tp/(fn+tp) = 0/(0+0)
its F1 will thus also be undefined.Sklearn gives a warning and sets the binary recall and F1 to zero.This in turn decreases the (overall) macro average since it adds a class that scores 0 for precision and recall.Firstly, I agree with the general underlying principle that we need to be assisting the user towards best practices. (Although some have argued that F1 for classification tasks is never best practice, and I tend to agree, let's leave that alone for now; the issue applies to recall alone, which everyone agrees is a good thing to measure.)I also agree with you that in an ideal world, we would not be getting the set of labels from the prediction data, because then there is an arbitrary large penalty for a system that spuriously proposes a class known in the training but not in the test data. However, ignoring that class from the macro-average will introduce spurious variance into the scores of a cross-validation run. And it's hard to change what we currently do. So I'd rather add a warning than try to change the default behaviour.Basically, for multiclass classification, labels should always be specified. I would certainly considered a PR that improved the warnings for cases where the users had identifiable usage problems.While it is off-topic, I'm pretty interested in this, do you have any links?
The more I was thinking about the problem the more I thought something felt wrong about using this for my problem.It isn't even only for the case of labels from training that are not from test.
There are other cases, for example in my own case, I have an information extraction-ish problem,
that the output labels could be from an infinite set of possibilities.Anyway back on topic:I feel warnings are too easily ignored (though warning is better than nothing)What about in the next release adding a ""Deprecation Warning: Not specifying labels for multiclass classification is deprecated and will not be available in the next release. Update your code to explicitly define what your labels are."".
Then removing it in the next release after that.
(This is the semver based process we follow for all Julia packages, but idk how it works in the Sklearn/Python world)Possibly also adding extra options for label like label='auto_true' (documented as recommend for most usages) for source from the y_true only, and label='auto_both' to source from both.
Though both can still cause problems that being explicit would avoid.
(e.g as you say, unlucky folds in KFolds that misses some labels. For that case in particular I feel like this problem can be pushed up to the cross_validated_scorer which at least protects people who are using that for cross validated evaluations of normal classifiers)This sounds like a retrieval-like IE problem, as opposed to say NER where the number of possible phrases are quadratic in the length of the text, but the number of labels is few. I would not use macro-average for this, unless it's only a macro-average of recall. It gives too much weight to rare classes. Scikit-learn's metrics are also not designed to handle this use-case efficiently.F1 for classification is different from F1 for IR and F1 for IE. It's likely problematic in all of them (or so David Powers argues), because of the inappropriateness of the harmonic mean, and perhaps because Jaccard should be preferred over Dice coefficient (= F1) as a true metric and because it obviates the double penalty issue described by Manning for NER.For classification it's mostly a problem because it lends too much weight to the bias of a system towards the positive class. Basically, precision's denominator messes up a lot of things.I don't think we're yet ready to force the user to specify the set of labels. We've had plans to try push the set of labels from the whole dataset into the evaluation metric when CV is used, but it's proven hard to implement.I don't mind the labels='auto_true' approach and eventually making that default.",None yet
https://github.com/tflearn/tflearn/issues/639,"Anaconda 3.5
TensorFlow 1.0
TFLearn from latest Git pullIn:I am getting the same error. Any solution please...File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1533, in _ensure_xent_args
""named arguments (labels=..., logits=..., ...)"" % name)ValueError: Only call softmax_cross_entropy_with_logits with named arguments (labels=..., logits=..., ...)Please refer to this issue : ibab/tensorflow-wavenet#223Hello every one, I had been using Tensorflow 0.12.0-rc0. But unfortunately it cracked and uninstalled then after I intalled another one which is 1.1.0 ... It is annoying me more. I have the same error on training NN. How to revert to 0.12.0??? Any help, ThanksThe error is here in snapshot.Try this
tf.nn.softmax_cross_entropy_with_logits(logits = yPredbyNN, labels=Y)Thanks dear @370533136 I have managed using that code.Hello, another bug of error has come after tensorflow update.. I have been initializing to save model by saver = tf.train.Saver() but then after new TF version it is giving me error,
ValueError: At least two variables have the same name: W/RMSProp ...
Any idea help me. Thanksworked for me...cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))work for me also ... tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))Thanks guys, this saved me from a long time of frustration! :DThank you for the suggestion tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))This worked for me too!!!Thanks a lot
Worked for me tooPlease help! I already have my code written out as "" tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))""
and I still get this error:
""both labels and logits must be provided""same problem herecost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred,labels= y))try thisMay be you can use tf.nn.softmax_cross_entropy_with_logits_v2(logits = predy, labels= y).as you can see, softmax_cross_entropy_with_logits was deprecated. by the way, just following official recommends, good luck!@370533136 Thank you so much. You just solved a problem that I've been having for a week.from sklearn.utils import shufflerate = 0.001
EPOCHS = 25
BATCH_SIZE = 128
x = tf.placeholder(tf.float32, (None, 32, 32, 1))
y = tf.placeholder(tf.int32, (None))
one_hot_y = tf.one_hot(y, n_classes)
keep_prob = tf.placeholder(tf.float32)logits = ConvNet(x)
#logits = LeNet(x)cross_entropy = tf.nn.softmax_cross_entropy_with_logits(one_hot_y,logits)
loss_operation = tf.reduce_mean(cross_entropy)
optimizer = tf.train.AdamOptimizer(learning_rate = rate)
training_operation = optimizer.minimize(loss_operation)correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))
accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
saver = tf.train.Saver()def evaluate(X_data, y_data):
num_examples = len(X_data)
total_accuracy = 0
sess = tf.get_default_session()
for offset in range(0, num_examples, BATCH_SIZE):
batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]
accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y,keep_prob: 1.0})
total_accuracy += (accuracy * len(batch_x))
return total_accuracy / num_examplesprint('Done')Only call softmax_cross_entropy_with_logits with named arguments (labels=..., logits=..., ...)what should i do on this error?",None yet
https://github.com/scikit-learn/scikit-learn/pull/8350,"#8157Modify the Pipeline API such that steps given by the user at initialization is not modified.I made the changes.@jnothman was proposing a get_steps().Any suggestions @jnothman @ogrisel @raghavrv @GaelVaroquaux @agramfort ?Continue to review full report at Codecov.Other estimated being constructed with existing models are done by modifying coef_ or whatever. But it's not clear to what extent this is recommended. For instance, when replacing an attribute in __dict__ with a property, we've rarely bothered implementing setters. When adding an attribute in version x, we've not checked that the user setting only the attributes available in version x-1 will suffice to not get a NotFittedError. Basically, we've given the user no assurances that this is okay, let alone recommended it.I often find myself wanting to slice up pipelines. It deserves both how-tos and tests. Hacking steps_ is probably enough, but we should write out what that looks like and see if we're happy with it.This either requires clone to be aware of the specific frozen case, or for clone to more generally support polymorphism or singledispatch. I'm still in favour of the latter -- and given this change have an ancient implementation of a wrapper for a fitted estimator -- though I know you feel it opens floodgates to misuse and error.as in copy/wrap the estimators? or mutate them to mark them frozen?I do think that the ability to freeze things is valuable all over the place, notably for semi-supervision (e.g. decomposition transformation trained on lots of unlabelled data incorporated in feature processing pipeline for supervised task).Thinking about this instead of sleeping. Cloning is only part of the problem with freezing. The other part is overwriting fit so it does nothing. This can only be generically achieved with a wrapper.(I've made some edits where I'd failed to fix up a previous version of that comment)Not tonight :pI vomited my panicked thoughts about freezing into issue #8370.@jnothman @GaelVaroquaux Shall I change [WIP] to [MRG] get some reviews.
Any potential merging of this PR should wait for the introduction of the 'freezing business', right?+1I agree that it would greatly help to find the good API to avoid breaking stuff which were useful before. This might be one of the thing that I could work in the SciPy sprint (together with the API for the stacking which I would like to find an agreement).Aside from the problem of needing to support slicing, we've realised cloning Pipelines will break warm_start in the final step.We now have slicing support in master. We can consider this implementation in sklearn.compose for 0.22, but I think we should also consider how to deal with warm_start.I think the cloning/warmstart issue is also something @NicolasHug needs to think about ;)What is the desired behavior of cloning a compose.Pipeline when the final step sets warm_start=True?You don't want to clone the final estimator and use the one passed in estimator.I have a few questions:cc @thomasjpfanThanks @jnothman .We discussed it with @thomasjpfan and we thought about the following solution:Yes, set_paramas would keep both steps and steps_ synchronized.It is still fitting from scratch as long as the items in steps follow our API. The nice thing is that if one of the steps has warm_start=True, this also works. Pipeline is delegating what it means to fit to steps_.Agreed. It can be a @property with a setter, where it is initially set at __init__.I feel like we always want to fit a pipeline from scratch? Not fitting from scratch is an extreme use case that we don't want to support (at least explicitly?). It means that your final estimator has been already fitted on whatever the pre-processors of your pipeline can output... How do you do that without the pipeline in the first place?You mean someone doing e.g. pipe.steps[2] = some_new_est? Shouldn't this be totally forbidden? users should be using set_paramsyeah so we can just clone in set_params tooQuestion: how bad is the problem we are trying to solve here? Do we really care that pipelines don't respect our API conventions? Does it prevent us from doing more cool things?I can't say I have a compelling reason to go through all this, and it seems that the original motivation #8157 was addressed already.But if we do I'd like to move forward and try to implement #8350 (comment).I agree the current behaviour is mostly innocuous and have never thought the change was a priority without checking all the use cases are going to be handled.We have had bugs raised where the fundamental issue was the non cloning in Pipeline.... But I can't find them. And they're more ""surprising"" than ""broken"".The other reason to do this is to set an example of how to build such things in a compliant way!So... should we remove it from the roadmap?actually, this issue is quite weird/annoying and is caused by this: #10063We could also try and fix it by cloning the params in GridSearchCV but that would be somewhat strange as well...removing milestone, I think this needs a slep or a close.",API Needs Decision module:feature_extraction module:pipeline
https://github.com/aqibsaeed/Urban-Sound-Classification/issues/35,"AssertionError Traceback (most recent call last)
in
2 import glob
3 import os
----> 4 import librosa
5 import numpy as np
6 from sklearn.model_selection import KFoldc:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\librosa_init_.py in
10 # And all the librosa sub-modules
11 from ._cache import cache
---> 12 from . import core
13 from . import beat
14 from . import decomposec:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\librosa\core_init_.py in
123 """"""
124
--> 125 from .time_frequency import * # pylint: disable=wildcard-import
126 from .audio import * # pylint: disable=wildcard-import
127 from .spectrum import * # pylint: disable=wildcard-importc:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\librosa\core\time_frequency.py in
10
11 from ..util.exceptions import ParameterError
---> 12 from ..util.deprecation import Deprecated
13
14 all = ['frames_to_samples', 'frames_to_time',c:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\librosa\util_init_.py in
75 """"""
76
---> 77 from .utils import * # pylint: disable=wildcard-import
78 from .files import * # pylint: disable=wildcard-import
79 from .matching import * # pylint: disable=wildcard-importc:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\librosa\util\utils.py in
13 from .._cache import cache
14 from .exceptions import ParameterError
---> 15 from .decorators import deprecated
16
17 # Constrain STFT block sizes to 256 KBc:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\librosa\util\decorators.py in
7 from decorator import decorator
8 import six
----> 9 from numba.decorators import jit as optional_jit
10
11 all = ['moved', 'deprecated', 'optional_jit']c:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\numba\decorators.py in
12 from . import config, sigutils
13 from .errors import DeprecationError, NumbaDeprecationWarning
---> 14 from .targets import registry
15 from .stencil import stencil
16c:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\numba\targets\registry.py in
3 import contextlib
4
----> 5 from . import cpu
6 from .descriptors import TargetDescriptor
7 from .. import dispatcher, utils, typingc:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\numba\targets\cpu.py in
12 from numba import utils, cgutils, types
13 from numba.utils import cached_property
---> 14 from numba.targets import (
15 callconv, codegen, externals, intrinsics, listobj, setobj, dictimpl,
16 )c:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\numba\targets\listobj.py in
1069
1070 @overload_method(types.List, ""sort"")
-> 1071 def ol_list_sort(lst, key=None, reverse=False):
1072
1073 _sort_check_key(key)c:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\numba\core\extending.py in decorate(overload_func)c:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\numba\core\typing\templates.py in make_overload_method_template(typ, attr, overload_func, inline)c:\users\jerkm\appdata\local\programs\python\python38-32\lib\site-packages\numba\core\typing\templates.py in make_overload_attribute_template(typ, attr, overload_func, inline, base)AssertionError:There's some weird bug in the newer versions explained in librosa/librosa#1160 you have to install librosa 0.7.2 along with an older version of numba",None yet
https://github.com/scikit-learn/scikit-learn/issues/6186,"Multiclass prediction using SGD on sparse data is unnecessarily slow. It seems to be copying large arrays on every call to predict, predict_proba etc, which kills its performance.I think the main culprit is safe_sparse_dot, which uses scipy's ""CSR * dense"" routine:Because b is transposed and scipy's multiplication invokes b.ravel(), this is very slow (copies clf.coef_ internally).Keeping clf.coef_.T as a C-contiguous array here improved the prediction performance of SGD classifier 7300x for us (1s vs 137µs per call):The exact speedup numbers will vary, depending on coef_ size (~the number of SGD classes and features).This could be raised as an issue in scipy as well (I see no good reason for such inefficiency -- that ravel() is just too generous), but since the fix seems trivial, maybe it's worth addressing on sklearn side as well?This is using scipy 0.16.1 and sklearn 0.17.A quick grep shows me that safe_sparse_dot is used widely with transposed arguments inside sklearn, so this may be a more pervasive problem (not just SGD).can you give example code reproducing the speedup?Old discussion about it : january-2012
And the solution #545, which has been lost in 55a2f29 after partial_fit implementation.We should probably implement (CSR, Fortran) multiplication in sparsefunc.pyx. See also #516Also related: scipy/scipy#5348.According to your benchmarks, it 1.85ms vs 0.137ms hence a 18.5x speed up, not a 7300x difference.Still we should probably implement (CSR, Fortran) multiplication in sparsefunc.pyx.Actually I am not sure how we could implement (CSR, Fortran) matmul efficiently (without trashing the CPU cache).Indeed !@ogrisel : The exact speedup numbers will vary, depending on coef_ size (~the number of SGD classes and features, as mentioned above) and the sparsity of X.Increase coef_ size and sparsity of X for an arbitrarily large speed up (in our system, it happened to be 7300x).We could probably put some students on this, but to start with, I was looking to see:The problem is that there is one memory layout of coef_ that is optimal for partial_fit and one that is optimal for decision_function and we want to make it possible to interleave calls to partial_fit and predict / decision_function (e.g. to monitor validation error during out of core training).Also decision_function and predict is not expected to mutate the state of an estimator in scikit-learn.So ideally it would be great if we could find a way to always do fast sparse dense product operations without changing the memory layout of the input matrices (sparse and dense). I am not sure this is possible be this needs to be investigated properly.It's not possible to tell without auditing all calls to safe_sparse_dot in the scikit-learn code base.Alternatively, it might be possible to refactor the SGD cython code used by partial_fit to work with the transposed layout for the coefficients. But this need to be done carefully so that we do not introduce performance regression at fit time, both for dense and sparse training data.Sure, thanks.For the option of ""refactor partial_fit and keep only the transposed coef_"", isn't there an additional risk of backward incompatibility? My impression was that coef_ is kinda official and established as a C-order array in sklearn, and people might be relying on that. Wouldn't coef_ becoming a dynamic Fortran-order property break something?I tried the ""CSR * Fortran"" path in a naive way and even that doesn't seem to be that tragic. About 20% slower than ""CSR * C"" on my machine (MBP), which is, in the context of this issue, a rounding error.Here's my test code (Cython, signature ala scipy's csr_matvecs = AXPY for CSR matrix A and dense X, Y):(CSC is completely analogous, and I'm hoping that {CSR, CSC} x {C-order, F-order} already covers most of the common argument combinations to safe_sparse_dot())The memory layout is not part of the public API unless written explicitly in the docstrings. If we do the change, code should not crash but that might trigger additional memory copies for code that expect a C-order layout for the coef_ attribute.Your cython code is promising, would you mind contributing it as a PR to sparsefuncs_fast.pyx and call it from safe_sparse_dot (+ tests)?It would also be worth contributing it directly to scipy. If you do so, we could maintain a temporary backport in sklearn with a comment referencing the scipy version where this fix is available.I think so too. This optimization would be ultimately better served in scipy, so that other parts of the PyData ecosystem can benefit too (see the OP).I had a look at the way scipy handles these multiplications, but got lost. Scipy auto-generates some wrappers and thunkers and I didn't understand it.I can assign the PR to one of our students, but cannot promise any ETA. These simple tests were easy, but whipping things into a proper form (support for different dtypes, checks, tests...) could be a lot of work. Especially if it involves understanding scipy. If someone capable from your team can do it, that would be ideal.Or, perhaps split these routines out into a separate package, now that scipy's sparsetools have been deprecated. I see the name sparsetools is not taken on PyPI yet. Pulling in all of sklearn is an overkill for many apps, even though they might well benefit from safe_sparse_dot / smarter sparse AXPY.I think the right place is part of scipy as this code is useless without a scipy sparse datastructure. But also +1 for getting your two funcs in scikt-learn as a stop gap (which is probably easier to get in that fixing it in scipy).@piskvorky I hope you've had more success with scipy since. Basically all the scipy.sparse basic operations are implemented in scipy/sparse/sparsetools/csr.h et al.@jnothman ah, so it's no longer deprecated? That's great news! @menshikh-iv can you double-check please?Sorry, I missed the comment about deprecation. (Trying to quickly go through issues in anticipation of a release.) But as far as I can tell, sparsetools is only deprecated wrt its use as a public API (i.e. there will be no guarantee that sparsetools will be backward compatible), but it still is where implementation details should be changed.",Bug Moderate Sprint help wanted
https://github.com/scikit-learn/scikit-learn/pull/18109,Fix a matplotlib error in examples/miscellaneous/plot_partial_dependence_visualization_api.pySee DeprecationWarning in dev documentationmatplotlib version:thanks @cmarmoLGTM,None yet
https://github.com/Alohomora-team/AlohomoraAPI/issues/56,"Descrição da funcionalidadeEncontrar uma base de áudios públicos para realizar testes do dtw.Tarefas necessáriasEncontrei uma base de dados que é citada em vários artigos: a RSR2015. A parte ruim aqui é que ela é paga.Nesse texto são citados alguns datasets, mas a maioria deles são inacessíveis, o que me faz questionar se buscar um dataset que se adeque ao nosso contexto vale mais a pena do que criarmos o nosso próprio.A base de dados disponibilizada pela Mozilla (Common Voice) oferece, segundo o próprio texto de apresentação, para cada texto incluso no dataset, um arquivo .mp3 da frase sendo pronunciada. Pro nosso contexto, isso não serve pois, a ideia por detrás dos testes do algoritmo da API é ter um conjunto de pessoas, cada uma dizendo a frase comum a todo o dataset pelo menos três vezes. Dessa maneira conseguiríamos levantar uma métrica de quão bom o algoritmo está em discernir entre duas pessoas falando exatamente a mesma coisa.Acredito que vá ser um desafio encontrar uma base com 3 exemplos para mesma pessoa. Geralmente as bases de vozes seguem esse padrão.A não ser que exista uma para a tarefa de autenticaçãoComo já estamos na reta final, acredito que seja mais fácil e vantajoso criarmos a nossa própria base de dados da forma mais simples possível: cada membro do grupo grava 3 áudios e submete num drive comum. Já possuo dados de alguns familiares, e como o algoritmo de identificação por voz possui um teto de 5~10 pessoas utilizadas na comparação, não é necessário ter um dataset grande ou com minúcias mirabolantes. Dessa forma, acredito que conseguimos atingir o objetivo da issue gastando menos tempo e esforço (tempo e esforço que pode ser direcionado pra outro problema do projeto). O agravante em achar uma base de dados aqui é porque o algoritmo utilizado é Text-Dependent.Encontrei essa base de dados que é bem interessante, mas não se aplica mais ao escopo do nosso projeto, infelizmente.Também existe a ELSDSR mas não é Text-dependent.Aqui e aqui são citadas várias bases de dados super interessantes, mas nenhuma se aplica.Acredito que o mais lucrativo nesse momento é realizarmos o teste com a nossa base customizada mesmoAo escrever os códigos de teste, precisei incluir o pacote librosa, porém o docker não está conseguindo buildar a API quando a librosa tá incluída nos requirements.O Erro:",enhancement
https://github.com/scikit-learn/scikit-learn/pull/4974,"Added support for sample_weight option, refactoring some code along the way. See commit message for more details.I have tested that results do not change when no sample_weight option is specified, and when sample_weight = ones(len(X)). Also, in my weighted real-world use case, results look reasonable and are an improvement over using no weights.DeepakMaybe ""center""? the ""normalizer"" does things per sample.This should fit on one line.Center?Thanks for the contribution. Unfortunately I'm not really familiar enough with PLS to review the changes.
It seems after the refactoring the code got quite a bit longer. Is that mostly due to docs?There are a bunch of test failures because some classes are now missing a predict method.Thanks for the intial review, Andreas.I've fixed the test failures by adding back the predict() method, and adding some comments to the code saying that this ought to be considered for deprecation. I also added a new test, which checks adding identical sample weights does not change results. Lastly, changed comment ""Normalize"" to ""Center and scale"" as you suggested, and also made the one-line docstring change you asked for.As for reviewing the weight functionality itself, it boils down to reviewing the one line, line 86, which adds weights to the SVD, and one other location analagous to that for the NIPALS algorithm alternative, at lines 34-35, which is similar to how you do weighted linear least squares. If you're comfortable with those, I think we are good to go. If not or you don't think so, maybe I can write to the mailing list to ask for volunteers to review?Thanks,
DeepakI'm not a contributor to sklearn, but I've read probably 90% of the papers on PLS. I can take a look at the math within the next 48 hours (if that's useful).@walterreade That would be nice, thanks!So sorry! This dropped off my plate. I'll take a look this weekend. Thanks for the bump.I'm currently working through the code changes. I'll probably have a list of thoughts, but the first question I had was regarding handling pathological weights (e.g., all zeros, negative weights). Is it normal to have these types of checks?(Again, I'll apologize in advance for my any newbie questions.)Thanks Walter. Re: pathological weights, I had a look at other code in sklearn (e.g. logistic.py) and didn't see checks. Happy to add a check_weights() func to utils/validation.py and have this code call that if one of the core contributors suggests that's worthwhile.Separately, thanks for the typo spot. Will fix along with other issues you raise.Hi Deepak,I can't comment on whether or not there should be a weights check (since I don't know the specific guidelines for sklearn contribution.) I was just pointing that out as a potential issue.I've done a number of additional checks (e.g., weighting a single row to zero returns the same result of excluding that row from X and Y, etc.) I didn't not find anything amiss. I've tried to validate against an external algorithm, but there are so many pitfalls with that. So, unfortunately, I do not have anything concrete in that regard.I don't have any other concerns, and personally think it will be a good addition.Lastly, sorry for the delayed responses. It's been a hectic few weeks.Thanks Walter. I've now fixed the typo, and also created a new pull request, PR #5370 with a check_weights() utility function. Thanks again for your time.(Sorry, user error in accidentally closing this PR. Reopened it.)@walterreade if you had some time to check our implementation against a reference, without sample weights or with, that would be really great :)I looked at it again and I think the changes look ok. A little bit more tests would be nice, though.@amueller I will be out of town all next week, Sunday - Saturday, but when I'm back my current hectic schedule will have freed up. Then I will be able to focus on this again.Thank you, and don't worry. This has been suboptimal for a while and a week or two more doesn't really matter. Any help is much appreciated.Thanks @amueller , @walterreade. I have now added the weights sane values validation check (moved my PR #5370 to here), and also added the two tests @amueller suggested (zero weight same as removing sample, double weight same as duplicating sample) to tests/test_pls.py.make fails with some strange module import errors:
ERROR: Failure: ValueError (Attempted relative import beyond toplevel package)
so I am not sure if the two tests I added work fine, though I did try out the two situations (zero weight same as removing sample, double weight same as duplicating sample) with my own code and the results did match.where did you get the error? the tests should always use absolute imports (from sklearn.X import X)Sorry, user error -- my local tree was somehow slightly corrupted. Now test runs OK, including the two new tests I'd added.(And I checked -- test_pls.py does use only absolute imports.)",Waiting for Reviewer module:cross_decomposition module:utils
https://github.com/scikit-learn/scikit-learn/issues/17355,"When using FeatureUnion.set_params(**{}).transform(X), no deprecation warning is raised about an estimator being None rather than ""drop"". If .set_params is not used, then the warning is emitted as expected.In contrast, this does warnA warning, matching the version where the None estimator is passed to FeatureUnion.__init__No warningThis issue was meant to be fixed by #17360, but it hasn't been automatically closed ...
Anyway, I've checked and the warning is thrown in 0.23.2 in both the provided examples: closing.",Bug
https://github.com/scikit-optimize/scikit-optimize/issues/902,"python -c ""import skopt"" fails when scikit-learn==0.23.0 is installed.I reproduced the problem at least in the following python versions
python: 3.6.6, 3.8.2Can confirm that this breaks autogluon<=0.0.8 with import autogluonIssue on scikit-learn: scikit-learn/scikit-learn#17213A temporary workaround for people experiencing this would be to monkeypatch the missing import (assuming numpy >=1.13.3),A proper fix will be included in scikit-learn 0.23.1 (or if scikit-optimize makes a release with a fix from #904 before).I am using skopt 0.7.4 and scikit-learn 0.23.1 and looks like 0.23.1 introduced some new errors.Here's some example code taken straight from the website at https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html#minimal-example:When I run this example, I get the following output:If I downgrade scikit-learn back to 0.23.0 and do the monkey patch as described above, everything works ok.There are also changes in Gaussian processes regression in sklearn 0.23 which needs to be migrated:
scikit-learn/scikit-learn#15782Any updates on this? Can I help in any way?scikit-learn==0.23.1 does solve this for now.
they did properly deprecate the failing option now instead of simply removing:
source: scikit-learn/scikit-learn#17199But the error TypeError: object.__init__() takes exactly one argument (the instance to initialize) Is not solved with that, is it?but that's not what this issue is about originally, that was just attached to this issue.I added the information about TypeError: object.__init__() takes exactly one argument as it is part of scikit-optimize not being compatible with the most recent scikit-learn. Additionally some comments above said that the newest scikit-learn should remove the need for monkey patching. If it is preferred I can open a new issue (if the issue still exists).I think this is actually part of the original issue as it was meant to be a fix, but I think there's an issue with the fix. If you want to look into it, I think we should fix it in sklearn.Instead of
from skopt import BayesSearchCVtry,
from skopt.searchcv import BayesSearchCVthis works well for me.I am using scikit-survival for building CoxPH survival model and BayesSearchCV for tunning, however, I have error TypeError: object.__init__() takes no parameters when I try to run BayesSearchCV(estimator=CoxPHSurvivalAnalysis(), search_spaces=params_coxph, cv=10, n_jobs=-1).fit(X_train, y_train). How to solve this?scikit-learn==0.23.1
scikit-optimize==0.7.4
scikit-survival==0.13.0Version incompatibility has been fixed in #943 and #939 and in release 0.8.1",None yet
https://github.com/fossology/atarashi/pull/68,"@hastagAB , can you please update pyproject.toml and add Nirjas to it. You can remove the pip install -r requirements.txt from .travis.yml@GMishx Thanks, I totally missed pyproject.toml.
I've modified the required changes, please review.Few changes, commented on every line to make sure we don't miss anything.Most of the try except block was for testing only, forgot to remove those from the PR.
Removed a few unwanted try except.
The evaluator takes the output from the CLI itself, so I didn't raised error there on purpose.Changes looks good. Needs testing.Tested, Working fine on my system.
@Kaushl2208 Please Test the changes and merge this PR.@hastagAB Tested. Everything seems to work fine on my system too.Also, I don't have merge access.CC: @GMishx @ag4umsI see a new commit @hastagAB. Is it just a squash commit or something changed?Yes, I squashed the commits so that we can merge.I am getting follow error. Any idea why that is happening for me and not on travis?I'm not sure, I haven't encountered this error till now. Maybe you have an older (python2) version of SciPy.This was issue with my virtual environment. Just discarded my venv and installed again and it worked.Changes looks good.Tested, working as expected.@hastagAB , please squash the commits for PR merge.",ready
https://github.com/udacity/ud120-projects/issues/154,"I'm currently using scikit-learn-0.19, and I'm getting this warning that RandomizedPCA is deprecated:C:\ProgramData\Anaconda2\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.I've tried to use PCA in pca/eigenfaces.py:pca = PCA(n_components=n_components, whiten=True, svd_solver=""randomized"").fit(X_train)I've not found any example that indicates how I'm supposed to use ""transform"" as the warning message indicates.Here are my results using RandomizedPCA:Here are my results trying to use PCA:As you can see, I'm getting different, and in a lot of cases, worse results.What do I need to do to get the same results as RandomizedPCA using PCA?I think the new way you use PCA in pca/eigenfaces.py is the right way. I am not sure what the warning meant by saying ""transform"" here.About the result, I think it depends on the random_state when you split the train set and test set.For some reason, with the same random_state, it does not give the same result on the deprecated version and on the new version. Maybe for some other seed (different seeds for each version), it will yield the same result.This is still an issue in 0.20.2, and it also looks like the PIL library for imaging has been removed. Some googling recommends Pillow, but it just seems like the entire exercise is probably outdated. Hoping someone more skilled than I might have recoded the eignfaces.py code. I'll keep digging.There is an example in the scikit-learn Official website. visit this link for the updated version of the code for pca/eigenfaces.py in python3.",None yet
https://github.com/scikit-learn/scikit-learn/issues/17983,"My OS is windows 10 and using jupyter notebook in my venv. sklearn is installed successfully using the command:
pip install scikit-learnAfter importing the package using the below command:
from sklearn.model_selection import train_test_split
I am getting below error:You just need to pip install scikit-learn in your venv as well.
Hope it helps!As mentioned above, I have already done that.Closed the issue by mistake.
But I have already installed Scikit-learn in my venv. Also I have re-installed it, but not able to fix this issue.This seems like an DDL issue when loading scipy and not an issue specific to sklearn. Does the following work?It would be good to try to reinstall scipy.The above import did not work.
I have tried re-intallation again, but no luck!Another think you can try is to create a new env and use conda to install packages and not pip:It works now, I created a whole new VENV using conda and installed all the required libraries again.
Thanks @thomasjpfan :)",None yet
https://github.com/cvdelannoy/FRETboard/issues/4,"I want to run FRETboard locally but am running into dependency conflicts as well as a failed build for quast.Hi mayeshh!Thanks for your interest in FRETboard. I see the issue; I accidentally gave install instructions for the previous tool I wrote, which was something else entirely. The correct install command should be:Force of habit I guess ^^' sorry for the confusion, the readme is also updated now!Btw, if you're still experiencing issues with dependency conflicts I can recommend using a clean virtualenv or conda environment. Let me know if it works out now!Thanks for your quick response!! I was wondering about the url :)I did clean reinstallation in a python 3.8 env, and there were compatibility issues. It seems that the setup works best with python 3.7. After downgrading my version of python, there is still the following error:Looking through setup.py I installed the correct version:conda install scikit-learn==0.21.2This worked.I will close this issue.Odd, I haven't encountered any issues with python 3.8 or dependency issues with sklearn...thanks for reporting it, I'll dig a bit further some day to see if I can fix it.",None yet
https://github.com/scikit-learn/scikit-learn/pull/17843,"See #17650Instead of calculating the pairwise distances for all pairs of points to obtain nearest-neighbor graphs for estimators like DBSCAN, SubsampledNeighborsTransformer only calculates distances for a fraction s of the pairs (selected uniformly at random). This would make estimators that accept precomputed distance matrices feasible for larger datasets. In a very recent work I did with Google Research [1], we found that you can get over 200x speedup and 250x savings in memory this way without hurting the clustering quality (in some cases s = 0.001 suffices).[1] https://arxiv.org/abs/2006.06743Thank you for the PR @jenniferjang !You can run make flake8-diff locally to find the flake8 errors.Are there references of this approach being used before https://arxiv.org/abs/2006.06743 ?Hi @thomasjpfan, thanks for reviewing this. I initially implemented it as a transformer, but do you think it would work better as a function instead? I can't get the test check_methods_subset_invariance to pass for the transformer because in our case, transform generates the distance matrix for the data, which shouldn't satisfy the subset invariance property.To my knowledge I haven't seen this approach being used before in the literature, at least in the context of DBSCAN.It's not because of the distance matrix that it shouldn't satisfy the subset invariance property, only because of the random nature of the transformation. See other estimators that disable 'check_methods_subset_invariance', such as DummyClassifier and BernoulliRBMHi @jenniferjang , thanks for your work. Please note that some checks are failing because of sklearn.neighbors._base missing the attribute 'UnsupervisedMixin'.... I can't understand why I can't find the class in the github diff, but I'm finding it when checking out your branch... Do you mind checking if something went wrong in the workflow? Thanks!Sorry for the late response! @jnothman, @thomasjpfan, @cmarmo, it appears that the class UnsupervisedMixin, which was one of the parent classes of SubsampledNeighborsTransformer, was removed from scikit-learn, which caused the errors. I've removed dependencies on UnsupervisedMixin, and I believe it builds now.I’ve also added a new example, plot_subsampled_neighbors_transformer_dbscan.py, which plots DBSCAN and subsampled DBSCAN results side by side. However this seem to have issues building, and I'm not sure how to fix it.One thing I noticed is that paired_distances is awfully inefficient: on 30,000 points, paired_distances alone on 10% of edges took more than twice as much time as it took to run the entirety of DBSCAN without sampling. Depending on the size of the dataset, paired_distances takes between 50%-80% of the total runtime of subsampled DBSCAN. I’d like to look into improving paired_distances, either before or after finishing this pull request. Otherwise it would be infeasible to use SubsampledNeighborsTransformer to speed up DBSCAN unless the dataset is extremely large.In order to speed up subsampled DBSCAN, I sort the output of SubsampledNeighborsTransformer’s fit_transform. If you see other ways to make the subsampled_neighbors function faster, please let me know.",module:neighbors
https://github.com/scikit-learn/scikit-learn/issues/4497,"This is an issue that I am opening for discussion.Problem:Sample weights (in various estimators), group labels (for cross-validation objects), group id (in learning to rank) are optional information that need to be passed to estimators and the CV framework, and that need to kept to the proper shape throughout the data processing pipeline.Right now, the code to deal with this is inhomogeneous in the codebase, the APIs are not fully consistent (ie passing sample_weights to objects that do not support them will just crash).This discussion attempt to address the problems above, and open the door to more flexibility to future evolutionCore ideaWe could have an argument that is a dataframe-like object, ie a collection (dictionary) of 1D array-like object. This argument would be sliced and diced by any code that modifies the number of samples (CV objects, train_test_split), and passed along the data.Proposal AAll objects could take as a signature fit(X, y, sample_props=None), with y optional for unsupervised learners.sample_props (name to be debated) would be a dataframe like object (ie either a dict of arrays, or a dataframe). It would have a few predefined fields, such as ""weight"" for sample weight, ""group"" for sample groups used in cross validation. It would open the door to attaching domain-specific information to samples, and thus make scikit-learn easier to adapt to specific applications.Proposal By could be optionally a dataframe-like object, which would have as a compulsory field ""target"", serving the purpose of the current y, and other fields such as ""weight"", ""group""... In which case, arguments ""sample_weights"" and alike would disappear into it.People at the Paris sprint (including me) seem to lean towards proposal A.Implementation aspectsThe different validation tools will have to be adapted to accept this type of argument. We should not depend on pandas. Thus we will accept dict of arrays (and build a helper function to slice them in the sample direction). Also, this helper should probably accept data frame (but given that data frames can be indexed like dictionaries, this will not be a problem.Finally, the CV objects should be adapted to split the corresponding structure. Probably in a follow up to #4294To track the evolution of ideas here previous mentions of related idea:
#2904 (comment)It would fix #2879, and be a clean alternative to #1574 and #3524Sorry to be obtuse, but where does the reticence to depend or better integrate pandas come from? It's hard to find applied examples of sklearn in the community that don't include pandas these days, and the marginal dependencies over numpy and scipy are only dateutil and pytz. It seems as if we'd have to reinvent much of the masking and group-by wheel anyway to support data-dependent CV use cases.Proposal A along with dict of arrays seems like a good solution to me... :)@GaelVaroquaux IIRC, you said you were considering a dataset object for out-of-core learning. If that's indeed the case, this should probably part of our reflexion.+1 for A.I'm still reflecting whether we need to change the API of all estimators, though. I'd like to avoid that, but I'm not sure it is possible.I have nothing better than sample_propsIt means that everybody that uses sample_weight needs to change their code. Which is better than everybody that ever used y needs to change their code (which they'd have to for B).What's the advantage of A over kwargs?can you elaborate?A is a dict of names with array values. These variables could be passed directly as **kwargs, similarly resulting in a dict, without changing the current sample weight handling.So you would add **kwargs to all fit methods and ignore those that are not used?Perhaps not, but I want to know in what ways this is really a worse
solution than sample_props.On 8 April 2015 at 00:59, Andreas Mueller notifications@github.com wrote:Two aspects.I also find that ""**kwargs"" is harder to understand for someone who is
not a Python expert.I think mostly in being a little stricter with the interface. Also, there could be arguments to fit that are not of length n_samples (thought we try to avoid them).@GaelVaroquaux I think the issue you mentioned is caused by upgrading sklearn, not upgrading pandas ;)I just thought it worth raising as devil's advocate, so thanks for the initial responses.Sure, though naming errors are as much a real issue with sample_props. Indeed a confused user may have sample_props={'sample_weight': [...]} or sample_props={'weights': ...} instead of sample_props={'weight': ...}.Another issue in which all proposed solutions fail (but the incumbent approach of ""pass sample_weight explicitly works fine): if an estimator does not have sample_weight support but then it is implemented, its behaviour will change implicitly though the data does not. Is there any way we can avoid this backwards compatibility issue? I don't think the friendly answer is UserWarning(""Weight support was recently added. Your results may differ from before."")Somewhat related question: will transformers also output a modified sample_props? They must, right?Or perhaps we should at least have a way of introspecting which sample
props an estimator or method knows about so that the user can make
assertions in upgrades...? Too frameworkish?On 8 April 2015 at 04:06, Andreas Mueller notifications@github.com wrote:Fixes #2630, also see the wikiTo summarize the current state of the discussion, I think something like this would be a nice solution:Estimator:User:-> ValueError(""Sample properties 'weights' are missing, unknown sample properties 'weight'"")The only thing that is missing is a good way to document the required and optional sample properties of an estimator. I have no idea how we can do this. An advantage of having sample_weights as an explicit argument in fit is that you can directly see that an estimator uses it and it is obvious whether the description in the docstring is missing or not.I think just mentioning it in the fit docstring and / or the estimator docstring should be fine, shouldn't it?I don't think sklearn.seterr(""raise"") is good btw. It should be sklearn.seterr(sample_props=""raise"").
I could see sklearn.seterr(deprecation=""raise"") or sklearn.seterr(type_conversion='ignore') or convergence issues etc.That sounds reasonable. I think this is a more general feature that has an impact on many parts of the library. We should make a separate pull request for it before we deal with the sample properties, shouldn't we? Are there any disadvantages of having such a global state?If we had a ""deep"" approach, where the user needs to explicitly specify the path they want a sample prop to take when passing in the sample prop, would it be acceptable to have non-identifier keyword argument names? For example:Python accepts this. And if we want such a deep (path-based) solution unlike my #9566, we need to express paths with more than just double-underscores which should be reserved for nested estimators. (I note that such deep property mechanism makes modifying sample properties in resamplers as in #3855 problematic: a resampler would need to return props modified with the fully-qualified names.)Any updates on being able to use 'sample_weights' when performing RFECV?No worries, completely understandable. Thanks for all your hard work on Sklearn library. I love how simple and universal this package is.One last question, do you have any suggestions for a quick and dirty way to implement sample weights to RFECV? I just want to mess around with how samples weights change the features I get.Solution here for those interested: https://stackoverflow.com/questions/49581104/sklearn-gridsearchcv-not-using-sample-weight-in-score-functionBasically, you can define your own scorer and do index matching to get the right weights inside that function. GridSearchCV / RandomizedSearch won't do the splitting for you (yet...)I have not read all of the referenced PRs, issues and comments (it's a lot) but I went over this thread briefly.One comment I have: would it be possible to introduce this new parameter while keeping the existing sample_weight, groups, etc. parameters? Then as things get updated to support the new parameter, there can be a check that raises an error or warning if the user specifies both sample_weight as it's own parameter and also sample_weight within sample_properties. Then once sample_properties is fully implemented in the ecosystem support for sample_weight could be dropped, with warning of course.I am also going to briefly detail my use case and results below to support this feature.I am working on classifying activity data (accelerometer). My data looks something like this:In this case, X would be [feature_1, feature_2] and [subject, walk] are the 'sample properties'.
I use subject as the groups in LOGO cross validation because my end goal is to predict for a subject with no existing data.
I need to classify each walk into one of the categories in y_true, but I don't really need to classify each datapoint (in reality, I have thousands of datapoints for each walk). So ideally I would group y_pred by walk and select datapoints for each walk that either have above a certain threshold of prediction probability or pick the top 3 datapoints, etc. The physical reasoning for this is that if the subject did something weird for 1-2 steps I am happy to discard that because I have a lot of other data to work off of.
I was able to achieve this by essentially copy-pasting sklearn.model_selection.__validation.py and hacking it up to passthrough a parameter metadata (a dict) to the scoring function, which I made to have the signature score(estimator, X, y, metadata=None).Doing this increased my cross-validation scores considerably, I guess I had a lot of ""bad"" data points that I am now discarding.I've not yet looked at all of your contribution yet, @adriangb, but you might want to check out the latest proposal at #16079I haven't contributed to scikit-learn @jnothman, but would love to start! Thanks for referring me to the current discussion, I will comment there.Well, I don't expect to be able to do too much, but it cant' hurt to try! I was also interested in working on IterativeImputer (#16638 (comment)). That's probably not any easier.",API Enhancement
https://github.com/scikit-learn/scikit-learn/pull/9173,"Continues PR 8082Changes made:can you fix the flake8 issues?very first round ;)Continue to review full report at Codecov.I think we should also give the user an option to plot everything as a single line graph. This will allow them to get a good plot results in case number of unique parameters are 3 or more but the total number of cases are within say 20.Adding x-axis labels will be a challenge. But we can give each combo an ID and then print a table below showing the value of each parameter for each ID on the plot.Thoughts?Also, could you help me understand the circleCI error (details here):Exception occurred:
File ""/home/ubuntu/miniconda/envs/testenv/lib/python2.7/site-packages/docutils/writers/_html_base.py"", line 671, in depart_document
assert not self.context, 'len(context) = %s' % len(self.context)
AssertionError: len(context) = 1
The full traceback has been saved in /tmp/sphinx-err-RgKdVr.log, if you want to report the issue to the developers.
Please also report this if it was a user error, so that a better error message can be provided next time.
A bug report can be filed in the tracker at https://github.com/sphinx-doc/sphinx/issues. Thanks!
make: *** [html] Error 1./build_tools/circle/build_doc.sh returned exit code 2Action failed: ./build_tools/circle/build_doc.sh@aarshayj can you merge master? This is an issue with old sphinx, I think, which should be fixed in master.Why not (1/(1+exp(-decision))?oh wait this is the old thing",Stalled
https://github.com/mne-tools/mne-python/issues/8171,"plotting of multiple channels does not workget a plotTraceback (most recent call last):
File """", line 8, in
File ""/Users/.../opt/anaconda3/envs/mne/lib/python3.8/site-packages/mne/io/base.py"", line 1372, in plot
return plot_raw(self, events, duration, start, n_channels, bgcolor,
File """", line 21, in plot_raw
File ""/Users/.../opt/anaconda3/envs/mne/lib/python3.8/site-packages/mne/viz/raw.py"", line 414, in plot_raw
_prepare_mne_browse_raw(params, title, bgcolor, color, bad_color, inds,
File ""/Users/.../opt/anaconda3/envs/mne/lib/python3.8/site-packages/mne/viz/raw.py"", line 722, in _prepare_mne_browse_raw
ax.set_yticklabels(['X' * max([len(ch) for ch in info['ch_names']])])
File ""/Users/.../opt/anaconda3/envs/mne/lib/python3.8/site-packages/matplotlib/axes/_base.py"", line 63, in wrapper
return get_method(self)(*args, **kwargs)
File ""/Users/.../opt/anaconda3/envs/mne/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py"", line 451, in wrapper
return func(*args, **kwargs)
File ""/Users/.../opt/anaconda3/envs/mne/lib/python3.8/site-packages/matplotlib/axis.py"", line 1793, in _set_ticklabels
return self.set_ticklabels(labels, minor=minor, **kwargs)
File ""/Users/.../opt/anaconda3/envs/mne/lib/python3.8/site-packages/matplotlib/axis.py"", line 1714, in set_ticklabels
raise ValueError(
ValueError: The number of FixedLocator locations (2), usually from a call to set_ticks, does not match the number of ticklabels (1).Platform: macOS-10.14.6-x86_64-i386-64bit
Python: 3.8.5 (default, Aug 5 2020, 03:39:04) [Clang 10.0.0 ]
Executable: /Users/.../opt/anaconda3/envs/mne/bin/python
CPU: i386: 12 cores
Memory: 16.0 GB
mne: 0.20.7
numpy: 1.19.1 {blas=NO_ATLAS_INFO, lapack=lapack}
scipy: 1.5.1
matplotlib: 3.3.0 {backend=MacOSX}
sklearn: 0.23.1
numba: 0.50.1
nibabel: 3.1.1
cupy: Not found
pandas: 1.0.5
dipy: 1.1.0
mayavi: 4.7.1 {qt_api=pyqt5, PyQt5=5.12.3}
pyvista: 0.25.3
vtk: 8.2.0It looks like this has already been fixed with #8031 - it's already in the latest master and will be part of 0.21.is there a workaround in the meantime? can I downgrade something to make it work?pip install https://github.com/mne-tools/mne-python/archive/master.zip or wait 2 weeks for 0.21 to release then pip install --upgrade mnehttps://pypi.org/project/mne/0.20.8/pip install --upgrade mne should get it installed!",None yet
https://github.com/shankarpandala/lazypredict/pull/169,This PR updates lightgbm from 2.3.1 to 3.0.0.,None yet
https://github.com/tensorflow/tensor2tensor/issues/1456,"'m getting this error when trying to use t2t_trainer with magenta-gpu installed:(magenta) usuario@Strix:~/Escritorio/train$ t2t_trainer --data_dir=""${DATA_DIR}"" --hparams=${HPARAMS} --hparams_set=${HPARAMS_SET} --model=${MODEL} --output_dir=${TRAIN_DIR} --problem=${PROBLEM} --train_steps=1000000/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/linear_model/base.py:35: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ..utils.seq_dataset import ArrayDataset, CSRDataset
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ..utils import arrayfuncs, as_float_array, check_X_y, deprecated
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/utils/random.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ._random import sample_without_replacement
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import cd_fast
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/linear_model/init.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/linear_model/init.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/linear_model/sag.py:12: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from .sag_fast import sag
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import libsvm, liblinear
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import libsvm, liblinear
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/svm/base.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import libsvm_sparse
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/neighbors/init.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from .ball_tree import BallTree
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/neighbors/init.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from .ball_tree import BallTree
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/neighbors/init.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from .ball_tree import BallTree
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/neighbors/init.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from .kd_tree import KDTree
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:28: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ._online_lda import (mean_change, _dirichlet_expectation_1d,
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/utils/graph.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from .graph_shortest_path import graph_shortest_path # noqa
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/isotonic.py:11: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/manifold/t_sne.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import _utils
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/manifold/t_sne.py:27: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import _barnes_hut_tsne
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/manifold/t_sne.py:27: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import _barnes_hut_tsne
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/tree/tree.py:40: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ._criterion import Criterion
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/tree/tree.py:40: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ._criterion import Criterion
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/tree/tree.py:40: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ._criterion import Criterion
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/tree/tree.py:40: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from .criterion import Criterion /home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/cluster/k_means.py:37: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import k_means /home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/cluster/k_means.py:38: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ._k_means_elkan import k_means_elkan
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import _hierarchical
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from . import hierarchical /home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/cluster/dbscan.py:20: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ._dbscan_inner import dbscan_inner
/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:14: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
from ._hashing import transform as _hashing_transform
INFO:tensorflow:Loading hparams from existing json /home/usuario/Escritorio/ds/hparams.json
INFO:tensorflow:Overwrite key label_smoothing: 0.1 -> 0.0
INFO:tensorflow:Overwrite key max_length: 256 -> 0
INFO:tensorflow:Overwrite key max_target_seq_length: 0 -> 2048
INFO:tensorflow:Overriding hparams in transformer_base with label_smoothing=0.0,max_length=0,max_target_seq_length=2048
WARNING:tensorflow:From /home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py:278: init (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.
Instructions for updating:
When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.
INFO:tensorflow:Configuring DataParallelism to replicate the model.
INFO:tensorflow:schedule=continuous_train_and_eval
INFO:tensorflow:worker_gpu=1
INFO:tensorflow:sync=False
WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.
INFO:tensorflow:datashard_devices: ['gpu:0']
INFO:tensorflow:caching_devices: None
INFO:tensorflow:ps_devices: ['gpu:0']
INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 20, '_task_type': None, '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7efc3abc9510>, '_tf_config': gpu_options {
per_process_gpu_memory_fraction: 1.0
}
, '_protocol': None, '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {
per_process_gpu_memory_fraction: 0.95
}
allow_soft_placement: true
graph_options {
optimizer_options {
global_jit_level: OFF
}
}
, '_model_dir': '/home/usuario/Escritorio/ds', 'use_tpu': False, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7efc3b4c3dd0>, '_environment': 'local', '_save_summary_steps': 100, 't2t_device_info': {'num_async_replicas': 1}}
WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7efc3abc31b8>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:ValidationMonitor only works with --schedule=train_and_evaluate
INFO:tensorflow:Not using Distribute Coordinator.
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.
INFO:tensorflow:Reading data files from /home/usuario/Escritorio/ds/score2perf_maestro_language_uncropped_aug-train*
INFO:tensorflow:partition: 0 num_data_files: 1
WARNING:tensorflow:From /home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a tf.sparse.SparseTensor and use tf.sparse.to_dense instead.
WARNING:tensorflow:Shapes are not fully defined. Assuming batch_size means tokens.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Setting T2TModel mode to 'train'
INFO:tensorflow:Using variable initializer: uniform_unit_scaling
INFO:tensorflow:Transforming feature 'targets' with symbol_modality_310_512.targets_bottom
WARNING:tensorflow:From /home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/framework/function.py:987: calling create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.
Instructions for updating:
Shapes are always computed; don't use the compute_shapes as it has no effect.
INFO:tensorflow:Building model body
INFO:tensorflow:Transforming body output with symbol_modality_310_512.top
INFO:tensorflow:Base learning rate: 2.000000
INFO:tensorflow:Trainable Variables Total size: 19061760
INFO:tensorflow:Non-trainable variables Total size: 5
INFO:tensorflow:Using optimizer Adam
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2019-02-16 17:30:27.735273: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-16 17:30:27.821970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-16 17:30:27.822387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.60GiB
2019-02-16 17:30:27.822419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-02-16 17:30:28.036805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-16 17:30:28.036885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0
2019-02-16 17:30:28.036891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N
2019-02-16 17:30:28.037097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7711 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from /home/usuario/Escritorio/ds/model.ckpt-0
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /home/usuario/Escritorio/ds/model.ckpt.
2019-02-16 17:30:37.134709: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.136520: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.138138: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.140584: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.143127: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.145318: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.148035: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.149650: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.151261: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.153675: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.156145: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.158038: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.160499: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.162547: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.164320: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.166616: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.169107: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.170976: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.173398: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.175018: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.176643: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.179173: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.181886: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.183832: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.186242: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.187922: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.189518: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.191852: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.194489: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.196683: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.199091: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.200664: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.202178: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.204392: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.206710: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.208562: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.210872: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.213397: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.214951: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.217271: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.218810: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.220619: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.222160: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.224491: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.226124: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.228021: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.229809: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.231640: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.233268: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.234916: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.236479: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.238621: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.240217: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.241846: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.243441: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.245441: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.247033: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.248677: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.250221: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.251950: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.253640: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.255367: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.256938: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.259079: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.260704: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.262334: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.264171: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.266418: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.268087: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.269736: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.271325: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.273002: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.274691: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.276325: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.277877: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.280079: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.281632: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.283299: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.284877: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.287094: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.288717: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.290681: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.292376: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.294078: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.295679: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.297268: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.298802: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.301206: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.302774: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.304487: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.306026: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.308171: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.309720: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.311383: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.312921: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.314617: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.316230: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.317902: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.319492: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.321615: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.323160: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.324815: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.326351: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.328421: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.329969: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.331948: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.333532: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.335254: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.336814: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.338401: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:37.339992: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:40.245546: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-02-16 17:30:40.245579: W tensorflow/stream_executor/stream.cc:2127] attempting to perform BLAS operation using StreamExecutor without BLAS support
Traceback (most recent call last):
File ""/home/usuario/.conda/envs/magenta/bin/t2t_trainer"", line 10, in
sys.exit(console_entry_point())
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/magenta/tensor2tensor/t2t_trainer.py"", line 34, in console_entry_point
tf.app.run(main)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
_sys.exit(main(argv))
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/magenta/tensor2tensor/t2t_trainer.py"", line 29, in main
t2t_trainer.main(argv)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 393, in main
execute_schedule(exp)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 349, in execute_schedule
getattr(exp, FLAGS.schedule)()
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py"", line 438, in continuous_train_and_eval
self._eval_spec)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
return executor.run()
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
return self.run_local()
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
saving_listeners=saving_listeners)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
loss = self._train_model(input_fn, hooks, saving_listeners)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
return self._train_model_default(input_fn, hooks, saving_listeners)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
saving_listeners)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
_, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
run_metadata=run_metadata)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
run_metadata=run_metadata)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
raise six.reraise(*original_exc_info)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
return self._sess.run(*args, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1312, in run
run_metadata=run_metadata)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
return self._sess.run(*args, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 929, in run
run_metadata_ptr)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
feed_dict_tensor, options, run_metadata)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
run_metadata)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(3825, 512), b.shape=(512, 512), m=3825, n=512, k=512
[[node transformer/parallel_0_4/transformer/transformer/body/decoder/layer_0/self_attention/multihead_attention/q/Tensordot/MatMul (defined at /home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/layers/common_layers.py:2935) = MatMul[T=DT_FLOAT, _class=[""loc:@train...d/MatMul_1""], transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](transformer/parallel_0_4/transformer/transformer/body/decoder/layer_0/self_attention/multihead_attention/v/Tensordot/Reshape, Read_18/ReadVariableOp)]]
[[{{node transformer/parallel_0_4/transformer/transformer/body/decoder/layer_2/self_attention/multihead_attention/v/Tensordot/Shape/_2433}} = Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1404...rdot/Shape"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]Caused by op u'transformer/parallel_0_4/transformer/transformer/body/decoder/layer_0/self_attention/multihead_attention/q/Tensordot/MatMul', defined at:
File ""/home/usuario/.conda/envs/magenta/bin/t2t_trainer"", line 10, in
sys.exit(console_entry_point())
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/magenta/tensor2tensor/t2t_trainer.py"", line 34, in console_entry_point
tf.app.run(main)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
_sys.exit(main(argv))
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/magenta/tensor2tensor/t2t_trainer.py"", line 29, in main
t2t_trainer.main(argv)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 393, in main
execute_schedule(exp)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 349, in execute_schedule
getattr(exp, FLAGS.schedule)()
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py"", line 438, in continuous_train_and_eval
self._eval_spec)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
return executor.run()
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
return self.run_local()
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
saving_listeners=saving_listeners)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
loss = self._train_model(input_fn, hooks, saving_listeners)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
return self._train_model_default(input_fn, hooks, saving_listeners)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1237, in _train_model_default
features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
model_fn_results = self._model_fn(features=features, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 1368, in wrapping_model_fn
use_tpu=use_tpu)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 1429, in estimator_model_fn
logits, losses_dict = model(features) # pylint: disable=not-callable
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 374, in call
outputs = super(Layer, self).call(inputs, *args, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 757, in call
outputs = self.call(inputs, *args, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 295, in call
sharded_logits, losses = self.model_fn_sharded(sharded_features)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 356, in model_fn_sharded
sharded_logits, sharded_losses = dp(self.model_fn, datashard_to_features)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/utils/expert_utils.py"", line 231, in call
outputs.append(fns[i](*my_args[i], **my_kwargs[i]))
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 392, in model_fn
body_out = self.body(transformed_features)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/models/transformer.py"", line 210, in body
losses=losses)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/models/transformer.py"", line 161, in decode
losses=losses)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/models/transformer.py"", line 1316, in transformer_decoder
vars_3d=hparams.get(""attention_variables_3d""))
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/layers/common_attention.py"", line 3408, in multihead_attention
vars_3d_num_heads=vars_3d_num_heads)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/layers/common_attention.py"", line 3275, in compute_qkv
vars_3d_num_heads=vars_3d_num_heads)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/layers/common_attention.py"", line 3235, in compute_attention_component
antecedent, total_depth, use_bias=False, name=name)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/layers/common_layers.py"", line 2935, in dense
return tf.layers.dense(x, units, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/layers/core.py"", line 184, in dense
return layer.apply(inputs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 817, in apply
return self.call(inputs, *args, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 374, in call
outputs = super(Layer, self).call(inputs, *args, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 757, in call
outputs = self.call(inputs, *args, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/keras/layers/core.py"", line 963, in call
outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]])
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 2985, in tensordot
ab_matmul = matmul(a_reshape, b_reshape)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 2057, in matmul
a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4560, in mat_mul
name=name)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
op_def=op_def)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
return func(*args, **kwargs)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
op_def=op_def)
File ""/home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1770, in init
self._traceback = tf_stack.extract_stack()InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(3825, 512), b.shape=(512, 512), m=3825, n=512, k=512
[[node transformer/parallel_0_4/transformer/transformer/body/decoder/layer_0/self_attention/multihead_attention/q/Tensordot/MatMul (defined at /home/usuario/.conda/envs/magenta/lib/python2.7/site-packages/tensor2tensor/layers/common_layers.py:2935) = MatMul[T=DT_FLOAT, _class=[""loc:@train...d/MatMul_1""], transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](transformer/parallel_0_4/transformer/transformer/body/decoder/layer_0/self_attention/multihead_attention/v/Tensordot/Reshape, Read_18/ReadVariableOp)]]
[[{{node transformer/parallel_0_4/transformer/transformer/body/decoder/layer_2/self_attention/multihead_attention/v/Tensordot/Shape/_2433}} = Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1404...rdot/Shape"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]Facing the same issue. Did you get a work-around for the same?",None yet
https://github.com/scikit-learn/scikit-learn/pull/17622,"The initial goal of this PR was to improve scalability of MiniBatchKMeans on multi-core machines. Working on that I noticed several inconsistencies between kmeans and minibatch kmeans, dense and sparse, fit and partial_fit, ... so this PR also refactors some parts for more consistency and better exploit of the inheritance of minibatchkmeans from kmeans. I also reworked a lot of tests (parametrize as much as possible and fix some broken tests).Here's a list of what this PR actually changes:First batch of review comments (mostly about variable names to ease readability):@jeremiedbb there is a conflict. I am not how you want to resolve it (about the verbosity tests).I changed the reassignment heuristic because it used to break the equivalence between sample weights and scaled sample weights (the existing test used to pass because it did not trigger reassignment).I chose to reassign each time 10 * n_clusters samples have been processed. 10 is arbitrary but we want to do it often enough but not too often (for efficiency).Here's a small experiment replacing 10 by values from 1 to 1000 (vertical axis, top to bottom). On horizontal axis are different values for the reassignment ratio (from 0.01 to 0.3). Darker means smaller inertia.It seems that the more reassignments we do the better. 10 is 4th row.Interesting but I am not sure how to interpret the vertical axis. Can you please add axis labels :) ? I assume it's a log scale?I know, I'm sorry about that :/ I wanted to but I restarted the kernel before doing it and the run was very long so I did not have the courage to run it again.For top to bottom, values are [1, 2, 5, 10, 25, 100, 250, 1000]",module:cluster
https://github.com/scikit-learn-contrib/polylearn/issues/10,"First, thanks for the hard work on this package. It looks like a great way to get higher-order interactions to potentially improve on the standard FM models/packages.It looks like the constant offsets/intercepts are not learned. Is this a to-do item, or is it something that's easy to fix by, for example, doing a global demean of the training outputs y_train in the case of regression? What about classification? Does it matter at all in that case?Hi, thanks a lot!First of all, I agree this is a feature that should be implemented, and it should not be too difficult. Would you be interested in contributing it? I am a bit caught up the following month, but I can look into it afterwards.Regarding workarounds:I think in the case of regression it's simply a case of subtracting the mean of y_train, and adding it back at the end, as you say. For classification this is not the case, but using sample weights can deal with imbalanced classes quite well.A simple way around this is to add a dummy column, as performed by the fit_lower='augment' option.If you're training a third order FM, as long as you use fit_lower='augment', fit_linear=True a dummy column is added so basically an intercept is learned.Otherwise, you can do this in user code easily by using add_dummy_feature.Of course, this workaround leads to a regularized intercept which might not be ideal.HTH!That is very helpful and answers my question, thanks.I might have time to contribute this feature, depending on the complexity. What would be involved?The first step should be figuring out what objective function we want, so we can work out the intercept upgrades. Then, writing some failing unit tests.Sure, sounds fun. I imagine we can just take the current objective functions and stick a + b into them?If I'm training a second order FM, how can I fit the intercept? I see ""use fit_lower='augment', fit_linear=True"" can not give mi an intercept. Thank you!If i'm not mistaken, if you use fit_lower='augment', fit_linear=True, you will (indirectly) be learning an intercept; check the dimensionality of the learned weight vectors and matrices: they will be greater by 1 than the input features. The first entry should correspond to the intercept.I set parameters as follows:
loss = 'logistic', fit_lower='augment', fit_linear=1, degree=2, n_components=2
My feature number is 29 and len(fm.P_) == 29 and fm.P_.shape == (1, 2, 29).
Is there anything wrong I've done?Thanks for pointing that out, you are not doing anything wrong. Indeed, fit_lower='augment' was designed with lower degrees in mind, not with linear terms in mind. If you set fit_linear=False, fit_lower='augment' you will indeed get fm.P_ to be of width 30, but there will be no linear term fm.w_.This is kind of by design of the API, and I realize it is not ideal. We could change the api with a deprecation cycle, but I would prefer a pr to actually learn the bias by coordinate descent explicitly.For your use case, I recommend that you just add the dummy feature (a column of all ones) explicitly:(I had some typos in the comment above. If viewing this by e-mail, please visit the updated comment on github)Thank you for replying!
I modified _cd_direct_ho routine. When calling _cd_linear_epoch, it modifies X adding a dummy feature to fit the intercept (_w[0]).",enhancement
https://github.com/BrikerMan/Kashgari/issues/175,"The output is:WARNING: Logging before flag parsing goes to stderr.
W0724 02:31:49.620104 139957599934336 lazy_loader.py:50]
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:18 frames
FailedPreconditionError: From /job:worker/replica:0/task:0:
Error while reading resource variable layer_crf/transitions from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/layer_crf/transitions/N10tensorflow3VarE does not exist.
[[{{node layer_crf/transitions/Read/ReadVariableOp}}]]During handling of the above exception, another exception occurred:FailedPreconditionError Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
1368 pass
1369 message = error_interpolation.interpolate(message, self._graph)
-> 1370 raise type(e)(node_def, op, message)
1371
1372 def _extend_graph(self):FailedPreconditionError: From /job:worker/replica:0/task:0:
Error while reading resource variable layer_crf/transitions from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/layer_crf/transitions/N10tensorflow3VarE does not exist.
[[node layer_crf/transitions/Read/ReadVariableOp (defined at /usr/local/lib/python3.6/dist-packages/kashgari/layers/crf.py:80) ]]Original stack trace for 'layer_crf/transitions/Read/ReadVariableOp':
File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
""main"", mod_spec)
File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
exec(code, run_globals)
File ""/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py"", line 16, in
app.launch_new_instance()
File ""/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
app.start()
File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py"", line 477, in start
ioloop.IOLoop.instance().start()
File ""/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py"", line 888, in start
handler_func(fd_obj, events)
File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
return fn(*args, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
self._handle_recv()
File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
self._run_callback(callback, msg)
File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
callback(*args, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
return fn(*args, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
return self.dispatch_shell(stream, msg)
File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
handler(stream, idents, msg)
File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
user_expressions, allow_stdin)
File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 196, in do_execute
res = shell.run_cell(code, store_history=store_history, silent=silent)
File ""/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py"", line 533, in run_cell
return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
interactivity=interactivity, compiler=compiler, result=result)
File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
if self.run_code(code, result):
File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
exec(code_obj, self.user_global_ns, self.user_ns)
File """", line 2, in
model.build_tpu_model(strategy, train_x, train_y, test_x, test_y)
File ""/usr/local/lib/python3.6/dist-packages/kashgari/tasks/base_model.py"", line 195, in build_tpu_model
self.build_model_arc()
File ""/usr/local/lib/python3.6/dist-packages/kashgari/tasks/labeling/models.py"", line 114, in build_model_arc
output_tensor = layer_crf(tensor)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 591, in call
self._maybe_build(inputs)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 1881, in _maybe_build
self.build(input_shapes)
File ""/usr/local/lib/python3.6/dist-packages/kashgari/layers/crf.py"", line 80, in build
trainable=True)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 384, in add_weight
aggregation=aggregation)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 663, in _add_variable_with_custom_getter
**kwargs_for_getter)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 155, in make_variable
shape=variable_shape if variable_shape.rank else None)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 259, in call
return cls._variable_v1_call(*args, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 220, in _variable_v1_call
shape=shape)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 198, in
previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2495, in default_variable_creator
shape=shape)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 263, in call
return super(VariableMetaclass, cls).call(*args, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 460, in init
shape=shape)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 649, in _init_from_args
value = self._read_variable_op()
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 935, in _read_variable_op
self._dtype)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 587, in read_variable_op
""ReadVariableOp"", resource=resource, dtype=dtype, name=name)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
op_def=op_def)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
return func(*args, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
op_def=op_def)
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2005, in init
self._traceback = tf_stack.extract_stack()CRF layer does not support multi-GPU (#174) and TPU, you may use bi-lstm for training on TPU. We will try to make CRF layer compatible with multi-GPU and TPU.",bug pinned
https://github.com/tensorflow/tensorflow/issues/42106,"My first time posting a GitHub issue, so excuse me if I leave something out.System informationDescribe the current behavior
Every time I call tf.keras.models.load_model on my SavedModel in version 2.3.0, the model fails to load. I had saved the model with tensorflow 2.0.0 installed, and I could load it with 2.0.0, but when I upgraded to 2.3.0, I got the following error:Describe the expected behavior
Ideally, I should be able to load up my model and just use it from there.Standalone code to reproduce the issue
I could reproduce this with the model in model.zip and with the following code:I have not analyzed your model but can you verify if it is compliant with saved_model documented compatibility constrains?@bhack how would we go about finding the GraphDef version? I didn't use any deprecated APIs, so I'd assume that that's the problem, if it is indeed incompatiblity.@adityapaul,
I was able to reproduce the issue. Facing an error while loading the model with TF v2.3, whereas the code runs fine on TF v2.0. Please find the attached gist.In order to expedite the trouble-shooting process, could you please provide the code you had to used to build the model. Thanks!Yes It could be useful to double check ""non-deprecated, non-experimental, non-compatibility APIs""@bhack Thanks for that! I'll take a look and see what exactly I was doing and make sure I wasn't using anything non-compatible. I was able to load the model in TF 2.1.0, but my code still didn't entirely work. However, I imagine that it's probably something wrong with my code then rather than an underlying TensorFlow issue.@amahendrakar Here's the code that I used to train the model. Let me know if you need anything else!@adityapaul In the behaviour statement you have written ""elu"" instead of ""relu"" in the activation section !
This might help!@sarthak3248 Hmmm, I had used ELU activations in the two models that were portable to TF 2.3, so I'm not too sure if that's the problem I'm facing. Thank you though!@sarthak3248,
On running the code, I am facing an error stating NameError: name 'max_len' is not defined. Please find the gist of it here.Could you please provide the complete code and the dataset you are using to save the model? Thanks!Sure. The full code is here:The SICK dataset is attached here. The 50-dimensional GloVe vectors are too large to upload here, but can be downloaded with@adityapaul,
Thank you for the update.@jvishnuvardhan,
I was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it here. Thanks!@adityapaul I see a deprecation warning on your model save:",TF 2.3 comp:keras regression issue stat:awaiting tensorflower type:bug
https://github.com/scikit-learn/scikit-learn/issues/16155,"While using the boston_housing data set, a data set hosted by the Scikit-learn package and used to demo models on house price prediction, I came across a feature titled 'B'. This struck me as odd because all other features had been given descriptive names such as 'AGE' or 'TAX'. It turns out that B = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town. I naively assumed, as this data was being hosted by a prestigious package, that these data were in the data set because they offer significant explanatory value, which would point to a strongly pervasive racist mentality in the population at the time. However, after reading the blog post attached below, it appears as though the data in the B feature of the Boston housing data set were manufactured in an attempt to encourage segregation of the races. If true, this would be strong evidence of systemic institutional racism and by continuing to use this fraudulent data we would be perpetuating the effect desired by the author. I hope you will agree that we would be doing the scientific literature a service by investigating this issue further and ultimately consigning this data to historic reference archives and not encouraging its use in modern research by hosting it.I look forward to your response,Jamie R. Sykeshttps://medium.com/@docintangible/racist-data-destruction-113e3eff54a8Thanks for raising this issue and for providing the reference with additional details.+1 to find a different housing dataset that wouldn't contain such variables or assumptions and substitute it in examples. Haven't looked for datasets we could use as a replacement yet.Hi, I wrote an article on this dataset and found the same problem with the language. I didn't address the assumptions behind the existance of the column because I'm not as intimate with the dinamics of race and gentrification in the US, but I can see how it can be problematic as well. Can we at least change ""blacks"" for ""black people""?One option is removing it. Another one, proposed by the article, is going back to the original census and reconstruct the data. Following the article's logic, reconstructing 36 datapoints should suffice, and the author found 20. I don't know how feasible it is to find the other 16.Hi, I'm the author of the post @martinacantaro and @jrsykes are referring to above. While I second @martinacantaro on removing the column, I also suggest reconstructing the original tract data, and that this process should be documented and submitted along with the newly-constructed dataset, along with the original paper.The fact that we distribute this dataset as such is indeed a problem because one could assume that we think that casually making such assumptions on the segregationist propensity of house buyers is fine.Here are some possible ways to deal with this:a- keep the data as such but add a warning in the documentation to state that this variable casually makes problematic assumptions and that using this dataset without questioning those assumptions will likely be considered as some kind of implicit endorsement of a racist worldview,
b- try to reconstruct the original Bk variable from the Census data but apparently this is not easy (or even not possible if the Boston dataset authors made errors when building the dataset as @mcarlisle suggests in the blog post).
c- drop the B variable,
d- drop the full dataset and point our users to alternatives such as the California Housing dataset.
e- add an option to load_boston such that, by default the B variable is not included but can be added back if the users really want to use it.The problem with c and d is that we will break tutorials and educational resources written by others, including tutorials that aim at educating machine learning practitioners on fairness related issues. For instance: https://scikit-lego.readthedocs.io/en/latest/fairness.html uses scikit-learn's load_boston loader to illustrate to impact of the B variable on a ""fairness proxy"". I am not familiar enough with the literature to say whether or not the analysis and method proposed in this particular tutorial are valid but we should probably not prevent others to study those issues.I think I would be in favor of a mix of proposals e and a, along with a FutureWarning deprecation cycle.Edit: s/not dropped/not included/Ok but I would prefer not to completely remove the variable but instead not load it by default unless the user really needs it in which case they can pass a specific param to load_boston (e.g. to study the ethical implications of this variable for instance). E.g. load_boston(include_racial_segregation_variable=True).Those naming suggestions feel weird to me.+0 for EthicalWarning or even EthicalFutureWarning that would subclass both a new EthicalWarning class and a FutureWarning class. This warning (EthicalFutureWarning ) would stop being raised when calling sklearn.datasets.load_boston in scikit-learn version 0.(x + 2).WDYT?Personally and professionally, I'm enamored with the notion of introducing something like EthicalWarning, although I understand that this would cause many in tech that labor under the fallacy of ""technology and mathematics are ethics-neutral"" to get up in arms (present company not considered in this statement, just a general observation).It is clear (to me, at least) that, at this point in technological history, with the ubiquity of large common-use datasets collected under statistically under-controlled, possibly politically charged, and certainly financially motivated, methodologies, that those using said datasets in the present and future are aware of such datasets' origins and potential concerns before using them. The forever questions are,Does scikit-learn, as an enterprise, feel the need to dip their toes into these data ethics questions, and if so, to what extent? Or, does the project wish to remain under the impression that the tools they provide and maintain have status EthicsNeutral == True? I have a hunch (not at the moment backed up with evidence) this is not the only questionable dataset in the collection.I think people have generally used the Ames housing data instead.I would favor removing the dataset, potentially with a longer deprecation cycle and replacing it entirely. I don't think we need to introduce a new warning type but we can be explicit about the cause of the removal.
Edit: I'm fine with adding to the warning a way to load from OpenML as long as we raise the issue in the warning as well.+1One note: it's not really a good 1:1 replacement as it includes missing values and lots of categorical features. It's a good replacement in terms of semantics and an interesting dataset to play with, but it depends a bit what we want from the dataset.I would like to work on that issueSo it seems that we will go for removal. So we can start deprecating the function. We can as well mention fetch_openml. I am not sure yet what would be the alternative thought.As far as I understand there was a decision for removal during the last dev meeting.We need to decide which datasets we use for replacement in,For information, the list of examples that need updating:An alternative embedded dataset for non-synthetic toy regression in scikit-learn are the Linnerud datasets from load_linnerud (20 samples, 3 features).This dataset is probably more than enough for illustration purpose in the docstrings of regression estimators.A slightly bigger dataset for regression, load_diabetes (442 samples, 10 features) can be used as well.@lucyleeow looks like you already did a lot of work here. I'd be happy to help, anything I can get on started or take over? If not I'll look for another issue..Hi @noatamir, there are plenty of tests that still use Boston (sorry I didn't keep track of the ones left as it was just me working on it). Feel free to tackle them! Maybe comment here or open a PR before you starting working on something so we don't end up working on the same thing.Of note, the diabetes dataset (the other regression dataset) is quite noisy and seems to overfit with default parameters for some estimators. Depending on the test you may need to change the default parameters.The 'last' test with Boston is sklearn/neural_network/tests/test_mlp.py. I am a bit stuck with this because in the test the first 200 samples of Boston are scaled:The diabetes dataset in sklearn has already be scaled:I'm also not sure of the purpose of the scaling. The boston dataset is used in the following tests:Suggestions?Neural networks convergences faster when the input are scaled (MinMaxScaler or StandardScaler.For these tests, we would not need to scale the dataset, because the dataset is already scaled.Thanks @thomasjpfan. I guess my question was more why scale only the first 200 samples? Is it to help convergence but just at the start...?Regardless, if the tests do not require only the first 200 samples to be scaled, then it would be safe to replace boston with diabetes.Nevermind I misread the code again. i see it is only using the first 200 samples, sorry for noise.",module:datasets
https://github.com/scikit-learn/scikit-learn/pull/3681,"Fix for #3453
Ping @arjoly . Added support for zero_one_loss and accuracy_score@MechCoder could you please help in figuring out the test failure?The errors look like somehow you're transforming metric outputs into integers...Your current code in _check_targets is reporting the type as multilabel-indicator when the input is multiclass-multioutput. I don't see why that's creating the current barrage of errors, but it can't possibly be correct behaviour.Coverage decreased (-0.04%) when pulling 392f18a on akshayah3:metrics into 9580431 on scikit-learn:master.@jnothman The issue was with the _check_targets method. I fixed it, could you review the code please?Thanks for tackling this issue!Can you also update the documentation and narrative documentation?@arjoly I have made some changes you suggested!Coverage increased (+0.01%) when pulling 04c02b7 on akshayah3:metrics into 9580431 on scikit-learn:master.Coverage increased (+0.01%) when pulling 43861b6 on akshayah3:metrics into 9580431 on scikit-learn:master.Coverage increased (+0.01%) when pulling edfe486 on akshayah3:metrics into 9580431 on scikit-learn:master.@arjoly Does this look good now?For the narrative doc, I was thinking in updating this page / file@jnothman @arjoly Any changes to be done?@arjoly I have adressed the comments, Does this look good?@arjoly Sorry for the late reply. I was busy with my university exams.
Could you review the latest commit?Can you ensure that we still get meaninfull error message?Now, we haveWhile previously it was returning?@arjoly Any more changes to be made?@Akshay0724, there was a request at #3453 that this be finished up. Do you intend to complete it, or should we find another contributor?Don't any developer wants to resolve conflicts within this?
I need a multiclass-multioutput metric for grid search in my task and I see no support of such metric in sk-learn unfortunately.@jnothman I'll be happy contribute to this one as I need it personally.",Waiting for Reviewer module:metrics module:utils
https://github.com/scikit-learn/scikit-learn/issues/4429,"I dig into #3945 and #2640. PR #3945 claimsActually, the reason why the sample code given by @AlexisMignon in #2640 raises an exception is that the simplified formula C = (\sum_i w_i x_i x_i') - \mu \mu' does not apply when \mu is not the mean of X.The gmm created in this linedoes not estimate mean in M-step. The gmm.means_ is initialized by k-means, and does not change in EM. When the code in m_step before #3945 merged is executed, in one of component the mean is initialized to [[ 3.42118979 3.53824541]] by kmeans. However, the real mean is [0.02670257 0.02945094] before the exception is raised. Using C = (\sum_i w_i x_i x_i') - \mu \mu', of course, the estimated covariance is not positive definite. IfWe don't have the exception at all.In #2640, without the snippet resulting the error, I also cannot figure out why the covariance is not positive definite. Anyway, I think the method log_multivariate_normal_density in gmm.py could be called by methods in other modules, so it is better to check the covariance first before Cholesky decomposition.Besides, when using parameter params='wc' to initialize a gmm, we always have non-positive-definite covariance problems, with full and tied type covariance. You could try it with any random generated data. I think it is strange to build GMM without estimating means. It would be better to deprecate params initialization other than wmc.Finally, the original code using equation C = (\sum_i w_i x_i x_i') - \mu \mu' before #3945 merged, I think, it is OK as long as we could guarantee \mu is the mean of X. But I found the code in Matlab use the way as #3945 does. I cannot see other reasons to use that way except preventing round-off errors, which I really doubt.Ping @amuellerThanks for digging into this. You are right, not estimating the means is a problem.
I also think it is strange, and we should deprecate the usage.So you say matlab uses the same way we currently do it (after the PR)? I looked at vlfeat and they also do the same. I guess it is for numerical reasons. I don't see a strong reason to change it back. Do you?I don't understand your statement ""In #2640, without the snippet resulting the error, I also cannot figure out why the covariance is not positive definite. ""
Why would you check the covariance before the cholesky? You could also only raise an error, right?Besides the deprecation, I don't know why there is init_params and params, which both accept 'wmc' initialization format. It seems redundant.Yes, I agree with you. I don't think we need to change it back in _covar_mstep_full, but do you think we need to change _covar_mstep_tied? Matlab does as we do now after the PR for full covariance type.Sorry I don't make that clean. I just don't know why @arendu has non positive definite covariance error in #2640 . I think it would be better to catch numpy.linalg.linalg.LinAlgError: in _log_multivariate_normal_density_full.I see. I agree it would be nice to catch the error. I thought it was more informative.
I think the intention behind the ""duplication"" between init_params and params is that you might want to a) give a custom initialization, but want to keep on training (init_params allows that), or want to set some fixed parameters and not train them at all (params allows that).
I think we want to deprecate params because estimating some parameters and not others can lead to trouble.For init_params it is not something we usually support in scikit-learn, but I can see how it could be useful.
A different way to do this would be to actually pass the initialization as parameters, which is what we do in different places, like means_init=some_array, covars_init=some_array etc.Yeah. We should provide an explicit way to let users initialize the means, covers, weights with their own initialization. And do you think we need to check users initialize them properly? Such as positive definite covariance, the sum of component weights should be 1.I will work on the params deprecation and catch numpy.linalg.linalg.LinAlgError:.Yes, we should check that. I'm +0.5 on changing the way users can initialize. The current way is different from the standard scikit-learn way, but deprecating things is always painful for users.Thanks for working on this :)Hello,Thanks @xuewei4d for the correct explanation of the reason why the covariance matrix was not positive definite.However, I ran into the problem because I was in a situation where I did want to keep the mean unchanged after initilialization. Basically the idea was to keep the kmeans initialization because it tiles the data cloud while the plain GMM solutions was tending to have confused centers and different covariances to catch non Gaussian tails. I do not claim it is a recommanded way to use GMMs, however, it did the job pretty well.I think it is convenient to keep 'init_params' and 'params' separate and assume that users know what they do when changing those values. At most, I would add a note in the class documentation to warn that it is an unrecommanded trick.@alexis-mignon I understand, so let users to worry about how to use init_params and params correctly.@amueller I will add to my GSoC proposal how to design the interface for passing initialization of mean, covariance, weights in GMM.thanks :)@alexis-mignon I think it is not a good idea to keep params, because it is a very special use-case, and we need to make sure that all seven possible combinations work correctly. They did (do?) not, and it looks to me like this is quite a hassle for a very specific use-case.
Also, I'm not even sure what the settings mean. What is the mean without adjusting the prior on the mixture components? Do you just compute the responsibilities and use those to update the mean, but then discard the responsibilities and not update the prior?I see some interest in being able to keep some parameter fixed:However, I am not sure the corresponding estimators in those settings are exactly the same as the one used in the present EM algorithm. The existence of this paper, , suggests that there might be some specific treatments when some parameters are known. However, I don't have access to science direct and could not read it.Hurray for closed science!
While I agree that there is an application for that, I would claim it is outside the normal sklearn scope. Sklearn is pretty black-boxy and not really good for plugging in known quantities into models.
I think you'd be better served with a probabilistic modelling framework for that.What framework would you suggest ?I'm not sure what the best in Python would be. I'd think of something like factory. I think there is some variational inference machines in python, too. Maybe https://github.com/bayespy/bayespy which also has a list of related packages?@amueller Besides param problem, I am curious about four covariance type spherical, tied,full,diag, since I never saw that before, at most the case where the covariance is fully estimated, but shared by each component. I found a comment in the gmm code points to a Murphy's 1998 paper, Fitting a Conditional Linear Gaussian Distribution (8 citations). It elaborates all the updating functions for four covariance type. However I didn't find similar material in his MLAPP book. I checked out Matlab implement. There are only two types diag and full, both of them could be shared (tied) or not shared. I think spherical is rarely used. I am just asking is that necessary to refactor covariance updating to that as Matlab does?spherical is the simplest one, right? I don't see a reason to remove it.
Having tied diagonal might be nice, but no-one ever asked for it, so maybe not so important?@tguillemot this is fixed in the new GMM, right?Sorry I haven't seen this issue.
This is fixed by #6666.",None yet
https://github.com/ljwolf/spenc/issues/5,"I guess scikit caught this typo and corrected it in version 0.23 without warning? It's a breaking change for spenc that causes import failure (can be solved by downgrading to sklearn 0.22the fix is to change attribute_score=skm.calinski_harabaz_score to attribute_score=skm.calinski_harabasz_score (with the sz)apparently there is a deprecation, though i've never once seen it triggered@ljwolf you cool to merge this? i'm going to go ahead and move spenc to a function-level import in geosnap to fix upstream, but spenc still won't be useable until we make this changeSorry yeah fine with this. It should also move to spopt.",None yet
https://github.com/shankarpandala/lazypredict/pull/181,This PR updates optuna from 1.5.0 to 2.1.0.,None yet
https://github.com/scikit-learn/scikit-learn/issues/14251,"There have been some issues around ColumnTransformer input requirements that I think we might want to discuss more explicitly. Examples are an actual bug when changing columns: #14237 and how to define number of input features #13603.
Related is also the idea of checking for column name consistency: #7242Mainly I'm asking whether it makes sense to have the same requirements for ColumnTransformer as for other estimators.ColumnTransformer is the only estimator that addresses columns by name, and so reordering columns doesn't impact the model and actually everything works fine. Should we still aim for checking for reordering of columns?Right now, transform even works if we add additional columns to X, which makes sense, since it only cares about the columns it is using.Should we allow that going forward as long as remainder is not used? (if remainder is used, the result of ColumnTransformer would have a different shape and downstream estimators would break, so I think we shouldn't support it in this case).In either case, what's n_features_in_ for a ColumnTransformer? The number of columns in X during fit or the number of columns actually used?
Does it even make sense to define it if we allow adding other columns that are ignored?Actually, whether adding a column works depends on how the columns were specified:And reordering the columns:passes through the ColumnTransformer but what is passed on is wrong.butworks, and reordering columns also works:Similarly, reordering columns should work if you use names for indexing, but not if you use boolean masks. That's ... interesting ... behavior, I would say.Four approaches I could think of:As I said above, if remainder is used we probably shouldn't allow adding extra columns for transform if we do (though we could also just ignore all columns not present during fit).From a user with an (admittedly ancient) usability background who very recently tripped over their own assumptions on named columns and their implications:From this very subjective list, I would distill that:My main wish would be for the whole Pipeline/Transformer/Estimator API to be as consistent as possible in the choice of which of the rules that @amueller laid out above should be in effect. Option number 1 seems to match this the closest. I don't quite understand the part ""...inconvenient as it doesn't really allow subsetting columns flexibly"", however. Isn't it this flexibility (as I understand you mean between fit and transform) which only causes problems with other transformers? I can't see a practical use case for the flexibility to have more/fewer columns between fit and transform.Reading my own ramblings (stream of consciousness indeed), I would not see a lot of harm for the end user in deprecating the ability to specify columns by name and only allow numeric indices/slices, because:The documentation could then refer users seeking more convenience to the contribution sklearn-pandas.So this is more or less a 180 degree change from my initial suggestion, but I found myself using a lot of ifs even explaining the problem in the issue and pull request, which made me aware it might be possible to reduce complexity (externally and internally) a little bit.Thank you for your input. I agree with most of your assessment, though not entirely with your conclusion. Ideally we'd get as much consistency and convenience as possible.
I also have a hard time wrapping my head around the current behavior, which is clearly not a great thing.Adding extra columns during transform indeed would not be possible with any other transformer and would be a bad idea. However, if you think of ColumnTransformer going from ""whatever was in the database / CSV"" to something that's structured for scikit-learn, it makes sense to allow dropping columns from the test set that were not in the training set. Often the training set is collected in a different way then the test set and there might be extra columns in the test set that are not relevant to the model.
Clearly this could easily be fixed by dropping those before passing them into the ColumnTransformer, so it's not that big a deal.I'm not sure why you wouldn't allow using strings for indexing. We could allow strings for indexing and still require the order to be fixed. This might not correspond entirely to your mental model (or mine) but I also don't see any real downside to allowing that if we test for consistency and have a good error message.Generally I think it's desirable that the behavior is as consistent as possible between the different ways to specify columns, which is not what's currently the case.Also: users will be annoyed if we forbid things that were allowed previously ""just because"" ;)Good points. I did not really mean that clarity and convenience were a zero-sum-tradeoff to me (clarity begets convenience, the other way round... not so sure).If ColumnTransformer wants to be used by users preparing raw data for scikit-learn (""pipeline ingestion adaptor""), as well as snugly inside a pipeline (""good pipeline citizen""), maybe it tries to be different things to different people? Not saying that it should be split up or anything, but this thought somehow stuck with me after re-reading the issue(s).Maybe some sort of relax_pipeline_compatibility=False kwarg? (Yes, I know, ""just make it an option"", the epitome of lazy interface design -- the irony is not lost to me ;). But in this case, it would clean up a lot of those ifs at least in its default mode while it still could be used in ""clean up this mess"" mode if needed. Although my preference would be to let the user do this cleanup themselves)Regarding not allowing strings for indexing: I suggest this because of (at least my) pretty strong intuitive understanding that by-name equals flexible ordering, and to keep other users from the same wrong assumption (admittedly subjective, would have to ask more users). Edit: reading comprehension. :) I guess it depends on how many users have this assumption and would have to be ""corrected"" by the doc/errors (if we end up correcting most of them, we might be the ones in need of correcting).Regarding taking something away from users (relevant xkcd - of course!). True, but it's still so new that they might not have used it yet... ;)I quite like option one, especially since we can first warn the user of the change in the order and say we won't be accepting this soon.Option 3 worries me cause it's an implicit behavior which the user might not understand the implications of.I understand @amueller 's point on the difference between test and train sets, but I rather leave input validation to the user and not enter that realm in sklearn. That said, I'd still half-heartedly be okay with option 2.I'm fine with 1) if we do a deprecation cycle. I'm not sure it's convenient for users.
Right now we allow extra columns in the test set in some cases that are ignored. If we deprecate that and then reintroduce that it's a bit weird and inconvenient. So I'm not sure if doing 1 first and then doing 3 is good because we send weird signals to the user. But I guess it wouldn't be the end of the world?@adrinjalali I'm not sure I understand what you mean by input validation.
Right now we have even more implicit behavior because the behavior in 3 is what we're doing right now if names are passed but not otherwise.Should we try and implement this now that the release is out? @adrinjalali do you want to take this? @NicolasHug ?
I can also give it a shot.I think clarifying this will be useful on the way to feature namesI can take this @amuellerColumnTransformer allows for negative indexing.
With #14237 merged, we still allow transform to have more features than fit.So this creates a silent wrong behaviour:I'd go simple and error if transform doesn't get the same number of feature as fit.I keep running into the fact that with remainder=""drop"" and named columns, transform requires same columns or more than fit even when they are not used (also #15085). I think it would be worth fixing this, even it if means special casing the all named columns case.The use case is either,Filtered columns on train now getting,it is quite annoying, because it means that the promise that a pipeline is a end to end solution between raw data and predictions doesn't hold. Now you need to prefilter/order columns in a consistent way both during train and predict. Re-opening this.BTW, some of the complexity is due to the fact that we are supporting both positional and named based indexing for dataframes. One could argue that ColumnTransformer should only support slices via getitem which would correspond to positional indexing for ndarray and named based indexing for dataframes. Though this would certainly raise other concerns though.I think I definitely wouldn't want to do that in ColumnTransformer. I'd be happy to have a ColumnSelector which would do some loose validation and select the columns and return the same object type as the one given to it. But since it's quite different from the other transformers we have, I rather have that either as an example or something in sklearn-extra, WDYT?Do you mean named only columns for dataframes or handling better remainder=""drop"" with named columns? For the former, it was just a though, I'm not particularly attached to it.I mean, I'm happy with ColumnTransformer being very strict about the input, and having another transformer to handle loose kinda input.OK, take a simple example,so you would say, OK have a model that works let's serialize it and make predictions on new data. Except no, that won't work because new data doesn't have the 'target' column and so this pipeline would fail once deployed. Yes, it's possible to fit on df_train.drop('tagret', axis=1) but it's just an another example illustrating the issue.Now as a user I don't understand why I would have to do something like,and keep the list of columns in sync, particularly if it means installing another package (scikit-learn-contib). It's just a very simple use case, and in real word situation the dataframe predictions are made on and the dataframe used for training can have slightly different columns and order (maybe there is just some other extra column used by another system). As long as access is done based on names, I don't see why the order need to match exactly. I mean I understand the implementation constraints due to supporting other types of slicing, but as a user I find this very unfortunate.I'm saying the user should take care of what the data exactly looks like when they feed it to an sklearn pipeline. I just don't think it's in the scope of sklearn to handle random and varying input shapes and columns, and I think doing so would result in silent bugs. I would very much rather have users write something like:I think of not doing so as bad practice, and I'd discourage people from having such patterns since it makes their code fail with odd errors if the input changes.I disagree. For me that's exactly what ColumnTransformer is for: i.e. select some columns from the data which can have plenty irrelevant columns and extract features from them. As long as the columns used by the model are there, it should work. IMO the point of a pipeline is to take raw input and return the prediction. Having additional column ordering/selection step outside of the pipeline is a problem. Having a new column selection step inside the pipeline is also annoying since ColumnTransformer already has all the information to do it.For other estimators I agree. In my mind ColumnTransform is the only one that should correctly handle named columns irrespective of order or unused colums. What kind of bugs you foresee? If the column is there, it would make the calculation it it isn't it would fail. I get that things are getting a bit more nuanced for integer indexing or boolean mask indexing but in that case the current situation is fine. I imagine most users still apply ColumnTransformer with named columns.Let's see what others think.From a user point of view, I think having more flexibility with named columns would be nice.",API
https://github.com/dabl/dabl/issues/236,"I was dabbling with this new library and going through the demo notebooks.
The notebook: https://github.com/dabl/dabl/blob/master/demo.ipynbgives the following error:How can we solve the error?Thank you for opening the issue. What version of scikit-learn are you using and what command gave you that error?VersionsSee #238 (comment) for that error. That was an issue in a deprecation in scikit-learn and has been fixed in the development version of dabl. Please try installing it from pypi.The issue is solved for dev version. I am closing the issue:",None yet
https://github.com/codewars/codewars-runner-cli/issues/570,"https://esolangs.org/wiki/Brainfuck#Implementation_issuesThe current interpreter used for the BF language version uses 0xFF as EOF, which poses a trap for both kata makers and kata solvers:At least this should be documented somewhere.Is this what you mean?/cc @DonaldKellettOlder docs in documentation/environments needs to be reviewed and reflected to codewars.com wiki so it's easier to find.Yes.I haven't encountered that piece of information anywhere (for as long as I've been doing BF katas). I'm not quite sure where to put this, since it should not be put in example tests because it's an important implementation detail, but putting in on the wiki does not help either (nobody should read the runner environment docs to know the implementation details before they attempt any katas. That page is mostly for reference on setting up tests).We have this wiki page, but it's not maintained very well since it has been created. I think we can fill in the necessary language specifics there (e.g. the , command writes 255 to the current cell on EOF).The pages also need the full list of currently supported extra packages (e.g. Python has numpy, tensorflow, etc... but it's not listed there; it's listed here in this repo's doc but it's rather hard to find IMO; Dockerfile shows even more packages, but no clue what they are for.) along with links to their own documentations whenever possible. (Package without any documentation is unusable after all.)Yet another thing: There are many undocumented features for CW test framework (e.g. JS doc has nothing about before and after, which looks like setup and teardown in OO-style TDD but no one knows exactly how to use it).From my experience, I don't think this is a reliable nugget of information to rely on. It seems while reaching -1 almost certainly means EOF, there are often unwanted characters before then (I think the \n character?). BF is already hard enough on its own, having users implementing perl's chomp functionality would make it unnecessarily complicated.It would be nice if the BF environment actually gets changed to feed exactly the given string in the method (so there are no redundant non-print chars) and then 0, which while isn't a standard implementation is often cited for how much sense it makes. Both this online interpreter, this visualizer and this ide writes 0 to the cell when the input is absent or terminated.",area/documentation kind/discussion language/brainfuck
https://github.com/microsoft/graspologic/pull/463,"Added a Copyright and license header to the top of each python file.LICENSE.txt added with MIT license and LICENSE file with APLv2 license removed. Note: this means we are changing the licensing from Apache License to MIT License.
We have received agreement from all contributors to this change.Updated CONTRIBUTING.md file with new information, though I suspect it will break the sphinx docs page being generated now. Going to coordinate fixing that in a different way
with @bdpedigo, but specifically trying to make sure we take advantage of the Github flavored markdown as much as possible.README.md still needs more work but I want to postpone that until we actually do the full rename in setup.py and update all the pypi paths and RTD doc paths and whatnot. Also should probably put a blurb or a link to a .md file in there about the merger in general.one minor suggestion, feel free to ignore if it makes sense to address later but i noticed it was out of date even for graspy :)Deploy preview for graspy ready!Built with commit b43f1e4https://deploy-preview-463--graspy.netlify.app",None yet
https://github.com/microsoft/graspologic/pull/463,"Added a Copyright and license header to the top of each python file.LICENSE.txt added with MIT license and LICENSE file with APLv2 license removed. Note: this means we are changing the licensing from Apache License to MIT License.
We have received agreement from all contributors to this change.Updated CONTRIBUTING.md file with new information, though I suspect it will break the sphinx docs page being generated now. Going to coordinate fixing that in a different way
with @bdpedigo, but specifically trying to make sure we take advantage of the Github flavored markdown as much as possible.README.md still needs more work but I want to postpone that until we actually do the full rename in setup.py and update all the pypi paths and RTD doc paths and whatnot. Also should probably put a blurb or a link to a .md file in there about the merger in general.one minor suggestion, feel free to ignore if it makes sense to address later but i noticed it was out of date even for graspy :)Deploy preview for graspy ready!Built with commit b43f1e4https://deploy-preview-463--graspy.netlify.app",None yet
https://github.com/scikit-learn/scikit-learn/issues/13211,"When using SVC(probability=True) or SVR(probability=True) the output of predict_proba will not necessarily be consistent with predict, in the sense that,this is documented in the user guide,IMO this is a violation of the API contract and should be fixed.This is being continuously reported as a bug e.g. #4800 #12408 #12982 and a few stack overflow issues e.g. https://stackoverflow.com/a/17019830I encountered this in a project where detecting this discrepancy, evaluating the difference and deciding whether predict or argmax(predict_proba should be used in the end took some effort.One pitfall is for instance to use predict to compute the accuracy, and then predict_proba for ROC AUC which can lead to somewhat problematic results if the predictions of these methods are not consistent.Several approaches could be used to fix it,One possibility could be to deprecate, but keep it to allow access to that functionality in libsvm.AFAIK this only happens when the data is not scaled. We could raise a warning if the user passes unscaled data. Do you have an example where it happens with scaled data?This was observed on the L2 normalized TF-IDF data which was scaled.I don't have a reproducible example (at least not one I can share). I can try to reproduce this later but the number of issues about it and the fact that the docs say that predict and predict_proba can be inconsistent in general is a strong indicator.Here is a minimal example,which returns,Not that using SVC in this use case is good but that's another story (#13209)...@agramfort I agree. WDYT about deprecating (but not removing) probabilites=True and suggesting to use CalibratedClassifierCV explicitly instead? (cf issue description for more details).Another user report of this bug #14100.It's a pretty bad bug IMO because it breaks one of the assumptions of the API and it takes some effort to discover it. If we run estimators check on SVC(probability=True) they would fail, as we enforce it there.Another solution could be to expose CalibratedSVC, and deprecate SVC(probability=True), since CalibratedSVC and SVC are two different models.I think CalibratedSVC sounds like a very reasonable solutionIt would be good to benchmark the difference between using CalibratedClassifierCV(SVC()) and SVC(probability=True). @rth you say it's faster? @agramfort why do you expect it to be much slower?
I think that would be my preferred solution.If we introduce another class, CalibratedSVC, we have to duplicate a lot of API and create a lot of boilerplate. I'm not sure what ""being two different models"" means because estimators often implement widely different things depending on the parameters.Basically people should also just stop using probability=True. It's very expensive and users are not aware that is expensive and what it does. Having a separate class would make that more clear, but if we can get rid of our reliance on the Platt-Scaling in libsvm and use our own, I'd prefer that.Well in the one example I have tried. More extensive benchmarks would be needed (also note that 5 CV folds need to be used in CalibratedClassifierCV to be comparable I think)What I mean is when one does,this object contains 2 models. One (uncalibrated) for est.predict and one (calibrated with Platt scaling, using 5 CV folds) for est.predict_proba.I guess the main concern with deprecation is backward compatibility. For instance, people using SVC(probability=True) in some production system, would have to fall back to CalibratedClassifier(SVC()). That should theoretically produce equivalent results, but in practice can we really guarantee that?Another solution is to just to take the argmax of the probability for predict when predict=True, but that's also not backward compatible.(In general I have not read the libsvm code that much, so I may be missing something)We don't need to if we deprecate, right? We're providing an alternate solution, it doesn't need to be exactly the same.Or one could return softmax(clf.decision_function(X)) internally when returning clf.predict_proba(X)... ?@psds01 that would definitely defeat the point of the probability=True option and would also not be backward compatible.I just ran into that issue today, it is especially problematic because we cannot trust the ordering in the internal classes_ variable which lead to a pretty deep and hard to locate bug in my case.Here is a minimal example that shows the problem for me: https://gist.github.com/jgrizou/6a8ab93c24a6deac9682ca4c1c778766In the example, you see that a classifier trained on the exact same data (X) but with different labels (y0 or y1) leads to the columns from predict_proba() output to be differently allocated to the labels. Which is needless to say very problematic.Below is a visual description of the problem, on the left the classifier map matches with the data (that is for y0), on the right the probabilistic map resulting from predict_proba() is flipped. Those are the same data with different labels, and the SVC was trained starting with the same seed.As a workaround, I had to implement a function to compute classes_ to match what the predict_proba() returns, see get_ordered_classes(clf, X, y) in the gist linked above.Maybe this can be a quick solution for you, when probability=True, you could recompute the classes_ variable to make sure it matches the data and users can safely trust it.I take the opportunity to thank the all community for the hard work on sklearn!It turns out that my get_ordered_classes(clf, X, y) function is very naive and does not scale. This is because predict() and predict_proba() disagree sometime and as said before predict might differ from argmax(predict_proba).Thus it seems impossible to know with absolute certainty which column refer to which label in the probability matrice returned from predict_proba().Am I missing something here? Is there a way to know with certainty which column of the probabilistic prediction matrice is associated to which labels?No it is not always the case, you can see a minimal example here: https://gist.github.com/jgrizou/6a8ab93c24a6deac9682ca4c1c778766. (sklearn.version == '0.20.2'). I think it is actually impossible to know for sure when classes_ is correct or not.From my quick investigation, it might be because predict_proba() rely on another library (libsvm) but no reference to the ordering of classes is passed between the two programs, seeI ended up using CalibratedClassifierCV which works as expected (although the proba prediction values differ significantly). Given these issues, I would vote for removing the probability=True and guide users towards CalibratedClassifierCV. Or fix the classes_ issue if this is feasible.@jgrizou why did you use ""prefit""? You shouldn't use that if you want to replicate the behavior of probabilities=True. If you use prefit and use the same dataset for training and calibration you're basically defeating the whole point.I'll try to check out this issue, I messed with the ordering of the classes_ here way too much years ago :-/The problem with the example is that there's not even enough samples to run CalibratedClassifierCV properly. I'll have to check the code to see what libsvm actually does in this case.@jgrizou I think your ""temporary fix"" is not correct. You're just computing the mapping that would give you the best results. There is no evidence that that is actually what the model is using.I also get somewhat different results between CalibratedClassifierCV and libsvm but not dramatically so.If I useI can run CalibratedClassifierCV to compare results, but for that dataset, the results are consistent between predict and predict_proba.Ok, looked a bit closer through the code and libsvm just doesn't do stratification, so you can easily reproduce the behavior with thisturns out that, unsuprisingly, the result basically only depends on the random state. If you have 6 samples and do 5-fold cross-validation, if the problem is not trivial, the priors will just tell you to predict the opposite of the class, which is exactly what happens.So I would argue what @jgrizou is expected behavior. Don't do unstratified 5-fold cross-validation on 6 samples.
We could warn in this case, but it might not be worth that if we go for a broader fix.Hi @amueller , thanks for looking into it!Totally agree, see #13211 (comment)You mean that when calling clf.predict_proba() from a SVC with probability=True, we call libsvm and that by default (without me asking) it is performing a cross-validation procedure (with a number of fold I did not specify and cannot know), and this is leading to a bad/reverse fitting due to the low number of samples ?If it is the case, the user should know that a cross-validation is being applied.If this is not what you suggested, I have tried re-explaining the problem below.My concern is that clf.classes_ does not match with the ordering of the clf.predict_proba() matrice generated from a SVC with probability=True.There is no cross validation and the class prediction sometimes differ between predict() and predict_proba(). This is shown in the first two examples in: https://gist.github.com/jgrizou/6a8ab93c24a6deac9682ca4c1c778766.Here is the problem:The output of predict_proba() is:and the variable classes_ contains: [0 1]. Which should indicate the class ordering of the columns in the above matrice.Hence predict_proba() class prediction is: [1 1 0 1 0 0], which is the opposite of the true labels.I also initially thought it was a problem with the classifier having a hard time fitting the function with few points, and hence fitting the opposite function. But the problem is not that hard really, I do not want/ask cross-validation, 6 samples in a 2 dimensional space. See the actual data below.I think it is a bug not an expected behaviour because since I switched to CalibratedClassifierCV I have never encountered this problem again.PS: I am aware my use-case is weird but those are constraints coming from my research. Same with the use of prefit when using the same dataset for training and calibration. I do not have enough data to do a proper CV procedure but I still need probabilistic predictions.You very explicitly asked for it by setting ""probability=True"". That is exactly what this option does. That is documented at the very top of the SVC user guide and at several other places:
https://scikit-learn.org/dev/modules/svm.html#support-vector-machinesWe should probably add it to the ""probability"" parameter as well, where it's not so far.yes there is 5-fold cross-validation whenever you set probability=True.And using ""prefit"" with the training data for calibration is not a proper use. There is no easy way to guard against that, but ideally we'd raise an errorYou're using cv='prefit', which is not equivalent and arguably wrong. If you ask for different behavior you'll get different behavior. If you had used cv=KFold(5) you would have seen the same behavior, depending on the ordering of the data (liblinear shuffles by default, sklearn does not, but you can set shuffle=True in KFold.Thanks @amuller for taking the time to answer and explain again.You are right cross-validation is mentioned multiple times in the documentation and that probably explains the discrepancies/reversed mapping I observed.I probably exhausted my quota but I will risk looking like a fool again :), just to try and wrap my head around this.What I find weird is that I often got fully reversed mapping from SVC with probability=True. Not just a badly fitted classifier but a totally wrong one with a boundary line exactly where it would be if it was fitted with the opposite labels.I tried to reproduce the problem with cv=KFold(n_splits=5, shuffle=True, random_state=somevalue). I tried 10000 random states (see bottom of gist), and I could indeed train a classifier that made prediction errors (with scores of 0.33, 0.5 & 0.83), but:While I observed both of these things fairly often with SVC set to probability=True in my use-case, and at least once in the example I provided.All in all it does not really matter. It is just intriguing. It seems weird that the libsvm sometimes returns reversed mapping (with an accuracy of 0), and the cv=KFold(5) never does that in 10000 attempts. Maybe the ordering of columns returned by libsvm is wrong sometimes hence the reverse map. Or maybe libsvm does something different than cv=KFold(5) and does actually return a reverse map. Or maybe I am far out of my league and I am missing the obvious.Just to be clear, I am discussing something I found intriguing, I do not complain about this. sklearn is amazing work and I thank the all community again. I am just being curious here and sharing what I observed.@jgrizou I wouldn't say you exhausted your quota yet (sorry if I was a bit snippy) and I appreciate your last remark :) I agree that the behavior you observed wrt the difference between CalibratedClassifierCV using KFold and SVC(probabilty=True) is not sufficiently explained by what I said.I'll try to look into it. We're looking for ways to make the probability=True less confusing (it is quite confusing right now, and no amount of documentation can solve confusing API). In the meantime, #14618 tries to add another pointer that might help debugging this easier.As I think you acknowledge, trying to do cross-validation on that few samples is probably not a good idea and so why it fails might not be the most important question. But I'm very interested in making the behavior here less surprising.Putting aside reports of inconsistent results with probability=True which, if true means something is likely wrong in libsvm, is an orthogonal issue and should be addressed in #13662. This issue is about purely API consistency and decoupling of probability=True|False and consistency of predict and predict_proba.To re-iterate; the fact that predict and predict_proba can be inconsistent is IMO a bug, that breaks the API expectations and no amount of documentation is sufficient to fix it, in my opinion. We have a common test for this that passes because this issue happens only occasionally.So the choices (adapted from the initial issue description) could be,
a) deprecate probability=True and predict_proba, then suggest using CalibratedClassifierCV + SVC. This is bound to make users unhappy, who are currently using this option.
b) when probabilty=True, compute predict as argmax of predict_proba. It could have been the solution except that it silently breaks backward compatibility. And I guess a lot of users run SVC(probability=True) in production -- we can't make this change silently.
c) deprecate probability=True and predict_proba, then add e.g. CalibratedSVC and CalibratedNuSVC classes (ending with CV could have been better, but harder to read) that behave as SVC(probability=True) and where predict is computed from predict_proba.
d) deprecate probability=True and replace it with probability='calibrated' for which predict is computed from predict_proba.I would probably vote for d) unless we are OK introducing 2 other classes in c).Thoughts @amueller ?there's another option which is to fix libsvm, don't we?As far as I understand the issue, predict_proba does calibration with cross validation, while predict doesn't. That's not something that is fixable at the libsvm level -- it's just two separate models that we expose in one estimator.yeah, then I guess I like options (a) and (d).I'm gonna try to run benchmarks soon to compare runtime with CalibratedClassifierCV. At first sight I don't think they would be so different.Also note that in https://github.com/scikit-learn/scikit-learn/pull/16769/files#diff-b93e3121b913e8548606f6ddc913e5d4R227-R232, we're basically saying that they're equivalent.I'm fine with a and d.What about e) use a CalibratedClassifierCV trained with prefit=True under the hood? We can either make it a bugfix or introduce a new option for probabilities and deprecate the old one.(That only works if CalibratedClassifierCV is consistent in its predictions but I think that's the case, right?)The difference in terms of time isn't super obvious but it looks like the CalibratedClassifierCV is a bit slower. Comparing these 2 pipelines on about 30 datasets from CC18:svc is faster in exactly 50%, and on average, time(svc) / time(cc) is about 1.3.(all results are averaged over 10-fold CV for all datasets)@NicolasHug That's basically d) but using CalibratedClassifierCV instead of the libsvm implementation, right?If we say we want to get rid of the libsvm cross-validation because we prefer ours, which we have more control over, that's fine, but we still need to address the problem of two different models, so the effect is to deprecate probability=True and add a new option.FYI for weird reasons, all SVC results on openml are basically using probability=True and will break if we change the parameter. Not saying we shouldn't do it, but it will be quite annoying in some places.I favor d), though if we do a deprecation cycle, maybe e) is better because it allows us to keep the convenient user interface while also removing the weird code path.I guess we could cheat and say that CalibratedClassifierCV is faster or similar with the default 5-fold CV ^^ Are you sure it's not using 8-folds in libsvm?Generally I agree with the above comments -- it's a balance between not wanting to maintain calibration with CV in libsvm and avoiding to break existing productions models that potentially won't get exactly identical results with CalibratedClassifierCV as before and will need to re-validate models.It would have been nice if we had some way of polling users between 2-3 main choices. Not that it would fully determine the outcome but just to get a feeling.Personally I'm OK with any of the options mentioned in the last few comments.I was thinking of still relying on libsvm for predict, but that might not make methods consistent then. I'm not sure. I guess I'd rather have d) anyway, which we can even provide as a bugfix if we want to.The paper says 5 folds (top of page 31). The 10 folds are a different thing and come from openml 's run_model_on_task, i.e. for each dataset the total time is averaged over 10 folds.Yeah that wouldn't fix the issue then ;)5-fold is defined here:any objection to d) @jnothman ?moved to 0.24None, except that probability='calibrated' looks ugly to a user unfamiliar with the history.",Bug Needs Decision
https://github.com/scikit-optimize/scikit-optimize/issues/925,"Using scikit-optimize version 0.7.4, the doctests fail:This seems to have been fixed with 0.8.0, closing.",None yet
https://github.com/scikit-learn/scikit-learn/pull/17670,"Resolves #7980
Superses and closes #8301This PR defined a parse_version function in sklearn.utils and replaced some ways to compare versions in some files.Fortunately, some files mentioned in #8301 are already fixed by other changes:@hs-nazuna thanks for your PR. I edited your PR description to have ""Close #8301"" so that the previous PR will be closed if we merge this one.Hmmm I am not too sure why all the CI are red, investigating ...How can I avoid this error ...?The following is my new idea.Thanks @hs-nazuna ! A few minor comments but otherwise LGTM.I removed _parse_version which was no longer used (and is private so it is fine to remove it without deprecation cycle).I removed another back-slash (similar to one of @rth's comment). I think this looks good and should be good to merge once the CI is green.I also ran the tests with the minimal version of dependencies just to check that nothing was badly broken.Merging, thanks @hs-nazuna !Thanks a lot @hs-nazuna it feels good to see a 2-3 year old issue getting closed! Thanks @rth as well for proposing the good compromise of using setuptools in the 99% cases where setuptools is installed.",None yet
https://github.com/kivy/python-for-android/issues/2157,"So I built my app and installed it to my phone (Pixel 2), I tried testing it (with pandas in requirements) and it didn't work.Can anyone help? Thanks in advance.This is an unusual build platform, did you check if pandas built correctly in the p4a log?And there is another issue scikit-learn is not pure Python and does not have a recipe, so this will fail.Yes, I have checked the log, also I don’t encounter an error with scikit-learn seems to have built correctly but I might be wrong... Also, what do you mean by ‘Unusual build platform’. Are you talking about building on Colab?Yes, nothing negative (to my knowledge) about that platform; the default installed libraries do vary between build platforms which can cause a build fail.The information shared includes:
15:38:05.856 8025-8066/com.graffbt.coronaviruscp I/python: ModuleNotFoundError: No module named 'pandas'This is normally interpreted as the module is missing from the apk. Yes, the issue is found by the app, but that does not necessarily mean the cause of the issue is in the app. In this case the message is interpreted as something is missing from the apk, which is likely due to the apk build.You can check if pandas is in the apk by unpacking the apk, can you find pandas?
Instructions are here
https://python-for-android.readthedocs.io/en/latest/troubleshooting/#unpacking-an-apk
(do both the unzip and the tar steps to see the python related files)If you can't find pandas, the next step is p4a clean_all (this is important)
Then p4a apk , save the FULL p4a log output, search for all instances of pandas.Logs are:Also, looked through the APK in Android Studio, it doesn't last time I checked...I'm sorry, I don't understand what ""it doesn't"" means in this context.The log file is missing some stuff, was p4a clean_all used first?And a reminder, when we get past this issue, scikit-learn is not going to work.",None yet
https://github.com/EpistasisLab/scikit-rebate/issues/65,The deprecation warning in the title shows when importing ReliefF from the latest version of skrebate published to anaconda. I believe it just requires replacing the sklearn.externals.joblib import with joblib?Thankssame here this should be changed,None yet
https://github.com/scikit-learn/scikit-learn/pull/16625,"Closes #10488
Fixes #10144
Fixes #8234This implements a top-k accuracy classification metric, for use with predicted class scores in multiclass classification settings. A prediction is considered top-k accurate if the correct class is one of the k classes with the highest predicted scores.Thanks for the PR @gbolmier , a few comments ;)Thank you so much for taking the time of the review @NicolasHug!
Let me know if there is more to improve :)Thanks @gbolmier a few more.Let's also add a small section in the User Guide!Don't we want a default scorer for that metric? I understand that k might be blocker for that, though.I did add the metric to THRESHOLDED_METRICS. To avoid failures, in many tests (see changes in test_common.py) that expect y_score can be 1D, I created a METRICS_REQUIRE_2D_Y_SCORE, and added if statements in the tests concerned.I'm not a huge fan because it adds an additional layer of complexity to the tests, but couldn't think of a better solution, any suggestions?BTW, I don't understand how y_score in other metrics could be 1D, even in the binary case we need both scores to discriminate them as we are not only dealing with probabilities. What am I missing?I do have an entry in doc/modules/classes.rst, but when I generate the doc, the link isn't working:Thanks for your help :)Looks like it renders correctly on CI: https://103206-843222-gh.circle-artifacts.com/0/doc/modules/classes.htmlRe 1d: this metric, similar to others only requires the ranking of the data. For binary, the decision_function returns a 1d array.This is already implied by the combination of THRESHOLDED_METRICS and METRIC_UNDEFINED_BINARYplease fix the merge issueHi, @gbolmier looks pretty work. Just wonder that will this top-k metric provide global update? as follows:Hi @pengxin99! Nope, you need to give to the metric all the data at once (same for all other metrics in sklearn)Ok, thanks :)Should be better like that @jnothmanThanks for your work and patience @gbolmier, a few more minor comments below otherwise looks good.Also please add it to https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-valuesThis is how you learn the ropes :), thanks a lot for the review @rth! Let me know if it needs any other changesThanks @gbolmier! LGTM. Should be good to merge unless anyone has more comments?@jnothman I think you review comments were addressed.",Waiting for Reviewer module:metrics
https://github.com/numpy/numpy/issues/5888,"Not sure whether this is something we consider a bug, but flagging for potential discussion and so it doesn't get lost: I hadn't realized until today that NumPy in practice does not provide a forward compatible ABI, i.e., if you build with numpy 1.9 then try to run against 1.8, this may not work. Apparently packages that care about this like sklearn are actively working around this by carefully installing old versions of numpy before building wheels.In particular, we have several times added extra fields to the dtype struct. In practice this is normally fine b/c no-one actually accesses these fields, but Cython in particular does struct size checking. For backwards compat -- build against 1.8 and then run against 1.9 -- the struct appears to get larger, and Cython merely issues a warning (which we suppress). For forward compat -- build against 1.9 and then run against 1.8 -- the struct appears to get smaller, and in this case Cython issues a hard error.We could work around this by simply exposing a truncated struct to user code, so that Cython sees a small struct when doing sizeof, and the actual object is always larger then this, meaning that we always hit the warning path rather than the error path.I don't know if this is the only problem we would have to fix in order to achieve forward compatibility, e.g. I haven't checked the C API import code to see what import_multiarray or import_umath do when they find themselves running against an older version of numpy.If we want to take ABI compatibility seriously I guess we should probably also start requiring C API users to explicitly state which version of the ABI they expect, and enforce that they don't get access to anything newer than that. This would at least give us the option then in the future to provide different versions of the same function to old-users and new-users.I'm pretty sure that numpy itself also raises an error when running against an older version than compiled against: https://github.com/numpy/numpy/blob/master/numpy/core/code_generators/generate_numpy_api.py#L90This is related to the desire to hide implementation details, which is discussed in http://docs.scipy.org/doc/numpy/reference/c-api.deprecations.html#background in the docs. And maybe in other places?Some more digging for discussions on the Cython issue: RuntimeWarning http://thread.gmane.org/gmane.comp.python.cython.devel/13072. Still can't find the more recent discussion though.This has come up again now that pip has started to properly support build-requires (see PEP 518). The problem is that scipy (for example) then has to decide what version of numpy to declare a build-dependency on. If they depend on plain numpy, then they'll always be built against the latest version of numpy, and then you end up with scipy binaries that require the latest version of numpy, and this breaks stuff. OTOH if scipy declares a specific version of numpy to build against, like numpy == 1.8.1, then they have problems because this doesn't work if you're trying to build on python 3.6, because numpy 1.8 just flat out doesn't work on python 3.6. Instead you need numpy == 1.2.x. Or worse, maybe you're building on python 3.7, where no-one knows which version of numpy you'll need, yet scipy has to make a guess and encode into their sdists and if they guess wrong then things break.There's lots more discussion here: pypa/pip#4582Anyway, it seems like what we really want is a way for scipy to tell numpy ""I'm using the 1.8 ABI"" at build time, and produce a binary that uses the 1.8 ABI, even when building against a newer numpy. That breaks us out of this loop, because scipy can build-require numpy >= 1.8 and build against the latest version, but then the resulting binaries work the same regardless.One way this might work:We already have most of the code we'd need to implement this – every binary that uses the C ABI already gets a magic number compiled in saying which version of the ABI it expects, so if you have a scipy-built-against-numpy-1.8 and import it on numpy 1.13, then numpy 1.13 has a little table that says ""ah, this is expecting the 1.8 ABI, which contained these entries, let me export just those ones"". The problem is that right now this flexibility is only exposed at import time, not at build time.Thanks for bringing this up again @njsmith. I've thought about it for a bit just now and have the vague feeling some tricky issue could come up (also remembering the pain of the not working OS X 10.5 SDK), but can't put my finger on it. Implementation of what you propose seems relatively straightforward.@njsmith I assume you mean 1.12.x?Yes.Out of curiosity, where is the magic number table defined?Related: https://discuss.python.org/t/support-for-build-and-run-time-dependencies/1513I've just read through that, and agree with most the responses you got from Nathaniel, Thomas and Paul. In general, this isn't really a major issue, it's just something to be aware of at the moment: just build your wheels against the lowest NumPy version you want to support at runtime.That one requirement can be expressed in the current pyproject.toml and setup.py. A new option ""use this ABI"" would perhaps make this a little easier, but it's low priority imho given that in practice there aren't many complaints about this.I don't think there is any guarantee that this will continue to work for every numpy version ever released in the future. For example, maybe some day numpy 2.0 will be released with a very different API which won't be compatible with packages built against numpy 1.x. So having some way to indicate that would still be useful.By the way, I never claimed that it's a big issue. But I've been bitten by this (not with numpy but with another Python package with a C API).Of course, there is no such guarantee. That's a different issue though - the only way to protect against future API or ABI changes is to use install_requires <= numpy_current_released_version for your own packages. Which would be the right thing today in many case, but unfortunately no one does this.Well, we would definitely increment major version, so most software (with very little risk) should be able to say < 2.0 probably? Not sure that is good habit though.But why not? Why can't numpy have some kind of ABI guarantee? In fact, that's already the case in practice. Why not make it official? This will not make it impossible for numpy to change its ABI, it just means that the version needs to be increased to 2.0 if that happens.@jdemeyer I think we have that guarantee, except in some rare cases were we have a long deprecation beforehand. I suppose I misunderstood the thread maybe then.We will do exactly that, and make every effort to keep ABI compatibility for 1.x. Again that has very little to do with either forward compatibility or your pypa thread though. It seems like we're talking past each other here. If we release 2.0 with an incompatible ABI, then one should build against 2.0 as the lowest supported version. That can be done today, simply by putting numpy == 2.0.0 in pyproject.toml. In whatever other scheme you come up with you'll have to make the same update.We recently ran into this with pycuda + numpy as well. We hadin our requirements.txt, but that will not work since pycuda has numpy>=1.6 in its setup_requires, which means it will be built and linked against the latest version of numpy currently on PyPI (1.16.4) (installed to a temp folder by pip during package building), and the numpy 1.14.0 we're pinning is not forward compatible with that, so you get a crash when trying to import pycuda.Reported this against pycuda here: inducer/pycuda#209A possible workaround from the pycuda side would be for them to pin to the exact oldest version against which it can build (so 1.6 in this case) in its setup_requires. That would make the pycuda maximally compatible with any later 1.x numpy the user wants to use (provided numpy maintaines backward compatibility throughout the 1.x series). At the moment it sort of requires the user to use the latest numpy (or well, whatever was the latest when pycuda was built).If I recall correctly, SciPy pins its build system to the oldest numpy it wishes to support, but tests against later numpy versions.@mattip Alright, that sounds reasonable. I'll see what @inducer says regarding pycuda.",23 - Wish List
https://github.com/sebp/scikit-survival/issues/120,"Hi, I tried to import sksurv.ensemble and got this errorAnd here is my version:Is there anyway to fix this problem?Hi, I have updated pandas but still get the same error.I have also tried conda update --all afterwards but the problem still persists.And also, after I restarted jupyter notebook, it showed another error:But this error is only shown ONCE. Afterwards, it went back to show the previous errorI think I have figured out the solution. The previous user of my computer used pip install for almost all the packages, while I installed sksurv using conda install. After uninstalling and reinstalling sksurv using pip, everything works well nowAnyway, thanks a lot!",None yet
https://github.com/scikit-learn/scikit-learn/pull/16018,"Resolves #12153
Supersedes #13833This PR adds the infrequent categories to OneHotEncoder by:Partial review. Nicely thought out, @thomasjpfan! I'm not sure about the inverse_transform behaviour, but I don't think it is so fundamentally important.Brief review. Please describe the tests.What is the interaction with drop?Ok, this is not supported ATM (As Joel suggested, please document this).I am however concerned about potential future complexity. Supporting interactions with drop is one of the reasons #13833 was so tricky. @thomasjpfan do you think things will be easier with the implementation proposed in this PR? I haven't looked at the code in details so I don't have an informed opinion yetCurrently, if handle_unknown=='ignore', then drop must be None. Going with this, lets assume that handle_unknown=='error' then we can try to support drop as follows:Sorry I wasn't clear, I was more concerned about the complexity of the code, from a maintainability point of viewSee e.g. https://github.com/scikit-learn/scikit-learn/pull/13833/files#diff-d12408664448c94dbd880579e1b2e4d9R452Implementation-wise, I do not see an issue with including the drop support. I would adjust _compute_drop_idx to take into account of the infrequent categories (if they exist).I think the things left to discuss for this PR is:I don't like it for get_feature_names. Don't mind for inverse_transformJust a couple of glitches for the style. I will make an in-depth review now but from what I remembered it was already fine.As a note for reviewers: The reason this PR deprecates 'ignore' in favor of 'auto' is shown in this example:An alternative is to keep 'ignore', where we would ignore unknown categories
during transform time:everything failing ;)partial review, gotta runLet's leave this until after #17317 is merged?Okay, we can wait for missing value support first.",module:metrics module:preprocessing
https://github.com/neurospin/pypreprocess/pull/331,,None
https://github.com/scikit-optimize/scikit-optimize/issues/676,"I'm interested to implement the Neighbourhood Algorithm (NA) in Python. This algorithm is quite popular in the geophysical community for solving optimization/inversion problems. It is detailed in this paper and I summarize it here below.Instead of reinventing the wheel, it would be great to reuse the nice features that are implemented in scikit-optimize like its API, high level plotting functions, benchmarks, (dask)/joblib integration, serialization of optimization results and possibly more in a near future.However, I'm not sure if the algorithm described above would fit in scikit-optimize.Also, I would fully understand if you feel that such algorithm has not its place inskopt. My question is then: is skopt designed so that it is easy to extend it with additional algorithms while still leveraging all the features mentioned above? For example, is it safe to subclass Optimizer or create my own class with a similar interface?Sorry for this long post and for asking a lot of questions. I'm very new to skopt. I should probably have looked more into skopt's docs and code before, but I really want to avoid unnecessary development effort if I can just reuse (at least) bits from skopt. If you think this algorithm would fit in scikit-optimize, then I'm happy to start working on a PR.I should add that this algorithm has been published a while ago, and although I'm not very familiar with the most advanced methods currently implemented in skopt I wouldn't be surprised if the later outperform it in many aspects.IMO our community (geophysics / geodynamics / geomorphology) would still benefit from an implementation of NA within the Python scientific ecosystem (a Fortran version already exists). This would be useful at least for quick comparison/benchmarking and for legacy purpose.Hi Benoit! Good to see you around here.There isn't much activity these days, mostly because the few core devs are busy their life, so it might take some time for us to respond.In principles, I think it would be nice that scikit-optimize eventually expands beyond bayesian optimization to include more gradient-free optimization techniques. However, every feature we include adds in terms of maintenance cost, which we can already barely sustain.On the other hand, developing a new algorithm might help us identify issues in the interface with regards to use cases we didn't really consider, so that might be beneficial in the end.What do others think? @betatim @iaroslav-ai@benbovy Besides the interface, do you see components that could be readily reused?Hi Gilles! Thanks for your answer. No worries if it takes time for you to respond, that perfectly fine!Yeah, I fully understand that and I'm fine with the idea of implementing it in a separate package.Most of the components I think:From a less technical point of view, my hope with integrating NA with scikit-optimize is also ""advertising"" the optimization methods already implemented in skopt, which would be more discoverable to our community (including me) and perhaps better suited depending on the problem we want to solve.(disclaimer: I'm not very familiar with bayesian optimization so please correct me if anything below is wrong).Conceptually, NA like how it is presented in Sambridge's paper is actually very close to bayesian optimization as it also uses/refines a surrogate model for the objective function, which here is simply based on nearest neighbor interpolation (uniform probabilities are assumed inside each cell of the Voronoï tessellation). So I guess using sklearn.neighbors.KNearestNeiborsRegressor would just work here?Where it departs from bayesian optimization in the strict sense is on how the next points are chosen: there is no optimization of such an acquisition function. Instead, the algorithm simply (more arbitrarily?) generates a fixed number ns of new points by uniform sampling within each of the nr “best” Voronoi cells.Maybe there is a way to reformulate this in terms of an acquisition function so that it would be straightforward to implement it with skopt, I don’t know, would it make any sense? It looks like it would add unnecessary complexity. There might be some convergence issues when optimizing the acquisition function, too.Another option would be to create a separate optimizer class that mirrors the API of skopt.optimizer.Optimizer. This seems perfectly feasible to me. My only (small) concern is to maintain such class so that it stays up-to-date with skopt.We discussed (or have interest in) adding other algorithms like for example in #636 and #625. There the issue is also a discussion around ""do they fit into the current assumptions of skopt?"" -> having a bit of a think what we can modify/loosen in terms of assumptions made in skopt to allow adding such other algorithms would be nice.My impression was that there are some algorithms that only work with certain objective functions (like hyperband) and there are algorithms that need a special ""acquisition function"" (like Neighbourhood Algorithm). I say ""acquisition function"" because it would be a weird kind of acquisition function mostly to please the skopt conventions :) Off the top of my head a possible step towards supporting more exotic schemes of picking the next point(s) is to make our infrastructure so that acquisition functions are classes that encapsulate the optimisation of themselves and the optimizer just uses them (instead of currently where Optimizer decides how to optimise the acquisition function?Yes. It is part of the public API so it is a bug if we break it without deprecation warning.Sounds like this would be very useful in this case! I'd be happy to start working on this as my first contribution to this project. Unfortunately I don't have much time for that right now.",None yet
https://github.com/scikit-learn/scikit-learn/issues/17751,"I just saw locally that we have a MatplotlibDeprecationWarning:We might need to edit our conftest to handle this deprecation to avoid some surprises.@glemaitre Hi! Can you give me an example of what you executed that raised this error?@glemaitre one thing we can do is to remove both keywords arguments (warn, force) since we are using their default arguments as mentioned in matplotlib.use()
Do you have any other suggestion?removing arguments will keep the same behavior as before.
Should I make PR with removing those arguments?",Easy help wanted
https://github.com/skorch-dev/skorch/pull/575,"As reported in #573.I think we should agree on a solution for #579 first (i.e. whether to use LooseVersion or packaging.version.@BenjaminBossan Please review again, changed to LooseVersion.Just the minor comment, rest LGTM@BenjaminBossan done",None yet
https://github.com/scikit-learn/scikit-learn/issues/11991,"This can be broken up into multiple pull requests.Remove:Admittedly I'm not sure it is wise to rush this change, given that we might still be backporting fixes to 0.20.X for a little while...I also can't wait for this to happen, but I think we might want to wait for the final 0.20.0 release or even 0.20.1 before removing Py2 code to make backports easier.From experience with IPython/Jupyter and backport: do not rush to remove code, it will make backport way harder, and trip git bisect/git blame.We still have some py2 compat code here and there close to 18 month after dropping 2.7 and only remove it when it bother us.Also feel free to use our backport bot for trivial backport.Should we add some tests to make sure at least the new PRs don't include things such as from __future__ ..., etc.?If master is Py3only, the change to get python2-ism is slim.""I'm going to start implementing this feature by making sure it works with Python 2 even if the whole library is Python3 only"" said no-one ever.There are few construct that will likely survive the transition (not making use of general unpacking... etc), but nothing easy to detect.One thing you might be able to do is turn more deprecation warnings into fatal errors.Thanks for sharing your experience on this @Carreau !There are a few further references to Py2:I am working on sklearn/datasets/base.pyWorking on sklearn/ensemble/tests/test_forest.pyIs there any pending task in this issue?@albertvillanova you could check and see if everything in #11991 (comment) is done, or at least a PR is open for them.@adrinjalali that is the reason why I was asking if there are any additional tasks, because the ones mentioned in this issue seem to have been already addressed.Then I think it's safe to close this one. We can always have PRs which do further cleanups, but seems like this is mostly done. Feel free to re-open @jnothman if you see obvious leftovers.I think all of the points were indeed addressed here.@albertvillanova thanks! If you are looking for for another issue to contribute, you can also have a look at stalled PRs starting with those that are short and not too controversial as they are more likely to get merged faster.@rth thanks for the advice. However it's easier said than done. It seems that most of the PR are waiting for reviews, aren't they?@albertvillanova I've put some effort in checking stalled PR and labelling them as ""help wanted"": perhaps you will find your way in this pool.Thank you @cmarmo. That is very useful!",Easy
https://github.com/scikit-learn/scikit-learn/issues/5150,"This bug appears in current master, and for any dense svm class.Output:Here we see that svmlib internally have lost 2nd class, at the same time sklean's wrapper class keeps all class labels inside, that's why predict_proba returns matrix of shape (n_samples, 2) instead of (n_sample, 3) (what is expected by bagging classifier implementation). I understand that it's insane usage of weights by itself, but together with bagging and dataset with many labels, bagging randomly zeroes complete classes, and this bug shows itself, because bagging expects that svm's return probability of classes which they hold (e.g. all classes).I investigated this a little bit, and can try to fix this, if someone will say that all this usage with bagging makes sense (Because i don't really sure about this).Yeah that looks like a bug. I wonder if this shows in any other models or just the svm. If you are interested, could you maybe add a test to the common tests in utils/estimator_checks.py to see if this happens elsewhere?(or you could just loop over all_estimators()).We could just fix it by handing all points to libsvm but that's probably not what we want to do, right? Does libsvm handle them efficiently?
Could you maybe do a benchmark?I've tested all classifiers that have sample_weights parameter in fit, and have predict_proba method.
Looks like only SVC affected. NuSVC uses same implementation, but it will always throw exception ""ValueError: specified nu is infeasible"" if total sample weight of some class equals zero.Thanks for checking.Here a script. I have included Regressors as well, but I am not sure whether that makes sense here/is relevant for the scope of this bug.Outputthanks for that. You should use sample_weight=w, though. Using a positional argument is what caused the SGD errors, right?Now all classifiers which predict_proba method (except NuSVC which throws ValueError, and SVC of course) treat such input silently and predict_proba return columns for all classes from dataset.We can remove all ""incorrect"" classes inside BaseSVC's fit method, initialize internal variable classes_ from this fixed dataset, and feed this dataset into underlying implementation. At least now classes_ and predict_proba output will be consistent.predict_proba should return probabilities for all classes in the classes_ attribute, which should be the same as np.unique(y_train)
And I don't think we should raise an error.
This is a valid input, and I don't think these are ""incorrect"" classes. It's just a bug in SVCOk, i'll try to fix this.How about such results?producesConfusing, but at least it looks mathematically correct. And current master already produces same output if you will run this code:Which is the same. Because here we vanish class_weight for second class, instead of doing it through sample_weights.so it is just using ""balanced""?
Why do you say this is correct, though? shouldn't the probability for class 2 be zero?@amueller,No, i forced SVC to use any sample_weights. To obtain such results i've removed this call https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/svm/src/libsvm/svm.cpp#L2342 And all corresponding memory allocation/deallocation. Function call at that line is removing samples from dataset, whose sample_weight is equals to zero. And if it deletes samples of entire class - svm training starts on this 'truncated' dataset without knowledge about all possible classes. But output of this 'fixed' code now is consistent with case when you specify some class_weight=0. And as i found out original libsvm also produces same results.
But with this 'fix' new bug apears - on some small inputs training freezes (maybe infinite loop).Nevermind, i tried to explain such outputs, but i made a mistake in my thoughts.Of course it should, but even original libsvm returns same probability estimates.
If someone wants to test it on original libsvm:dataset.txt:code:It produces in predictions.out:Maybe it's easier to just fix meta estimators, so that they wouldn't pass 0 weight samples into estimators. And throw an error for any such input. Bagging for example can can chose samples for estimator training by choosing weights (default, if estimator supports sample_weights), or subsampling from dataset.Because in case with this bug, if you want to return 0 prob for any 'incorrect' class, you must take into account that SVM classifier also contains bunch of other attributes, like support_, n_support_, dual_coef_, coef_, what values will they have in this case? It will look ugly.I think this should be fixed in libsvm. Do you have an explanation for the libsvm behavior? It seems highly odd to me. I guess it is a combination of how the class weights change the loss combined with the OVR approach. I don't have time to go through the math right now.This seems to be separate from the ""balanced' / ""auto"" issue, though, right?No.Can you point at that issue? I don't know about which you are asking.this one.Ah, sorry, now i understood. Yep, it isn't related to balanced/auto. I've updated code listing in first post.So as i suspected, it's not a bug, i asked about it cjlin1/libsvm#50 (comment)If someone didn't understand what they said there:
alphas of j-th class bounded from above with C value, if C (or values of entire sample_weights vector) of some class is equal to 0 — y.T*alphas = 0 constraint forces other alphas to be 0's. Thus you don't even have any support vectors in solution (because entire alpha vector - 0's)In terms of original optimization problem - C = 0 means that you cannot penalize xi values of entire class, and minimum of problem achieved when you classify any points in dataset with label of that strange class whose C=0 (because xi's could be any positive numbers, C=0 will not penalize them).basically: using class-weights with OVR is not a great idea without calibration. Or is there another conclusion?
That should maybe be mentioned in the docs.
However, the original issue still persists",Bug
https://github.com/yihui-he/channel-pruning/issues/123,"Hello,sorry to disturb you again.Here are the results of my program:
/home/linux/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/externals/joblib/init.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
warnings.warn(msg, category=FutureWarning)
/home/linux/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)
no lighting pack
using CPU caffe
[libprotobuf INFO google/protobuf/io/coded_stream.cc:610] Reading dangerously large protocol message. If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons. To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 553432081temp/bn_vgg.prototxt
using CPU caffe
including last conv layer!
run for 500 batches nFeatsPerBatch 100
Extracting conv1_1 (50000, 64)
Extracting conv1_2_V (50000, 22)
Extracting conv1_2_H (50000, 22)
Extracting conv1_2_P (50000, 59)
Extracting conv2_1_V (50000, 37)
Extracting conv2_1_H (50000, 37)
Extracting conv2_1_P (50000, 118)
Extracting conv2_2_V (50000, 47)
Extracting conv2_2_H (50000, 47)
Extracting conv2_2_P (50000, 119)
Extracting conv3_1_V (50000, 83)
Extracting conv3_1_H (50000, 83)
Extracting conv3_1_P (50000, 226)
Extracting conv3_2_V (50000, 89)
Extracting conv3_2_H (50000, 89)
Extracting conv3_2_P (50000, 243)
Extracting conv3_3_V (50000, 106)
Extracting conv3_3_H (50000, 106)
Extracting conv3_3_P (50000, 256)
Extracting conv4_1_V (50000, 175)
Extracting conv4_1_H (50000, 175)
Extracting conv4_1_P (50000, 482)
Extracting conv4_2_V (50000, 192)
Extracting conv4_2_H (50000, 192)
Extracting conv4_2_P (50000, 457)
Extracting conv4_3_V (50000, 227)
Extracting conv4_3_H (50000, 227)
Extracting conv4_3_P (50000, 512)
Extracting conv5_1_V (50000, 398)
Extracting conv5_1_H (50000, 512)
Extracting conv5_2_V (50000, 390)
Extracting conv5_2_H (50000, 512)
Extracting conv5_3_V (50000, 379)
Extracting conv5_3_H (50000, 512)
Every time I get here,I get stuck.Have you ever had this problem? Is there any solution?Duplicate of #104",None yet
https://github.com/Jamiroquai88/VBDiarization/issues/28,"Onnx file is generated with AMI corpus dataset but the data set available in repository to run the application is different.
Due to which while running the application i am getting this errorreturn self._sess.run(output_names, input_feed, run_options)
onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: 0 for the following indices
index: 2 Got: 121 Expected: 100
Please fix either the inputs or the model.Actually the command in run.sh file is
""python3.7 diarization.py -c ../configs/vbdiar.yml
-l lists/list_spk.scp
--audio-dir wav/fisher-english-p1
--vad-dir vad/fisher-english-p1
--mode diarization
--out-emb-dir embeddings""where as on github link is""python diarization.py -c ../configs/vbdiar.yml -l lists/AMI_dev-eval.scp --audio-dir wav/AMI/IHM_SUM --vad-dir vad/AMI --out-emb-dir emb/AMI/IHM_SUM --in-rttm-dir rttms/AMI""I got same error!pip install onnxruntime==0.3.0, it was worked@haha010508 with onnxruntime==0.3.0 i start getting new error
raise TypeError(message)
TypeError: Expected sequence or array-like, got <class 'NoneType'>ONNX file has nothing with AMI, it is x-vector architecture trained on voxcelebs.
do you still have some issues? I think it looks like you are putting there some unexpected inputs, if you want me to replicate that, you will need to specify more informationI'm facing exactly the same issues as described above. I think the root case is in mismatch between external libraries versions. All dependencies are not binned to any version and even Python version was not provided. I have no clue what is the exact setup to get it working.Python 3.8 result:Downgraded to Python3.7 and onnxruntime==0.3.0:Full pip freeze output:@Jamiroquai88 could you provide pip freeze and Python version when it works for you? It will be really helpful.Thanks!could you please try python3.6?The issue still persists (python3.6):Could you just provide output from both commands python -V and pip freeze? I think it will be the most valuable thing what we can do at this moment.I would expect some differences in spherecluster library, try to use the version I have.Closee due to inactivity.",None yet
https://github.com/vzhou842/profanity-check/issues/18,"I can see the joblib issue has been addressed via #13, but there is no release with it and the pypi still gives me old version, with the error.Uhm some more warnings",None yet
https://github.com/IBCNServices/pyRDF2Vec/issues/6,"Trying to install using python 3.7/3.8 gave me similar but more complex errors. So downgraded to try with python 2.7. Get this single error for scikit_learn (see below).Hi,We do not support Python 2.7 since it has passed its end of life. Could you try installing it with python > 3.5?Also, please try installing all dependencies: python3 -m pip install -r requirements.txtUsing python 3.7.7 I get the output below. I tried installing freetype and png. But did not seem to get that to work. Trying to install the requirements.txt gives me the same error for matplotlib.Hmm it's been a while since I used a Windows machine... Perhaps this is related to pydicom/deid#97 ?Nevertheless, matplotlib is not really a strict dependency, and we only use it visualize a KG (which often is impossible because they are too large).Perhaps you could try cloning the repo and only install the dependencies required for this minimal example (numpy, sklearn, pandas and rdflib):After cloning the repo and installing the packages (numpy, sklearn, pandas and rdflib) I get the following ImportError trying to execute the above piece of code you gave me.You will have to make sure they are installed in your global path. For now, you could just run the example from the rdf2vec/ directory as the converters.py and rdf2vec.py are in there. I will (hopefully) be able to release a better update today.Could you check if you still have issues with a clean install?Works now! Great thank you :-)",bug
https://github.com/IBCNServices/pyRDF2Vec/issues/6,"Trying to install using python 3.7/3.8 gave me similar but more complex errors. So downgraded to try with python 2.7. Get this single error for scikit_learn (see below).Hi,We do not support Python 2.7 since it has passed its end of life. Could you try installing it with python > 3.5?Also, please try installing all dependencies: python3 -m pip install -r requirements.txtUsing python 3.7.7 I get the output below. I tried installing freetype and png. But did not seem to get that to work. Trying to install the requirements.txt gives me the same error for matplotlib.Hmm it's been a while since I used a Windows machine... Perhaps this is related to pydicom/deid#97 ?Nevertheless, matplotlib is not really a strict dependency, and we only use it visualize a KG (which often is impossible because they are too large).Perhaps you could try cloning the repo and only install the dependencies required for this minimal example (numpy, sklearn, pandas and rdflib):After cloning the repo and installing the packages (numpy, sklearn, pandas and rdflib) I get the following ImportError trying to execute the above piece of code you gave me.You will have to make sure they are installed in your global path. For now, you could just run the example from the rdf2vec/ directory as the converters.py and rdf2vec.py are in there. I will (hopefully) be able to release a better update today.Could you check if you still have issues with a clean install?Works now! Great thank you :-)",bug
https://github.com/hackingmaterials/matminer_examples/issues/58,"sorry, but I just pip install the lib, then import failure...
could anyone give me some idea about it?
thanksHey there, thanks for the question.The error actually has to do with the specific version of pymatgen and matminer you're using (this is a well-known problem with matminer <=0.6.2 with newer pymatgen releases). You should install automatminer's requirements file using the requirements file here: https://github.com/hackingmaterials/automatminer/blob/master/requirements.txtIf you still need help, please ask on our help forum: https://matsci.orgThe github issue tracker is for development only.thanks",None yet
https://github.com/scikit-learn/scikit-learn/pull/7985,"Fixes #7975
Fixes #7821
(Duplicate issues)If different samples have different noise-levels(Should be weighted differently), these noise levels should be added in the fit() method. Not in constructor, as it is currently.-Alpha passed to constructor of GaussianProcessRegressor now must be scalar.
-In addition to alpha, sample_alpha argument in fit() is added to the kernel matrix diagonal. None(default),float or array, shape(num_samples,)Quick test:


sample_alpha.zipLooks good so far apart from the deprecation. You also need to change / add tests.So one of the tests failed... What does this mean?It means you used python2 syntax. Please use print with parenthesis.Still failing test, one error I don't know what is and the other seems to be related to execution speed of some scripts(I can't possibly have slowed down fit() that much?)Is sample_alpha the best name? (sample_alphas, sample_noise, something else...)test failure is because of pep8 issues:
https://travis-ci.org/scikit-learn/scikit-learn/jobs/181514957#L488Ok, thanks for the help. First time collaborating on a project on github...@maka89 no worries, thanks for the fix :)Realized that LML function needs sample_alpha added to diagonal of K as well....Quick test again:sample_alpha.ziptest should be in the unit tests, and if you don't trust the current unit tests, you should improve them ;)Ok, so none of the test_gpr.py functions gave any errors... Don't know what the prosedure with test usually are... should I upload the results? I can make a test function of the script I posted above if you think that is useful.as you only moved a parameter, I don't think we need additional tests.Ok, great.Thanks for the PR. I like alpha (or sample_alpha) being moved to fit. However, I think it is rather confusing that we would now have three places where we can specify noise: alpha in __init__(), sample_alpha in fit(), and the WhiteKernel. In particular, adding alpha and sample_alpha in fit() could be confusing to users. I would prefer removing alpha (after deprecation) and using 1e-10 rather than 0 as default for sample_alpha.In case it is helpful, here is the minimal test I proposed in#7975 :I think the last line might need to be modified a little to pass, though. Maybe y_std[0] will be slightly larger than y_std[1]/5.0.I agree removing on removing alpha. I will change the deprecation warning. Is it 0.20 or 0.21 it will be removed ?
Also agree that sample_weights is a bit confusing... Are we sticking with sample_alpha for now?It'll be removed in 0.21I'm not sure about removing alpha. Is the main thing that it'll be redundant with the WhiteKernel? sample_alpha is much harder to specify than alpha and impossible to grid-search.Good point with the grid search...
Yes, putting alpha=0.1 equals WhiteKernel(0.1,""fixed"")...
The optimum solution imo would be if there was a general way to pass arguments to a kernel through fit(). Or add an extra kernelfunction through fit().Then you could feed the noise to, say, a WhiteKernel through fit(). I think this would be good because there might be other kernels that take sample-dependent parameters...For instance I have a GP project on my github where the kernel takes input-noise for each datapoint as parameters. It is hard to implement in the current system.This seems like a bigger project, thoughyeah and not very aligned with the scikit-learn API, I'm afraid. We could attach more properties to samples but that requires #4497The thing with alpha though is that you would not grid-search it. If you use a WhiteKernel as part of your kernel you can optimize the noise level (i.e., alpha) along with all other kernel parameters like length-scales etc. by optimizing the log-marginal-likelihood. The main point of treating the noise level differently than other kernel parameters was that it can be sample-specific. But as noted in the related issues, it is actually much cleaner to specify a sample-specific alpha in fit(). If we change that, I don't see why we should still allow specifying a non-sample-specific noise level in __init__() if that is perfectly possible via the WhiteKernel (except for consistency with Ridge). ""There should be one-- and preferably only one --obvious way to do it."" ;-)I think this would be beyond the scope of this PRWhich simple case do you mean? Having a fixed, non-sample-specific alpha? We could support this also be allowing a scalar value for sample_alpha, which would be used for each sample.Ok, but what should we do in the following situation: someone does not specify alphain _init__() so its default value of 1e-10 is used. When fitting, the user sets sample_alpha to [1e-10, 1e-12]. The implementation in this PR would use alpha + sample_alpha internally, thus effectively [2e-10, 1e-10 + 1e-12] is used. I doubt that this is what the user intended. Changing the default of alpha to 0 would be problematic for numerical reasons. I am not sure how to resolve this situation.A question or observation while the alpha definition of the GP is in discussion in that PR, I was wondering if it makes sense, in case of a multi-dimensional Y, to allow an alpha per Y column ?If alpha is a vector, its size is checked wrt X rows, to allow a noise definition per observations (or a weight), but what about allowing a matrix definition of alpha ?So I am pretty sure that for all the other hyperparameters, you use the same value of the hyperparameter for every column of y. Maybe one could defend allowing for matrix alpha, since alpha is being treated sort of different that the other hyperparameters, but it would not be straight forward to implement, so I don't think it is for this PR? For instance, with matrix alpha, fitting a GP, would require number_of_columns_in_y matrix inversions instead of only one.Well, maybe not in my mind... I am using multiple Y because I am lazy, in the sense that I am building one model to predict multiple outputs that may not be living in the same space...
So, in the current implementation, using multiple outputs is closer to multiple observations of the same Y than multiple Ys living in their own space ?Certainly not something to be done in that PR, as I said, just a question (maybe not the right place). I'll look closer to the implementation and I'll go back to my Rasmussen copy.Is there any progress on this PR? I think we did not came to a conclusion regarding the alpha vs. sample_alpha issue. Based on @GaelVaroquaux judgement (if I get it correctly, correct me if I get you wrong), we should both allow a scalar alpha in __init__ (the easy case) and a vector-valued sample_alpha. sample_alpha should default to None (according to sklearn conventions). We cannot let alpha default to 0 at the same time, because then the defaults would result in numerical instabilities. Thus, we need to keep the default alpha=1e-10. What should we do when sample_alpha is set by the user:I don't mind the third option you mentioned.There is also something to be said for removing alpha in init() altogether imo. Could just add a fixed WhiteKernel corresponding to alpha=1e-10 to default kernel and specify WHY in the documentation? This would corresond to the current behavior if no kernel is specified by the user, though.If the reason for not removing alpha is grid searching, then consider that (1) One does not typically grid search with GPR. (2) If one is to do so, then it makes little sense to grid search only alpha. Grid searching a scalar length_scale in, say, a RBF kernel AND alpha would be kind of similar to grid searching gamma and C/alpha for kernel ridge/SVR. But the kernel length_scale is unavailable to grid search using the standard tools (if I understand the scikit learn grid search correctly, I haven't tried it).When we remove alpha and always add a WhiteKernel, the problem would be that we could not set alpha=0.I agree but we had this discussion already further above.I meant to change the default kernel only (to 1.0*RBF() + WhiteKernel(1e-10,""fixed"") ) and specify why the fixed whitekernel is there in the documentation and recommend adding one. If the user, not having read the documentation, specifies a kernel without a WhiteKernel() added, then he is is free to do so(and probably get numerical problems). Dunno, none of this is optimal, and like I said earlier, I don't mind your solution either.True, changing the default kernel would also work. The problem would be that a user who does not use the default kernel would always have to add a WhiteKernel manually. It would be more explicit but probably also more error-prone because users would have to remember to do so. Plus: it would break backward-compatibility because everyone would have to adapt his kernels. I think this is not an option unfortunately.",Stalled module:gaussian_process
https://github.com/scikit-learn/scikit-learn/pull/17662,"Fixes #12401
Supersedes and closes #12419The plan for deprecating legacy squaring behaviour is summarized in this comment.It looks like adding a FutureWarning caused other TSNE tests to fail. I'm going to add @ignore_warnings(category=FutureWarning) to the relevant tests, as per the Contributing docs: ""The warning should be caught in all other tests (using e.g., @pytest.mark.filterwarnings), and there should be no warning in the examples.""A related question: should I be running my local tests to fail on Warnings, then? I might have not been using the right pytest flags, as I didn't catch this before opening a PR. (EDIT: Found the relevant pytest docs. I needed to be adding -W error::FutureWarning to my local testing.)Pinging @flosincapite. I think this is ready to go from [WIP] -> [MRG]. :)Looks great! There are some comments, but only because I was extremely picky :)Thanks a ton for taking the time to review, @flosincapite! :DThanks much for the reviews @jeremiedbb. :)LGTM, thanks @joshuacwnewton !thanks @joshuacwnewton, made a first pass, mostly looks good but I'm not quite sure about the deprecation planthanks @joshuacwnewton , some last nitpicks and one questionpre-approving since the remaining comments are easy to address. Thanks @joshuacwnewton !Thanks much for taking the time to review, @NicolasHug! :DThanks you @joshuacwnewton",module:manifold
https://github.com/DistrictDataLabs/yellowbrick/issues/1001,,priority: high type: technical debt
https://github.com/broadinstitute/getzlab-SignatureAnalyzer/issues/14,"Hi,While trying to run the latest commit and the latest PIP version, I kept getting missing key errors:(please excuse the X server errors - I don't know why it's complaining about not having X forwarding but it doesn't seem to impact anything.)I traced it down to this line:It looks like the ""chr"" prefix gets appended to the MAF but not the reference 2bit chromosome names. I was able to successfully run by removing this line and an identical one later in the file.I'm happy to make a PR to remove these lines, but I assume they were put there for a purpose. Maybe because they need to be coerced to strings and numpy is trying to interpret them as ints?Generally I try to avoid enforcing prefixes with references/VCFs/MAFs because it's easy to get mismatches like this. Every center has their own way of resolving them, and in enforcing one way or the other I have usually created more problems than I started with.I assume this would also lead to the MAF or spectra output having the ""chr"" prefix, or are they chomped off before final output?Hello,get_spectra_from_maf should now check the hg build file for the ""chr"" prefix and only attach it if necessary.",enhancement
https://github.com/broadinstitute/getzlab-SignatureAnalyzer/issues/18,"CMD: signatureanalyzer processed_latest_1015.maf -n 10 --cosmic cosmic2 --objective poissonHi, I have a header named Tumor_Sample_Barcode in my maf file and it returns an error like this, how to fix it ?
Is there an example maf file provided to test the command and help me to format the input maf file ?
thanksHere is the docs on the required columns, please check this:
https://github.com/broadinstitute/getzlab-SignatureAnalyzer/blob/master/docs/generating_mutational_spectra.mdAnd here is an example maf:
https://github.com/broadinstitute/getzlab-SignatureAnalyzer/blob/master/examples/example_luad_maf.tsvBest,ShankaraOkay, thanks for your reply.",None yet
https://github.com/scikit-learn/scikit-learn/pull/15050,"Resolves #14953Gets the dtypes in _fit and checks only when it has ""categories"".The fun part begins when we try to get the encoders to respect pandas categories (when they are numerical) :)@jnothman @amueller This would be good in the next release if we want to support pd.Categorical in the encoders in 0.24.Otherwise lgtmPlease add what's newMaybe it's worth solving for LabelEncoder too (see #12086, #13351)I agree with #12086 (comment)For OneHotEncoder and OrdinalEncoder, we can have a category=dtype'` option to allow users to use the categorical dtype.The LabelEncoder would be slightly more harder to do, since it is used internally in some estimators. If we were to warn in in those situations, there is nothing a user can do.If we want to resolve both issues, we may need a sklearn.set_config(use_pandas_categories_order) to configure it globally.So is the goal of this overall to decrease number of versions needed to change the behavior for category=""auto"" from the current one to that of categories=""dtype"" for categorical data by 1 version?If I understand correctly, the user is still getting the OHE they wanted with some categories in the OneHotEncoder estimator. Ideally these should match those of the categorical dtype. Currently they sometimes don't, but at the same time I'm not sure there was too much expectation that they would. Getting a warning one can do nothing about is not necessarily very helpful. Personally I currently have a pipeline with categorical dtypes + OHE and I would probably just ignore this warning, if it was raised. Or am I missing something?Yes this warning is not helpful without the other PR on categories=""dtypes"" (#15396)Should that be the error message in this case? ""Please set categories as a list to set the order of your categories explicitly?""Consider the case, when you have a dataset split with train_test_split, with a significant number of categories. Some categories might then differ between train and test, and one would use OneHotEncoder(handle_unknown=""ignore"") to handle unkown categories in train. To silence this warning the user would then need not only to sort categories, but also to remove unknown categories from the categorical dtype in train set. I'm just saying, I would not spend time doing that, I prefer having train and test use the same categories in dtype in my present project, and seeing repeated warning for this is annoying.Users only need to be told once that the categorical dtype is not respected (not each time they fit a pipeline) and maybe the documentation could be a better place to document this?This warning is going to be raised, currently with this PR I believe? Sorting categories is not going to help...I think that #14984, #15050, and #15396 might not be blockers for 0.22 and I would move them for 0.23.I think that it could be great to have a single issue (superseded #14953, #14954) to discuss the overall behaviour for categories in OneHotEncoder and OrdinalEncoder and from there having several PRs which follows the discussed proposals.LGTM, I guess this needs a what's new entryI'm quite strongly against merging this until we discuss it more. The discussion is tracked in #14953 (comment)Also Joel's approval is from a previous version, not the current PR statusMarking as requesting changes so this doesn't get accidently mergedI am happy with moving this to 0.23.I vote +1 because I agree with Thomas's comment #14953 (comment) (i.e., users won't get unexpected warning), perhaps I'm wrong.
Let's move this to 0.23.I don't think we should rely on some arbitrary pandas internal behavior that may change tomorrowarbitrary pandas internal behavior? If so, I agree that we should reconsider this PR. I admit that I lack experience outside scikit-learn.Actually I come up with another question: should we raise warning in OneHotEncoder where the order doesn't matter?I'm referring to the fact that pandas happens to use the lexicographic ordering by default. If we start relying on this, we're making pandas a de-facto dependency.I have the same concern, again the discussion is tracked in #14953 (comment)I'll trust you but note that this behavior is actually documented.
See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Categorical.htmlSo will pandas change it without a deprecation cycle? I don't know.So let my clarify my point: since this feature is designed for pandas, I guess it's reasonable to rely on some documented behaviors in pandas.removing from the milestone. We can put it back when we find a way forward.I still would rather have this than not, at least in OrdinalEncoder, and maybe in OneHotEncoder when the dtype is ordered.Updated to only warn in OrdinalEncoder",Waiting for Reviewer module:preprocessing
https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis/issues/13,,None
https://github.com/scikit-learn/scikit-learn/pull/9290,,None
https://github.com/gregversteeg/corex_topic/pull/39,,None
https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis/issues/13,,None
https://github.com/fairlearn/fairlearn-proposals/pull/11,,None
https://github.com/dask/distributed/issues/4005,"What happened:Tests fail immediately when the system does not have Internet access:What you expected to happen:I expected at least subset of tests to be usable in network-constrained environments.Anything else we need to know?:It seems that replacing has_ipv6() with explicit False helps it get past initial error. However, the majority of tests still fail because of network failures, e.g.:Even if I skip tests that fail immediately, a lot of tests error out during teardown:Environment:@jacobtomlinson I'm not sure what your priorities are like these days. But would this be easy for you to resolve?Looking through the log some of these issues appear to be related to IPv6 rather than a lack of internet connection.@mgorny could you share a little more about your network setup, what interfaces you have and what IP addresses any active interfaces have when you ran these tests?@mrocklin fixed some issues with importing distributed without a network here: #3991. Perhaps a similar solution of try/execpt with defaults to 127.0.0.1 would also work hereI'm running tests inside network namespace, with only lo interface set up, i.e. roughly:The hostname is also reset to localhost.",None yet
https://github.com/timothydmorton/VESPA/issues/29,"I have a folder that contains three files: ""star.ini"", ""fpp.ini"" and the phase-folded light curve. I then runheller$ starfit --all .and MultiNest (v3.10) seems to be running just fine:====>
Sampling finished. Exiting MultiNest
(...)
triple starfit successful for . in 7.1 minutes.
<====Then I runheller$ calcfppwhich ends with a RuntimeError:====>
/usr/local/lib/python3.7/site-packages/numpy/lib/npyio.py:2242: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.
output = genfromtxt(fname, **kwargs)
WARNING:root:sklearn not available
WARNING:root:progressbar not imported
WARNING:root:Cadence not provided in fpp.ini; defaulting to Kepler cadence.
Cadence not provided in fpp.ini; defaulting to Kepler cadence.
WARNING:root:If this is not a Kepler target, please set cadence (in days).
If this is not a Kepler target, please set cadence (in days).
WARNING:root:Transit band not provided in fpp.ini; defaulting to ""Kepler.""
Transit band not provided in fpp.ini; defaulting to ""Kepler.""
WARNING:root:If this is not a Kepler target, please specify filter.
If this is not a Kepler target, please specify filter.
'No object named /obs/df in the file'
ERROR:root:FPP calculation failed for ..
Traceback (most recent call last):
File ""/usr/local/lib/python3.7/site-packages/vespa/fpp.py"", line 236, in from_ini
single_starmodel = StarModel.load_hdf(single_starmodel_file)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 1256, in load_hdf
obs = ObservationTree.load_hdf(filename, path+'/obs', ic=ic)
File ""/usr/local/lib/python3.7/site-packages/isochrones/observation.py"", line 869, in load_hdf
samples = store[path+'/df']
File ""/usr/local/lib/python3.7/site-packages/pandas/io/pytables.py"", line 504, in getitem
return self.get(key)
File ""/usr/local/lib/python3.7/site-packages/pandas/io/pytables.py"", line 694, in get
raise KeyError('No object named {key} in the file'.format(key=key))
KeyError: 'No object named /obs/df in the file'During handling of the above exception, another exception occurred:Traceback (most recent call last):
File ""/usr/local/bin/calcfpp"", line 151, in
n=args.n)
File ""/usr/local/lib/python3.7/site-packages/vespa/fpp.py"", line 242, in from_ini
'Please run starfit --all {}.'.format(folder))
RuntimeError: Cannot load StarModels. Please run starfit --all ..
FPP calculation failed for ..
Traceback (most recent call last):
File ""/usr/local/lib/python3.7/site-packages/vespa/fpp.py"", line 236, in from_ini
single_starmodel = StarModel.load_hdf(single_starmodel_file)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 1256, in load_hdf
obs = ObservationTree.load_hdf(filename, path+'/obs', ic=ic)
File ""/usr/local/lib/python3.7/site-packages/isochrones/observation.py"", line 869, in load_hdf
samples = store[path+'/df']
File ""/usr/local/lib/python3.7/site-packages/pandas/io/pytables.py"", line 504, in getitem
return self.get(key)
File ""/usr/local/lib/python3.7/site-packages/pandas/io/pytables.py"", line 694, in get
raise KeyError('No object named {key} in the file'.format(key=key))
KeyError: 'No object named /obs/df in the file'During handling of the above exception, another exception occurred:Traceback (most recent call last):
File ""/usr/local/bin/calcfpp"", line 151, in
n=args.n)
File ""/usr/local/lib/python3.7/site-packages/vespa/fpp.py"", line 242, in from_ini
'Please run starfit --all {}.'.format(folder))
RuntimeError: Cannot load StarModels. Please run starfit --all ..
<====So now I'm supposed to tun starfit --all . again although I've just done that before. Anyways, if I do thatheller$ starfit --all .now gives an error message====>
1 of 1: .
Fitting .
single starfit...
mist_starmodel_single.h5 exists. Use -o to overwrite.
single starfit successful for . in 0.4 minutes.
binary starfit...
mist_starmodel_binary.h5 exists. Use -o to overwrite.
binary starfit calculation failed for ..
Traceback (most recent call last):
File ""/usr/local/lib/python3.7/site-packages/isochrones/starfit.py"", line 127, in starfit
fig1,fig2 = mod.corner_plots(corner_base)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 1070, in corner_plots
fig1, fig2 = self.corner_physical(**kwargs), self.corner_observed(**kwargs)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 1696, in corner_physical
return self.corner_derived(self.physical_quantities)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 1691, in corner_derived
fig = corner.corner(self.derived_samples[cols], labels=cols, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 2934, in getitem
raise_missing=True)
File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1354, in _convert_to_indexer
return self._get_listlike_indexer(obj, axis, **kwargs)[1]
File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1161, in _get_listlike_indexer
raise_missing=raise_missing)
File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1252, in _validate_read_indexer
raise KeyError(""{} not in index"".format(not_found))
KeyError: ""['Teff', 'mass', 'radius', 'logg'] not in index""
triple starfit...
mist_starmodel_triple.h5 exists. Use -o to overwrite.
triple starfit calculation failed for ..
Traceback (most recent call last):
File ""/usr/local/lib/python3.7/site-packages/isochrones/starfit.py"", line 127, in starfit
fig1,fig2 = mod.corner_plots(corner_base)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 1070, in corner_plots
fig1, fig2 = self.corner_physical(**kwargs), self.corner_observed(**kwargs)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 1696, in corner_physical
return self.corner_derived(self.physical_quantities)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 1691, in corner_derived
fig = corner.corner(self.derived_samples[cols], labels=cols, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 2934, in getitem
raise_missing=True)
File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1354, in _convert_to_indexer
return self._get_listlike_indexer(obj, axis, **kwargs)[1]
File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1161, in _get_listlike_indexer
raise_missing=raise_missing)
File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1252, in _validate_read_indexer
raise KeyError(""{} not in index"".format(not_found))
KeyError: ""['Teff', 'mass', 'radius', 'logg'] not in index""
<===I know that the problem can't be in the files that I'm providing because I've successfully run vespa on them on another machine.Any help would be appreciated.Hi Morten! I installed everything yesterday, andpip3 listgives===>
isochrones 2.0.1
VESPA 0.6
<===So following your comment, it seems to me I'm currently using incompatible versions of vespa and isochrones. Do you see any workaround how I can get vespa running now?I now did things as per your suggestion. I first did
pip3 uninstall isochrones
then deleted ~/.isochrones, and finally installed isochrones v1.2.2 via
pip install isochrones==1.2.2.Then I deleted all output files that I had previously generated for my star.ini, fpp.ini and CSV light curve files and started from scratch using
starfit --all .
This seems to have gone smoothly===>
(...)
Total Likelihood Evaluations: 988862
Sampling finished. Exiting MultiNest
triple starfit successful for . in 13.5 minutes.
<===But when I run calfpp, I receive a new error message that I'm not able to handle:heller$ calcfpp===>
/usr/local/lib/python3.7/site-packages/numpy/lib/npyio.py:2242: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.
output = genfromtxt(fname, **kwargs)
WARNING:root:sklearn not available
WARNING:root:progressbar not imported
WARNING:root:Cadence not provided in fpp.ini; defaulting to Kepler cadence.
Cadence not provided in fpp.ini; defaulting to Kepler cadence.
WARNING:root:If this is not a Kepler target, please set cadence (in days).
If this is not a Kepler target, please set cadence (in days).
WARNING:root:Transit band not provided in fpp.ini; defaulting to ""Kepler.""
Transit band not provided in fpp.ini; defaulting to ""Kepler.""
WARNING:root:If this is not a Kepler target, please specify filter.
If this is not a Kepler target, please specify filter.
INFO:root:Fitting transitsignal with MCMC...
Fitting transitsignal with MCMC...
INFO:root:Fitting data to trapezoid shape with MCMC for EPIC201497682_3....
Fitting data to trapezoid shape with MCMC for EPIC201497682_3....
ERROR:root:FPP calculation failed for ..
Traceback (most recent call last):
File ""/usr/local/bin/calcfpp"", line 151, in
n=args.n)
File ""/usr/local/lib/python3.7/site-packages/vespa/fpp.py"", line 260, in from_ini
trsig.MCMC()
File ""/usr/local/lib/python3.7/site-packages/vespa/transitsignal.py"", line 300, in MCMC
self.Ts_acor = integrated_time(Ts)
File ""/usr/local/lib/python3.7/site-packages/emcee/autocorr.py"", line 88, in integrated_time
f = function(x, axis=axis, fast=fast)
File ""/usr/local/lib/python3.7/site-packages/emcee/autocorr.py"", line 41, in function
acf = np.fft.ifft(f * np.conjugate(f), axis=axis)[m].real
FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use arr[tuple(seq)] instead of arr[seq]. In the future this will be interpreted as an array index, arr[np.array(seq)], which will result either in an error or a different result.
FPP calculation failed for ..
Traceback (most recent call last):
File ""/usr/local/bin/calcfpp"", line 151, in
n=args.n)
File ""/usr/local/lib/python3.7/site-packages/vespa/fpp.py"", line 260, in from_ini
trsig.MCMC()
File ""/usr/local/lib/python3.7/site-packages/vespa/transitsignal.py"", line 300, in MCMC
self.Ts_acor = integrated_time(Ts)
File ""/usr/local/lib/python3.7/site-packages/emcee/autocorr.py"", line 88, in integrated_time
f = function(x, axis=axis, fast=fast)
File ""/usr/local/lib/python3.7/site-packages/emcee/autocorr.py"", line 41, in function
acf = np.fft.ifft(f * np.conjugate(f), axis=axis)[m].real
FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use arr[tuple(seq)] instead of arr[seq]. In the future this will be interpreted as an array index, arr[np.array(seq)], which will result either in an error or a different result.
<===Any help will be very much appreciated.I have emcee 2.2.1 and emcee3 3.0.0.dev0heller$ pip3 list===>
emcee 2.2.1
emcee3 3.0.0.dev0
<===I uninstalled emcee and reinstalled emcee, making sure I have the latest version. But that's still/again emcee 2.2.1.How can I tell calcfpp to use emcee3 instead of emcee?This also didn't help. I installed emcee 3.0rc2 from the github master (https://emcee.readthedocs.io/en/latest/user/install), then deleted all previous output files from vespa and tried it again. The first step seems to have worked without problems:heller$ starfit --all .
===>
1 of 1: .
Fitting .
single starfit...
(...)
ln(ev)= -47.953257352215843 +/- 0.16984685405821168
Total Likelihood Evaluations: 879009
Sampling finished. Exiting MultiNest
triple starfit successful for . in 11.7 minutes.
<===But then this happens:heller$ calcfpp===>
/usr/local/lib/python3.7/site-packages/numpy/lib/npyio.py:2242: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.
output = genfromtxt(fname, **kwargs)
WARNING:root:sklearn not available
WARNING:root:progressbar not imported
WARNING:root:Cadence not provided in fpp.ini; defaulting to Kepler cadence.
Cadence not provided in fpp.ini; defaulting to Kepler cadence.
WARNING:root:If this is not a Kepler target, please set cadence (in days).
If this is not a Kepler target, please set cadence (in days).
WARNING:root:Transit band not provided in fpp.ini; defaulting to ""Kepler.""
Transit band not provided in fpp.ini; defaulting to ""Kepler.""
WARNING:root:If this is not a Kepler target, please specify filter.
If this is not a Kepler target, please specify filter.
INFO:root:Fitting transitsignal with MCMC...
Fitting transitsignal with MCMC...
INFO:root:Fitting data to trapezoid shape with MCMC for EPIC201497682_3....
Fitting data to trapezoid shape with MCMC for EPIC201497682_3....
INFO:root:Generating ['beb', 'heb', 'eb', 'beb_Px2', 'heb_Px2', 'eb_Px2', 'pl'] models for PopulationSet...
Generating ['beb', 'heb', 'eb', 'beb_Px2', 'heb_Px2', 'eb_Px2', 'pl'] models for PopulationSet...
ERROR:root:Error generating HEB population.
Error generating HEB population.
ERROR:root:FPP calculation failed for ..
Traceback (most recent call last):
File ""/usr/local/bin/calcfpp"", line 151, in
n=args.n)
File ""/usr/local/lib/python3.7/site-packages/vespa/fpp.py"", line 296, in from_ini
savefile=popset_file, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/vespa/populations.py"", line 2067, in init
do_only=do_only)
File ""/usr/local/lib/python3.7/site-packages/vespa/populations.py"", line 2113, in generate
MAfn=MAfn, n=n, **heb_kws)
File ""/usr/local/lib/python3.7/site-packages/vespa/populations.py"", line 1558, in init
**kwargs)
File ""/usr/local/lib/python3.7/site-packages/vespa/populations.py"", line 1577, in generate
n=2n)
File ""/usr/local/lib/python3.7/site-packages/vespa/stars/populations.py"", line 1779, in init
period=period, ecc=ecc, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/vespa/stars/populations.py"", line 1850, in generate
ecc_short=ecc, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/vespa/stars/populations.py"", line 1526, in init
ecclong=ecc_long, eccshort=ecc_short)
File ""/usr/local/lib/python3.7/site-packages/vespa/orbits/populations.py"", line 108, in init
obsx=obsx_long,obsy=obsy_long,obsz=obsz_long)
File ""/usr/local/lib/python3.7/site-packages/vespa/orbits/populations.py"", line 334, in init
self.obspos = random_spherepos(n)
File ""/usr/local/lib/python3.7/site-packages/vespa/orbits/utils.py"", line 50, in random_spherepos
c = SkyCoord(phis,thetas,1,representation='physicsspherical')
File ""/usr/local/lib/python3.7/site-packages/astropy/coordinates/sky_coordinate.py"", line 250, in init
frame_cls, frame_kwargs = _get_frame_without_data(args, kwargs)
File ""/usr/local/lib/python3.7/site-packages/astropy/coordinates/sky_coordinate_parsers.py"", line 201, in _get_frame_without_data
_normalize_representation_type(kwargs)
File ""/usr/local/lib/python3.7/site-packages/astropy/coordinates/baseframe.py"", line 141, in _normalize_representation_type
_representation_deprecation()
File ""/usr/local/lib/python3.7/site-packages/astropy/coordinates/baseframe.py"", line 128, in _representation_deprecation
'favor of representation_type', AstropyDeprecationWarning)
astropy.utils.exceptions.AstropyDeprecationWarning: The representation keyword/property name is deprecated in favor of representation_type
FPP calculation failed for ..
Traceback (most recent call last):
File ""/usr/local/bin/calcfpp"", line 151, in
n=args.n)
File ""/usr/local/lib/python3.7/site-packages/vespa/fpp.py"", line 296, in from_ini
savefile=popset_file, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/vespa/populations.py"", line 2067, in init
do_only=do_only)
File ""/usr/local/lib/python3.7/site-packages/vespa/populations.py"", line 2113, in generate
MAfn=MAfn, n=n, **heb_kws)
File ""/usr/local/lib/python3.7/site-packages/vespa/populations.py"", line 1558, in init
**kwargs)
File ""/usr/local/lib/python3.7/site-packages/vespa/populations.py"", line 1577, in generate
n=2n)
File ""/usr/local/lib/python3.7/site-packages/vespa/stars/populations.py"", line 1779, in init
period=period, ecc=ecc, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/vespa/stars/populations.py"", line 1850, in generate
ecc_short=ecc, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/vespa/stars/populations.py"", line 1526, in init
ecclong=ecc_long, eccshort=ecc_short)
File ""/usr/local/lib/python3.7/site-packages/vespa/orbits/populations.py"", line 108, in init
obsx=obsx_long,obsy=obsy_long,obsz=obsz_long)
File ""/usr/local/lib/python3.7/site-packages/vespa/orbits/populations.py"", line 334, in init
self.obspos = random_spherepos(n)
File ""/usr/local/lib/python3.7/site-packages/vespa/orbits/utils.py"", line 50, in random_spherepos
c = SkyCoord(phis,thetas,1,representation='physicsspherical')
File ""/usr/local/lib/python3.7/site-packages/astropy/coordinates/sky_coordinate.py"", line 250, in init
frame_cls, frame_kwargs = _get_frame_without_data(args, kwargs)
File ""/usr/local/lib/python3.7/site-packages/astropy/coordinates/sky_coordinate_parsers.py"", line 201, in _get_frame_without_data
_normalize_representation_type(kwargs)
File ""/usr/local/lib/python3.7/site-packages/astropy/coordinates/baseframe.py"", line 141, in _normalize_representation_type
_representation_deprecation()
File ""/usr/local/lib/python3.7/site-packages/astropy/coordinates/baseframe.py"", line 128, in _representation_deprecation
'favor of representation_type', AstropyDeprecationWarning)
astropy.utils.exceptions.AstropyDeprecationWarning: The representation keyword/property name is deprecated in favor of representation_type
<===I met the same problem. Vespa is not (yet) compatible with some latest packages.pip3 install astropy==1.3 pandas==0.19.2 emcee==3.0rc2 could make it work.I triedpip3 install astropy==1.3 pandas==0.19.2 emcee==3.0rc2which led to the following problem.Failed to build astropy pandas
vespa 0.6 has requirement pandas>=0.21, but you'll have pandas 0.19.2 which is incompatible.I was wrong. I made it work using below packages.astropy (1.3)
batman-package (2.4.6)
isochrones (1.2.2)
numpy (1.17.0)
pandas (0.21.0)
VESPA (0.6)I met another problem about ''mixed type'' that pandas cant recognize data tpye correctly when isochrone package read HDF. The reason is some values of [Fe/H] are nan or something that are recognized as 'object'. Then tpyes, float and object, exits in the same column.If you met the same problem, change the file (isochrone package /mist/grid.py) may be helpful.I can't downgrade to pandas 0.21.pip3 install pandas==0.21gives an error:Command ""/usr/local/opt/python/bin/python3.7 /usr/local/lib/python3.7/site-packages/pip install --ignore-installed --no-user --prefix /private/tmp/pip-build-env-macelzw7/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- wheel setuptools Cython ""numpy==1.9.3; python_version=='3.5'"" ""numpy==1.12.1; python_version=='3.6'"" ""numpy==1.13.1; python_version>='3.7'"""" failed with error code 1 in NoneSo I'm stuck with pandas 0.24.2. Then when I executestarfit --all .in my test folder with the fpp.ini, star.ini, and lightcurve.csv files, I receiveTraceback (most recent call last):
File ""/usr/local/lib/python3.7/site-packages/isochrones/starfit.py"", line 104, in starfit
mod.fit(verbose=verbose, overwrite=overwrite, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 556, in fit
return self.fit_multinest(**kwargs)
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 692, in fit_multinest
self._make_samples()
File ""/usr/local/lib/python3.7/site-packages/isochrones/starmodel.py"", line 865, in _make_samples
chain = np.loadtxt(filename)
File ""/usr/local/lib/python3.7/site-packages/numpy/lib/npyio.py"", line 1146, in loadtxt
for x in read_data(_loadtxt_chunksize):
File ""/usr/local/lib/python3.7/site-packages/numpy/lib/npyio.py"", line 1074, in read_data
items = [conv(val) for (conv, val) in zip(converters, vals)]
File ""/usr/local/lib/python3.7/site-packages/numpy/lib/npyio.py"", line 1074, in <listcomp>
items = [conv(val) for (conv, val) in zip(converters, vals)]
File ""/usr/local/lib/python3.7/site-packages/numpy/lib/npyio.py"", line 781, in floatconv
return float(x)
ValueError: could not convert string to float: '0.694466137561774976-309'Try pip3 uninstall pandas before installing pandas==0.21.I didn' t meet this problem. Check the values in the files of your test folder maybe of some help to you. Find which file includes or which function produces the '0.694466137561774976-309' is important.The test case I used as below:https://github.com/timothydmorton/vespa-tutorial/tree/master/Workshop-2018/TestCase1",None yet
https://github.com/tslearn-team/tslearn/pull/211,"This PR fixes the deprecation warnings that are raised when importing certain (now private) modules from sklearn.Many things will move to a private API in the new sklearn version. Their module name will change and have a leading underscore. e.g. sklearn.neighbors.base becomes sklearn.neighbors._base. Unfortunately, these new module names will cause a crash in environments with older sklearn versions.The proposed fix is the following for all deprecation warninings:There seems to be smth wrong with my repository, as many of these changes have already been made in a previous PR (the sax-mindist one).Hello @GillesVandewiele! Thanks for updating this PR. We checked the lines you've touched for PEP 8 issues, and found:The change in check_is_fitted does not seem to be backward compatible with Python 2.7 (which still expects 2 arguments there).Just one note about this PR: I am doing my best on the PRs to be merged in 0.3.1, but my agenda is very busy these days, and I am very likely to postpone my reaction to these to later times (eg. when my kids are back to school :p ). Please do not take that lack of reaction personally :)check_is_fitted still allows for the attributes parameter, it's just that the default behavior (None) is to check for attributes with a trailing underscore. I would keep private attributes private (starting with an underscore) because we don't want the users to access them if they are meant to be private.The attributes parameter is mandatory for scikit-learn < 0.22, and version 0.20.X was the last version of scikit-learn to support Python 2.7.Maybe #194 is also related and could be considered in the same PR?#194 is definitely related as well, in addition, we should patch the sklearn.dataset.generator functions such as make_blobs to make timeseries datasets.So that's a bunch of sklearn refactors that could all be handled in this PR!I would keep private attributes private, as we don't want the users to access them!@GillesVandewiele and @johannfaouzi : you seem to disagree about check_is_fitted's second argument. @GillesVandewiele , where have you seen that it's being deprecated? If it's just None by default, then I would suggest we keep our calls as they are for the moment.For the private API, I do not see a better hack than what you suggest @GillesVandewiele , so I would be OK with it, unless someone has a better idea.Then, there is #194 which is harder I guess (I am involved in a discussion scikit-learn/scikit-learn#14057 about it btw), and maybe longer term.@GillesVandewiele : do you get a warning if you keep the check_is_fitted calls with an attributes parameter?No (it used to be the case on 0.22.dev0 but they changed it shortly before the release of 0.22.0) and it is backward compatible (i.e. if you specify the attributes parameter, it will only look at the given attributes).Indeed, I seemed to be wrong! I will revert those changes later.@GillesVandewiele maybe, in the end, the easiest thing to do would be to start a new PR that just uses your try/except trick to import the kNN mixin from sklearn, no?@GillesVandewiele do you think you should do more than just fix the import for v0.4?By the way, here is the list of deprecation warnings from sklearn that the last Azure Pipelines run gets:EDIT: the KNeighborsMixin is in practice never used I think (no call to super from KNeighborsTimeSeriesMixin, and method kneighbors is overloaded), so we could probably just remove the import and inheritance.Hi Romain,Tomorrow I shall fix all import deprecation warnings and then we could merge this for v.0.4. We could discuss other potential deprecation issues in another PR afterwards then (just to finish this one up quickly).I tried fixing all the deprecation warnings related to imports and private function usage. It was not a lot of work, so sorry that it took so long!PS. I might have somehow broken the pep-8 checker. It raises a warning & points to a line that is no longer there.Continue to review full report at Codecov.Great ! Could you just add as comments which is the most recent import and the version for which the old one stops working ?Like this?Great !",None yet
https://github.com/microsoft/hummingbird/pull/87,"Closes #64:All CLA requirements met.Continue to review full report at Codecov.@interesaaat getting the value of Alpha is what's left.@NicolasHug I checked your comment which references using the _baseline_prediction attribute of the HistGradientBoostingClassifier model for doing this.
I noticed that it switches between being a <class 'numpy.ndarray'> and <class 'numpy.float64'> during different iterations, so could you elaborate on that?Right my bad, it's not always a float. In multiclass problems it's a numpy array of size n_classes because we build n_classes trees at each iteration. The decision function for a given class corresponds to the sum of all the predictions of the trees of that class, and the predicted probability to belong to a class is a softmax of these sums. It is the same idea as in multinomial logistic regression, where we get n_classes weight vectors.Since we build n_classes set of trees, we need to compute one initial prediction per class.@NicolasHug OK great, thanks a lot for this info!@interesaaat I'm still not sure about how we are going to map the _baseline_prediction to alpha and also how init_ is being mapped to alpha is still vague for me. For example, why do we have to take the natural logarithm here?Looks good. I haven't test it yet tho. I left a couple of comments. We will need to fix the coverage, but let's finish the coding first.I believe that something like alpha = np.array([operator.raw_operator._baseline_prediction.flatten().tolist()]) should work. The init_ part is not clear to me either. @NicolasHug is the init_ here used as in GradientBoosting? In that case we can just do a check as treelite (as pointed by Nicolas here) and open as issue if the value is different than zero. In this way we can have the operator in and figure out later this corner case.Short versionYes that should work (modulo shape conversion). As far as I understand, reading the existing code in convert_sklearn_gbdt_classifier, ALPHA corresponds to the initial prediction of the boosting process, which is precisely _baseline_prediciton.Long versionit is used in the GradientBoostingXXX estimators but not in the HistGradientBoostingXXX estimator.In the GradientBoostingXXX estimators, we allow users to pass an init parameter which is an arbitrary estimator object that specifies the very first prediction, i.e. the value at which predictions are initialized when starting the boosting process. (Kinda like the value you would use to initialize weights when doing gradient descent)In the HistGBDT code, there is no init parameter and we just hardcode it. The specific value isn't super important, but it corresponds to the mean of the target in regression (with LS loss), and to the log-odds ratio in binary classification, because that's the ""best"" we can do w.r.t. the loss while only looking at the targets.Predicting the log-odds ratio is also the default when init=None in the GradientBoostingXXX estimators, though in this case a dummy estimator is used. That dummy estimator always predicts a constant prediction which is the prior of the positive class, stored as its class_prior_[1] attribute.The current code for the GradientBoostingClassifier is correct: when init=None, ALPHA is set to the log-odds ratio to replicate the behavior of the estimator
(nice job to whoever wrote this BTW, this isn't obvious!)This is for the GradientBoostingClassifier. For the HistGradientBoosting estimators, there is no dummy-estimator, and the log-odds ratio is just in _baseline_prediction. Basically _baseline_prediction corresponds to np.log(operator.raw_operator.init_.class_prior_[1] / (1 - operator.raw_operator.init_.class_prior_[1]), so you just have to replace that@scnakandala is the genius behind everything :)Thanks for the detailed explanation @NicolasHug! Perhaps renaming Alpha to base_prediction or base_score (as in XGBoost) will be a bit more clear. WDYT?Looks good! 2 asks:Will merge after these are fixed (plus the reshape).Hi @interesaaat,
I've added the test by @ksaur. Unfortunately, it fails on 9 out of the 11 tests.
I'll push the changes so that you can have a look on the failing output.I see. Apparently the probabilities are off for some reason. Were the tests passing on your side before commit 0f82328?Yeah I tried before merging, they failed as well.Looking at the test results, the predicted probability is off by a high value (many a times even the first decimal digit is different while the expected decimal precision in test is 6). That's why all the 9 tests that assert on predicted probabilities fail.Did they ever pass locally for you @ahmedkrmn? We can backtrack using that info. Also, is the predicted class correct?Also, interesaaat / ksaur, how do you people generally debug these issues?Well, we start with regression, 1 tree and see why the results don't match. Then we do regression multiple trees -> binary -> multiclass. In the worst case we check the partial results within Sklearn execution and where they start to differ wrt Hummingbird.I can take a look at this.@interesaaat any new updates?No sorry I have been working in the ONNX PR. I am done with it so I will look into this next.Hey @ahmedkrmn I fixed the problem with the conversion. Basically, learning rate and normalization for hist_gbdt are already computed upfront and we don't need to do them on our side.2 questions:Great Matteo, thanks for the update!
I'll use a separate convert_sklearn_hist_gbdt_classifier function for dealing with histGBDT.Regarding moving the code of _tree_commons.py to gbdt.py, I can see that get_parameters_for_sklearn_common() function is used indecision_tree.py, so is moving it a good idea?
Or what you mean is that we create a separate get_parameters_for_sklearn_common() for histGBDT and put it in gbdt.py and leave the original function as is in the _tree_commons.py?Sorry, I only meant the code related to hist_gbdtm like from like 183 in gbdt. The other side of the condition is ok if stays in _tree_commons.pyHi @interesaaat,
Does test_sklearn_histgbdt_converters.py pass on your local machine after your additions?
It's still failing on mine.Yes it is passing all tests in my machine. Are you getting the same failures or something different? Like, all tests are still failing?@interesaaat When running the tests locally, sometimes one test fails as in the case with this last failing build. If you rerun the tests again, all of them pass, so that's a strange behavior!Can you please point me to the failing case? Is it the multiclass? This is probably due to rounding errors and we can fix it momentarily by increasing the tolerance.I ran the tests 20 times. It's always either that they all pass or 9/10 pass.Summary:
10/20 all passed.
02/20 failed at test_HistGBDT_classifier_binary_converter.
02/20 failed at test_HistGBDT_perf_tree_trav_multi_classifier_converter.
02/20 failed test_HistGBDT_gemm_classifier_converter.
01/20 failed at test_HistGBDT_tree_trav_multi_classifier_converter.
01/20 failed at test_HistGBDT_shifted_labels_converter.I can take a look. Can you please rebase with master? After my pass we can then merge this PR!Done!Thanks a lot, @interesaaat,
This was really an interesting PR to work on!",None yet
https://github.com/scikit-learn/scikit-learn/pull/2468,"This PR adds a simple meta-estimator which accepts any generative model (normal approximation, GMM, KernelDensity, etc.) and uses it to construct a generative Bayesian classifier.Todo:I think to make the discussion more fruitful it would be great to provides some examples on datasets where such models are actually useful either from a pure classification performance point of view, or more likely as samplers to generate new labeled samples for specific classes (a bit like you did with this KDE sampling example for digits).Ah, I hadn't even thought of that possibility! Yes, we could implement a sample routine, which would use the underlying models to return a random set of new observations fitting the training data. Great idea!I'll work on some examples soon to make the utility of this approach more clear.I added doc strings and tests. An incompatibility came up in the case of GMM: I opened an issue at #2473.I don't think this should be combined with Naive Bayes, except in the docs. The charm of Naive Bayes lies in its speed and simple code, no need to mess with that.On second thought: @jakevdp, is it too much of a stretch to merge this thing into the naive_bayes module? I guess it's not really ""naive"" in the NB sense, but it would remove some clutter in the top-level module. Also, take a look at the NB narrative docs, which explain pretty much the same thing that you're explaining in the module docstring.I initially thought about putting this within sklearn.naive_bayes (given that it inherits from BaseNB!) but didn't because, though it is Bayesian, it's distinctly not Naive in the sense that the term is used. If we could start over, it would be make more sense to have a submodule for generative classification of which naive bayes is a part, rather than the other way around. But given that we've made the API choice to have a naive_bayes submodule, I thought it would be less confusing to put general generative classification in its own module.Regardless of where the code goes, I had envisioned combining the narrative documentation for the two: as you mention, we can adapt the theoretical background currently put under the heading of Naive Bayes and show how it applies in both the Naive and the general case.True, but I've seen other people's production codebases that depend on MultinomialNB being in naive_bayes.py, and I'd have some explaining to do if we broke that :pCombining the narratives was the main what I was aiming at. It's your call to decide if it fits well enough to also combine the code.(FYI, I see you're using BaseNB. I've been thinking about killing that, because MultinomialNB and BernoulliNB can be implemented more straightforwardly as pure linear models, sharing no code with GaussianNB.)@larsmans - I ended up following your advice and moving everything into the naive_bayes submodule. That location might be a bit misleading, but I think it is cleaner.Still some tests failing... I'm going to try to fix those.I think this is pretty close to finished now. I added narrative documentation, examples, and the tests should pass.One missing feature that would be really helpful would be the ability to do class-wise cross-validation of the density estimators within GenerativeBayes. I'm not sure what the right interface would be for that, however... any ideas?Hmm... is there any way that program state can affect the results of cross_val_score? It fails here, but passes on my machine, and passes when I run the code alone. There doesn't seem to be any random element that would affect it... that's really strange.Ah - looks like it was something that had changed in master. I'll adjust the tests so that they will pass.Coverage remained the same when pulling 061f3fb on jakevdp:generative_class into ffde690 on scikit-learn:master.Changing status to MRG: I think this is ready for a final review, unless we want to add class-wise cross-validation at this time.Thanks @ogrisel. I've addressed all your comments.Regarding the CV issue: I think the first-order solution is to simply expose the estimator parameters using the get_params machinery in BaseEstimator. We can internally label the estimators, e.g. ""est1"", ""est2"", so that the fit parameters would become est1__paramname, est2__paramname, etc. This would be a quick addition, and allow the usual cross-validation tools to have access to the parameters.Coverage remained the same when pulling 3f8666a on jakevdp:generative_class into aa8139b on scikit-learn:master.I am not sure that will work as the number of sub-estimators is dependent on the number of classes . The list of subestimators in the estimators_ attribute is therefore only generated once we see the data in fit so as to be able to extract the number of classes or features from the data shape. On the other hand the grid search tooling manipulates the model and its parameters independently of the data, in particular prior to any call to fit. Hence we have a design mismatch. Maybe it would be possible to hack get/set_params to store the subestimators parameters on the GenerativeBayes object itself and delay the call to the recursive call set_params method on the sub-estimators objects at fit time.yes, I ran into that mismatch when I gave this strategy a shot. I'll think about your idea of hacking get/set_params, but I'm starting to think that just providing a CV tool within GenerativeBayes itself might be the answer.That might indeed be a better way. Note however that we have a similar issue for multi-class or multi-label classifiers that implement the OvR strategy by combining n_classes binary classifiers. It is possible that having per-classifier hyperparameter tuning (e.g. regularizer strength) would be beneficial for the overall performance of the model. @mblondel @pprett might want to pitch-in.I don't have any experience with tuning each binary classifier separately. One concern I have is that each binary classifier may produce predictions with different scales (e.g. one with predictions in [-1, 1], another one with predictions in [-5, 5]) and thus the argmax rule might not work at all.In any case, this is a combinatorial search and thus randomized search seems the way to go.Hey guys, I hope I'm not just wasting space in your inbox. I've tried to follow this discussion, but wanted to provide a couple notes from a user. I have utilized GMM classifiers in the past. I've also started playing with this commit to see the results using a GMM. One big feature needed for this function is the capability of tuning the number of components, n_components, for each class. I saw Jake was concerned with some features users would be interested in having, this is a biggie for people who use this type of classifier. It definitely impacts performance. Unfortunately I cannot provide you an example of a dataset (company policy).Thanks @jgbos - I agree that individually tuning hyperparameters is a vital feature of this. I'm still trying to figure out the best way to approach that, though (and I haven't had much time to work on this lately)Is there any chance that there would be some progress on this PR, or is it buried forever? I understand that we are hung up on the last TODO item. I'm wondering if we can come to a solution that does not require the ability to do class-wise cross validation for the density model?really cool examples :)@jakevdp you'll need to rebase@jakevdp just wondering, will you merge this anytime soon?@danielravina I am not sure @jakevdp has time to finish this. Please take over if you want and see my comments.Probably will not be finishing this myself. The main reason I never finished the PR is that I never really figured out how to deal cleanly with per-class hyperparameters.@danielravina @jakevdp did either of you or anyone else end up picking this back up? would be interested in working on this if not.This PR is actually fairly similar to the BayesClassifier / NaiveBayes classifiers in pomegranate (see tutorial here: https://github.com/jmschrei/pomegranate/blob/master/tutorials/Tutorial_5_Bayes_Classifiers.ipynb). If you pick this up I'd be happy to review it, but be sure to read the above discussion thoroughly to understand what the stalling issues were.Should this be moved to scikit-learn-extras or is it not complete enough?Hi, I am facing the same problem with a few estimators I wrote that also require per-class hyperparameters, i.e., lists/arrays/tuples. The way I see it there are two things you can do:",Move to scikit-learn-extra module:naive_bayes
https://github.com/scikit-learn/scikit-learn/pull/15166,Towards #9250ping @adrinjalali @glemaitre @thomasjpfan for a hopefully easy review :),None yet
https://github.com/scikit-learn/scikit-learn/pull/17702,"Fix #17673In base class use private _n_classes and for classifier create public attribute n_classes_.I think that there is a cleaner solution which might require a bit more refactoring.So since _validate_y is different for the classifier and regressor, it does not make sense to have it in the base class. We can make an abstractmethod such that it needs to be implemented by the inherited class. Then, we need to change the regression loss, we should not specify any n_classes because there are no classes in regression. I think that almost all the tests should pass with this patch. I will checkWe have an additional change for max_features where we can check is_classifier(self) instead of n_classes_We need to update the test of the loss function where we should not specify the parameter n_classes in the regression loss.Let's go for the next stepIt starts to look good. We will need an entry in whats_new/v_0_24.rst in the ensemble section to announce the deprecationWhere is this file? I cannot find it.It's doc/whats_new/v_0_24.rst.If you use vs code, type ""ctrl-p"" followed by ""24"" to navigate to it quickly.Thank you for the PR @simonamaggio !LGTM. @thomasjpfan are all changes fine with you?Thank you, this PR is almost ready!Done it now. Sorry I didn't get it the first time.Minor comment.Otherwise LGTMI solve the conflict and make the small change requested by @thomasjpfan
I will merge once the CIs are happy.Thanks @simonamaggio",module:ensemble
https://github.com/scikit-learn/scikit-learn/pull/11589,"This PR addresses #8438 (stalled PR which aimed at fixing #4225).The changes in this PR aim at allowing GridSearchCV and cross_val_score to accept sparse y. This can be particularly useful in some extreme learning situations where one has a lot of samples and many labels. This is only useful as long as GridSearchCV is passed an estimator which also accepts a sparse y. Future PRs such as #4354 are needed to add support for sparse y in Scikit-Learn estimators.Details:@GaelVaroquaux, @jnothmanLGTM aside from the comments.Ping @jnothman on this.It seems to me that you are still getting a deprecation warning.@GaelVaroquaux Yes, the Travis job failed because of DeprecationWarning: The default of the iid parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal. in GridSearchCV.@jnothman can you, please, review(/comment on) this PR ?Aside from a small comment, this look good to me.You should also add an entry to whats_newPing @jnothman : I think that you should have a look at this.@GaelVaroquaux I pushed the fixes and updated the what's new file.Sorry for the delay here, and thanks for your work.@jnothman Yes, OneVsRestClassifier already supports sparse y.@jbschiratti are you available to wrap this up?@adrinjalali Yes, sure ! I'll finish this. Thank you for reminding me about this :-)@jbschiratti tests failing.@adrinjalali Yes, sorry for the previous commit. I forgot to remove some >>> from a past conflict.@adrinjalali The two tests failing do not seem to be related to my PR...@jnothman For what it's worth, pytest sklearn/neural_network/tests/test_mlp.py does not fail on my computer (Ubuntu 18.04.2 LTS 64bit, conda 4.6.8 (Anaconda, version 2018.12), Python 3.5.6).This looks like it's in pretty good shape, but I'm unable to review fully now.Please add an |Enhancement| entry to the change log at doc/whats_new/v0.21.rst. Like the other entries there, please reference this pull request with :issue: and credit yourself (and other contributors if applicable) with :user:@jnothman Thank you for your comments. I took them into account. Also note that sklearn/neural_network/tests/test_mlp.py did not fail this time... (it is probably linked to #13585).I will admit that I'd not initially expected to support anything here but multilabel sparse matrices. I think it's okay to be more ambitious, with the caveats below, but I don't think we're ready to support other sparse targets in downstream estimators.Treating a sparse column vector as binary creates - to be pedantic - a backwards incompatibility in type_of_target and code that uses it and can handle sparse y. There may not be many of these, but I'm not sure that we have any need/desire to support sparse column vectors.So. The priority for sparse y support should be multilabel. Binary and multiclass column vectors technically create backwards compatibility issues, and we can take or leave their support. Multioutput we should do if it comes easily.@jnothman I agree that the typical use case of a sparse y is multi-label problems (in which one has a lot of possible labels and only a few ""active"" labels per sample). In such cases, splitter classes such as KFold or GroupKFold will work with work with GridSearchCV.Still, it would be good to have a real classifier (such as RidgeClassifier?) which accepts a sparse y. What do you think? Should it be addressed in another PR?what's the status of this?I think this depends on whether something like MultiOutputClassifier densifies each column of y to pass it to the underlying estimator.I mean if there's already a multilablel estimator that will work efficiently with a lot of classes (MLP??) then we could consider modifying it to handle sparse y.But I'd be very happy to see this merged as a minimal change that would enable downstream implementations.Hi @jbschiratti, thanks for your patience: if you are still interested in working on this PR do you mind resolving conflicts? Otherwise I will label this as stalled. Thanks for your collaboration.@cmarmo Sure, I'm still willing to work on it. I'll solve the conflicts.",Waiting for Reviewer module:model_selection module:utils
https://github.com/xingyizhou/CenterTrack/issues/64,"/CenterTrack/src/lib/utils/tracker.py"", line 2, in
from sklearn.utils.linear_assignment_ import linear_assignment
ModuleNotFoundError: No module named 'sklearn.utils.linear_assignment_'/CenterTrack/src$ python
Python 3.6.10 (default, Dec 19 2019, 23:04:32)
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.pip list
Package Version Locationattrs 19.3.0
backcall 0.1.0
bleach 3.1.5
cachetools 4.1.0
cycler 0.10.0
Cython 0.29.17
DCNv2 0.1 /media/iot-2/namvh/CenterTrack/src/lib/model/networks/DCNv2
decorator 4.4.2
defusedxml 0.6.0
descartes 1.1.0
dill 0.2.7.1
easydict 1.9
entrypoints 0.3
fire 0.3.1
flake8 3.8.1
flake8-import-order 0.18.1
future 0.18.2
importlib-metadata 1.6.0
ipykernel 5.2.1
ipython 7.14.0
ipython-genutils 0.2.0
ipywidgets 7.5.1
jedi 0.17.0
Jinja2 2.11.2
joblib 0.14.1
jsonschema 3.2.0
jupyter 1.0.0
jupyter-client 6.1.3
jupyter-console 6.1.0
jupyter-core 4.6.3
kiwisolver 1.2.0
llvmlite 0.32.1
MarkupSafe 1.1.1
matplotlib 3.2.1
mccabe 0.6.1
mistune 0.8.4
more-itertools 8.2.0
motmetrics 1.1.3
nbconvert 5.6.1
nbformat 5.0.6
notebook 6.0.3
numba 0.49.1
numpy 1.18.4
nuscenes-devkit 1.0.8
opencv-python 4.2.0.34
packaging 20.3
pandas 1.0.3
pandocfilters 1.4.2
parso 0.7.0
patsy 0.5.1
pexpect 4.8.0
pickleshare 0.7.5
Pillow 6.2.1
pip 20.1
pluggy 0.13.1
progress 1.5
prometheus-client 0.7.1
prompt-toolkit 3.0.5
ptyprocess 0.6.0
py 1.8.1
py-cpuinfo 5.0.0
pycodestyle 2.6.0
pyflakes 2.2.0
PyFunctional 1.3.0
Pygments 2.6.1
pyparsing 2.4.7
pyquaternion 0.9.5
pyrsistent 0.16.0
pytest 5.4.2
pytest-benchmark 3.2.3
python-dateutil 2.8.1
pytz 2020.1
PyYAML 5.3.1
pyzmq 19.0.1
qtconsole 4.7.3
QtPy 1.9.0
scikit-learn 0.23.0
scipy 1.4.1
seaborn 0.10.1
Send2Trash 1.5.0
setuptools 40.6.2
Shapely 1.7.0
six 1.14.0
sklearn 0.0
statsmodels 0.11.1
tabulate 0.8.7
termcolor 1.1.0
terminado 0.8.3
testpath 0.4.4
threadpoolctl 2.0.0
torch 1.5.0
torchvision 0.6.0
tornado 6.0.4
tqdm 4.46.0
traitlets 4.3.3
wcwidth 0.1.9
webencodings 0.5.1
wheel 0.34.2
widgetsnbextension 3.5.1
xmltodict 0.12.0
zipp 3.1.0warnings.warn(
""The linear_assignment_ module is deprecated in 0.21 ""
""and will be removed from 0.23. Use ""
""scipy.optimize.linear_sum_assignment instead."",
FutureWarning)
I tried to use scipy.optimize.linear_sum_assignment instead but faced errors at first try. Later downgraded to scikit-learn==0.21.0.Thank you very much!",None yet
https://github.com/rapidsai/cuml/issues/2632,,None
https://github.com/amueller/introduction_to_ml_with_python/issues/113,,None
https://github.com/nestauk/nesta/issues/244,,None
https://github.com/scikit-learn/scikit-learn/pull/17849,,None
https://github.com/mne-tools/mne-python/issues/7860,,None
https://github.com/nielstron/quantulum3/issues/149,,None
https://github.com/scikit-learn/scikit-learn/pull/12069,,None
https://github.com/SMTorg/smt/issues/194,,None
https://github.com/BayesWitnesses/m2cgen/pull/200,,None
https://github.com/vzhou842/profanity-check/pull/13,,None
https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/449,,None
https://github.com/sphinx-gallery/sphinx-gallery/issues/717,,None
https://github.com/cisco/mindmeld/pull/157,,None
https://github.com/keras-team/keras-contrib/issues/512,,None
https://github.com/itdxer/neupy/issues/258,,None
https://github.com/DistrictDataLabs/yellowbrick/issues/903,,None
https://github.com/tensorflow/probability/issues/355,,None
https://github.com/automl/auto-sklearn/issues/844,,None
https://github.com/WojciechMigda/PyTsetlini/issues/4,,None
https://github.com/Avik-Jain/100-Days-Of-ML-Code/pull/73,,None
https://github.com/scikit-learn/scikit-learn/issues/17392,,None
https://github.com/SauceCat/pydqc/issues/4,,None
https://github.com/tensorflow/tensorflow/issues/28846,,None
https://github.com/r9y9/wavenet_vocoder/issues/192,,None
https://github.com/scikit-learn/scikit-learn/pull/17199,,None
https://github.com/lyp-deeplearning/deep_sort_yolov3/issues/1,,None
https://github.com/deepset-ai/FARM/issues/485,,None
https://github.com/civisanalytics/civis-python/issues/374,,None
https://github.com/scikit-learn/scikit-learn/pull/17679,,None
https://github.com/junyanz/interactive-deep-colorization/issues/10,,None
https://github.com/bioFAM/MOFA2/issues/41,,None
https://github.com/scikit-learn/scikit-learn/issues/17209,,None
https://github.com/scikit-learn/scikit-learn/pull/15730,,None
https://github.com/PyThaiNLP/pythainlp/issues/330,,None
https://github.com/nubank/fklearn/issues/56,,None
https://github.com/gboeing/osmnx/issues/263,"#### Problem description (what did you do, what did you expect to happen, and what actually ####happened)Hello everybody,
I am trying to run the code as per Github example:Unfortunately I got the following error:AttributeError Traceback (most recent call last)
in ()
----> 1 gdf = ox.footprints_from_place(place='Emeryville, California, USA')AttributeError: module 'osmnx' has no attribute 'footprints_from_place'I also checked similar issues with module footprints_from_place, but I was pretty much unable to find a viable solution.
Should I upgrade OSMNX to version 0.9?
Please provide a valid path solution. Thanks in advance. Andrea#### What operating system, architecture, Python version, and OSMnx version are you using?
I am using a machine of 64 bits, running Windows 10PRO version 1803, Python 3.6.3, OSMNX 0.8.2#### Complete list of your environment's packages and their versions (for example, run conda list or ##pip list then paste the output below, between the two ""details"" tags)@Renton2017 yes, per the change log, the buildings module was deprecated in favor of a new footprints module introduced in v0.9. The buildings module will be fully removed in v0.10.If you're using OSMnx<0.9, then use ox.buildings_from_place. If you're using OSMnx>=0.9, then use ox.footprints_from_place. See the documentation for more usage details.Hi @gboeing , many thanks for the clarifications,
I will use the code suggestions, you sent me. Sorry if I do not have read carefully. Regards.
AndreaIt is really helpful for me! Thanks a lot.Hi,
I am trying to run this code on Jupyter on mac which I did it before, but I don't know why it does an error like this:AttributeError: module 'osmnx' has no attribute 'distance'and the code itself is:incr = 0
same_count = 0
for ind, row in nearestNodes_under2000.iterrows():
u, v, key = ox.get_nearest_edge(Graph, (row['Point_X'],row['Point_Y']))
nn = min((u, v), key=lambda n: ox.distance.great_circle_vec(row['Point_X'],row['Point_Y'], Graph.nodes[n]['y'], Graph.nodes[n]['x']))could you please help me what can I do to solve it.@Mehrdadbabazadeh17 this isn't related to this issue (deprecation of buildings module). Please open a new issue and completely fill out the new issue template and we can take a look.",question
https://github.com/scikit-learn/scikit-learn/pull/17665,"towards: #3020In linear_models such as Lasso there is an option to select normalize=True. However, if fit_intercept is set to False this won't have any effect.
Towards depreciating normalize in linear_models altogether we want to give a user an option to first normalize using StandardScaler and then call the linear_model.This tests make sure that the two options will give the same results and the same .coef_ (event though .intercept_ might differ)
The tests are done for both the sparse and the dense datasetsSo we are converging :POnly some style thing. You might want to merge master into your branch to discard the error with circle ciping @rth You should know about this issue as well. If you want to have a look to merge the PR once the changes will be done.Thanks for working on this @maikia !Also could you please merge master in to fix documentation CI?I think the tests look good (only one change). @agramfort @rth if you can have a look at the intercept and potentially merge the PR, it could be great.And codecov is reporting bullshit :)LGTMlet's deprecate the normalize param now.@glemaitre or @rth feel free to mergeThanks @maikia now you are responsible for all the tests in _coordinate_descent.py :P
Let's go for the deprecation of normalize now.",module:linear_model
https://github.com/brentp/peddy/issues/31,"Hi,When I try to run peddy, I always got warnings like the following.
""WARNING:
24 samples in vcf not in ped:""
Does this matter? The results looks fine.Thanks very much!that means your VCF has samples that are not in the ped file. can you post the full output?and the output of zgrep -m1 CHROM $vcf ?Sure, here are the full output of the warnings
""WARNING:
41 samples in vcf not in ped:
Br5190,Br5481,Br5009,Br5310,Br5216,Br5006,Br5391,Br5392,Br2520,Br2469,Br5156,Br2355,Br2379,Br5263,Br5207,Br5249,Br5347,Br2272,Br524$WARNING:
1 samples in ped not in vcf:
nan^[[1;31mped_check^[[0m
plotting
ran in 16.6 seconds
^[[1;31mhet_check^[[0m
loaded and subsetted thousand-genomes genotypes (shape: (2504, 4604)) in 0.5 seconds
/ysm-gpfs/home/sl857/anaconda2/lib/python2.7/site-packages/sklearn/utils/deprecation.py:52: DeprecationWarning: Class RandomizedPCA$
warnings.warn(msg, category=DeprecationWarning)
ran randomized PCA on thousand-genomes samples at 4604 sites in 0.8 seconds
Projected thousand-genomes genotypes and sample genotypes and predicted ancestry via SVM in 0.0 seconds
/ysm-gpfs/home/sl857/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.
warnings.warn(""Mean of empty slice."", RuntimeWarning)
/ysm-gpfs/home/sl857/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.py:82: RuntimeWarning: Degrees of freedom <= 0 for s$
warnings.warn(""Degrees of freedom <= 0 for slice"", RuntimeWarning)
ran in 9.9 seconds""and here is the output of zgrep -m1 CHROM $vcf
""#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Br2355 Br2379 Br2452 Br2469 Br2520 Br2294 Br5347 Br2309 Br2078 Br5374 Br2272 Br5321 Br5422 Br5391 Br5392 Br2071 Br5288 Br5006 Br5009 Br5010 Br5190 Br5207 Br5156 Br5243 Br5281 Br5310 Br5336 Br5481 Br5216 Br5230 Br5365 Br5437 Br5252 Br5263 Br5225 Br5249 Br2297 Br2328 Br2364 Br2477 Br2368""I also checked the IDs in both ped and vcf and they seemed to match each other.Thanks!This is odd.
There must be somethign odd about your ped file. Can you post it here or email it to me?
what is the output of cut -f 1 $ped ?The ped file is very large. Actually I got the ped file by converting the plink bim, fam and bed file format.ok. the ped file for peddy only needs the first 6 columns. Seems like you might have an empty row (maybe at the end?)ok, I will check the rows and columns. Thank you very much!I used the first 6 columns in ped file and the warnings disappeared. Thanks very much!Hi,
I am getting the following warnings when trying to run peddy on exome vcf file (unfiltered):
(Python 2.7.6)2018-09-17 16:01:16 mdrtc3611 peddy.cli[9666] INFO Running Peddy version 0.4.2
2018-09-17 16:01:16 mdrtc3611 peddy.cli[9666] WARNING 12 samples in vcf not in ped:
37675,40604,42667,43147,43146,39001,42666,40887,39002,40888,37676,426682018-09-17 16:01:16 mdrtc3611 peddy.cli[9666] WARNING 1 samples in ped not in vcf:
nan2018-09-17 16:01:16 mdrtc3611 peddy.cli[9666] INFO ped_check
Traceback (most recent call last):
File ""/usr/local/bin/peddy"", line 11, in
load_entry_point('peddy', 'console_scripts', 'peddy')()
File ""/usr/local/lib/python2.7/dist-packages/click/core.py"", line 722, in call
return self.main(*args, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/click/core.py"", line 697, in main
rv = self.invoke(ctx)
File ""/usr/local/lib/python2.7/dist-packages/click/core.py"", line 895, in invoke
return ctx.invoke(self.callback, **ctx.params)
File ""/usr/local/lib/python2.7/dist-packages/click/core.py"", line 535, in invoke
return callback(*args, **kwargs)
File ""/home/athinat/bin/peddy/peddy/cli.py"", line 207, in peddy
in (""ped_check"", ""het_check"", ""sex_check"")]):
File ""/home/athinat/bin/peddy/peddy/cli.py"", line 43, in run
prefix=prefix, **kwargs)
File ""/home/athinat/bin/peddy/peddy/peddy.py"", line 966, in ped_check
d = cyvcf2.par_relatedness(vcf_str,
AttributeError: 'module' object has no attribute 'par_relatedness'the output for zgrep -m1 CHROM $vcf is:
#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT 37675 37676 39001 39002 40604 40887 40888 42666 42667 42668 43146 43147and for cut -f 1 $ped is:1 39001 0 42666 2 -9
1 39002 0 42666 2 -9
1 42666 0 0 2 -9
2 40887 42668 42667 1 -9
2 40888 42668 42667 1 -9
2 42668 0 0 1 -9
2 42667 0 0 2 -9
3 40604 43147 43146 1 -9
3 43147 0 0 1 -9
3 43146 0 0 2 -9
4 37676 0 0 1 -9
4 37675 0 0 2 -9Is it by any chance obvious what I am doing wrong? the ped is space separated not tab
Thanks
Athinayou should use tabs. and make sure you don't have any empty lines at the end of the file.is it possible that you have a really old version of cyvcf2? you could update that as well.Thank you for the immediate reply.
I changed spaces for tabs that worked for the initial warnings but when trying to test with setup for cyvcf2 I get the following:running test
Searching for nose
Best match: nose 1.3.7
Processing nose-1.3.7-py2.7.eggUsing /home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg
running egg_info
writing requirements to cyvcf2.egg-info/requires.txt
writing cyvcf2.egg-info/PKG-INFO
writing top-level names to cyvcf2.egg-info/top_level.txt
writing dependency_links to cyvcf2.egg-info/dependency_links.txt
writing entry points to cyvcf2.egg-info/entry_points.txt
reading manifest file 'cyvcf2.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
writing manifest file 'cyvcf2.egg-info/SOURCES.txt'
running build_ext
skipping 'cyvcf2/cyvcf2.c' Cython extension (up-to-date)
/home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg/nose/config.py:264: RuntimeWarning: Option 'with-coverage' in config file 'setup.cfg' ignored: excluded by runtime environment
warn(msg, RuntimeWarning)
Failure: ImportError (/home/athinat/bin/cyvcf2-master/cyvcf2/cyvcf2.so: undefined symbol: EVP_sha1) ... ERRORTraceback (most recent call last):
File ""/home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg/nose/loader.py"", line 418, in loadTestsFromName
addr.filename, addr.module)
File ""/home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg/nose/importer.py"", line 47, in importFromPath
return self.importFromDir(dir_path, fqname)
File ""/home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg/nose/importer.py"", line 94, in importFromDir
mod = load_module(part_fqname, fh, filename, desc)
File ""/home/athinat/bin/cyvcf2-master/cyvcf2/init.py"", line 1, in
from .cyvcf2 import (VCF, Variant, Writer, r_ as r_unphased, par_relatedness,
ImportError: /home/athinat/bin/cyvcf2-master/cyvcf2/cyvcf2.so: undefined symbol: EVP_sha1Ran 1 test in 0.006sFAILED (errors=1)
root@mdrtc3611:/home/athinat/bin/cyvcf2-master# sudo python setup.py test
running test
Searching for nose
Best match: nose 1.3.7
Processing nose-1.3.7-py2.7.eggUsing /home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg
running egg_info
writing requirements to cyvcf2.egg-info/requires.txt
writing cyvcf2.egg-info/PKG-INFO
writing top-level names to cyvcf2.egg-info/top_level.txt
writing dependency_links to cyvcf2.egg-info/dependency_links.txt
writing entry points to cyvcf2.egg-info/entry_points.txt
reading manifest file 'cyvcf2.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
writing manifest file 'cyvcf2.egg-info/SOURCES.txt'
running build_ext
skipping 'cyvcf2/cyvcf2.c' Cython extension (up-to-date)
/home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg/nose/config.py:264: RuntimeWarning: Option 'with-coverage' in config file 'setup.cfg' ignored: excluded by runtime environment
warn(msg, RuntimeWarning)
Failure: ImportError (/home/athinat/bin/cyvcf2-master/cyvcf2/cyvcf2.so: undefined symbol: EVP_sha1) ... ERRORTraceback (most recent call last):
File ""/home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg/nose/loader.py"", line 418, in loadTestsFromName
addr.filename, addr.module)
File ""/home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg/nose/importer.py"", line 47, in importFromPath
return self.importFromDir(dir_path, fqname)
File ""/home/athinat/bin/cyvcf2-master/.eggs/nose-1.3.7-py2.7.egg/nose/importer.py"", line 94, in importFromDir
mod = load_module(part_fqname, fh, filename, desc)
File ""/home/athinat/bin/cyvcf2-master/cyvcf2/init.py"", line 1, in
from .cyvcf2 import (VCF, Variant, Writer, r_ as r_unphased, par_relatedness,
ImportError: /home/athinat/bin/cyvcf2-master/cyvcf2/cyvcf2.so: undefined symbol: EVP_sha1Ran 1 test in 0.006sFAILED (errors=1)Any feedback on that?can you install the cyvcf2 from bioconda? otherwise, you'll need libcrypto and libcurl and likely liblzmaBioconda did the job!
Thank you Brent",None yet
https://github.com/scikit-learn/scikit-learn/issues/6231,"Moving the discussion with @amueller from pydata/patsy#77 (comment).Proposing to:Related to this - perhaps also allow make_scorer's need_threshold = True to be used for non-binary classification problems. Not sure why it's limited to binary. Totally meaningful for multi-class situations, where the metric might use the decision function to evaluate based on the rank order or something (for example, SVC doesn't have a cheap predict_proba, but has a cheap decision_function, so it would allow it to be used with rank-based metris). From make_scorer perspective, it just seems like a completely arbitrary restriction because it shouldn't care about what the custom loss function does with the decision_function output.Responding to that second dot point only, make_scorer will automatically
pass on any additional kwargs.On 26 January 2016 at 15:32, pkch notifications@github.com wrote:@jnothman - I don't see how it would work. I meant that the when the classifier finishes training, its classes_ attribute should be passed to the custom loss function. This would help avoid calling LabelEncoder inside the custom loss function (which is required to match the unlabeled np.ndarray returned by predict_proba with the labels in the true y).make_scorer can only pass additional kwargs to the custom loss function if these arguments are known at the time make_scorer is called. But make_scorer is called to create a scorer, which is to be used on arbitrary future datasets, with arbitrary future classifiers. The class labels won't be known until the classifier is trained on a particular dataset.On another thought, if sklearn were to return DataFrame instead of np.ndarray this would be unnecessary. But I doubt it's in the plans?+1 on adding labels to all metrics.And I agree this is an issue with the current design of the scorers. I think the classifiers should actually pass the classes_ to the scorer. There is even more issues with this, if this happens in cross-validation, for example, as different folds might have different classes, so we might want to tell all classifiers what the classes are (but that proposal was rejected a while ago because it really clutters the API).I'm fine with a fix on the scorer level, though. The scorers could actually inspect the classes_ attribute and act accordingly. This is somewhat of a big API change though. I'd be happy to see a test and PRs though.This relates closely to how we compute the R^2 by the way, but that is harder to fix. The R^2 should use the training set mean, not the test set mean, but that's unknown to the scorer. Getting classes_ in the call from scorer is easy, but the R^2 computation is hard, as the model doesn't actually store the class mean :-(Ideally I'd fix both issues in one go, but I'm not sure how to do that.I feel this is pretty important, as currently results can be pretty wrong if not all classes are present in a test set.On second thought, I'm not sure what the right way to handle this is, though. Should we warn or error if macro-average is requested and not all classes are present?I'd be interested in seeing scorers access classes_ to pass that on to
capable metrics...On 8 October 2016 at 05:30, Andreas Mueller notifications@github.com
wrote:Re my comment at #9144 that scorer objects should look at the set of classes before cv, we should also consider how this might work with nested CV...Hm so we make labels a required parameter (with deprecation)?and do we want to store the labels in the scorer actually? we can also just make it a required argument of the __call__ and always provide it in GridSearchCV and cross_val_score etc.In general this problem also happens for the score method of GridSearchCV, where we don't have labels... but we stored them in fit... but these where the training labels. But I guess that's the best we can do?If we don't pass the labels along with X and y, then doing it for the built-in ones and the make_scorer ones would be quite different.The same issue arises in the models that support partial fit if a batch of data happens to not have a certain label. Allowing the user to specify the classes more generally might be worth pursuing. Potentially an optional parameter?@jmschrei how so? the first pass to partial_fit is required to specify all the classes.@jnothman so we add an extra constructor argument to all classifiers?
We said ""no"" to that a while ago, but I guess we can change that. I actually agree with your assessment.
So we add an optional constructor parameter to all classifiers?
I'm happy to do that (or rather, have @aarshayj do it ;). But I would like buy-in from @ogrisel @agramfort @GaelVaroquaux if possible before embarking on this.Explicit is better than implicit. I guess this applies in this case. +1 for a PR that adds classes as an optional hyperparam to all classifiers and a common test that looks like this:See #9290 for a draft of the API.Is adding classes to all classifier inits still on the table? If yes, I could work on it.There's actually a pretty good step towards this in #9585. There's also a PR that adds classes everywhere #9532 but that's indeed hairy.",API help wanted
https://github.com/scikit-learn/scikit-learn/issues/17208,"With the latest changes, KMeans is significantly slower on small datasets. The time needed to compute clusters is around ten times longer.Times with the following code are:
scikit-lern 0.22: ~0.015
scikit-learn 0.23: ~0.15I also tried on a bigger dataset with shape (300, 25) where clustering with the new version needed 3-4s while before it happened in miliseconds.Clusters would be computed as fast as before.Thanks for the report. Can you tell how many cores you have ?
Can you try setting OMP_NUM_THREADS=1 as an env var before launching python ? (I'm not proposing to do it permanently it's just to check if I'm on the right path)Thank you for fast reaction. I have 4 cores.
I also tried to set OMP_NUM_THREADS=1 and nothing changed regarding the speed - it was same slow than before.I can reproduce the slowdown on my laptop and the changes I made in #17210 solve the issue for me. We are going to merge it and it will be available it the nightly builds. Here are the instructions to install it: https://scikit-learn.org/stable/developers/advanced_installation.html#installing-nightly-builds. We'd be very interested if you could check if it fixes your issue (you'd have to wait a day after we merge it)I checked out your PR and installed the package with pip install -e .. It slightly improves the speed but it still much slower than before than me. Did I make anything wrong?Now times are:
scikit-lern 0.22: ~0.015 s
scikit-learn 0.23: ~0.15 s
scikit-learn #17210: ~0.11 sI did cleaner measurements and indeed the proposed fix is still ~3x slower. Profiling showed that it's a new helper which takes 90% of the execution time. It's called at each iteration. I'll make a pr to move it outside of the iteration loop.Out of curiosity, is performance critical for you for problems that take ~10ms to run ?@jeremiedbb thank you. That would be great.We use scikit as the Orange dependency (Orange is a graphical tool for data analysis) and on the other problem (data with size (300, 25)) clustering which took milliseconds before (0.1 - 0.5s) takes few seconds now. Most of Orange's users have data with smaller sizes and for them, it is quite a difference. I mean it is not the most critical issue, but if it is possible we would prefer that things would be computed faster.I opened #17235. It's better but still not as fast as scikit-learn 0.22.EDIT: the conclusions are wrong, please ignore.The reason is the overhead of the deprecation of positional args in 0.23 as we can see in this profiling:Using public functions which are now wrapped in the positional args deprecation context manager in tight loops may have a non negligible overhead. The solution would be to do someting like:and call _function internally. That would apply to all utilities like metrics, validation, ... I don't know if we want to dive into this to improve perf of millisecond problems. What do you think @adrinjalali @rth ?Another alternative is to disable the wrapping with a context manager. For instance:Would this improve the overhead?WDYT @thomasjpfan?This could work. I have another idea I want to try out ;)@jeremiedbb Can you provide the code snippet you used to profile?I take back what I said. I misinterpreted the profile. The overhead of the decorator is actually the very small bars right under inner_f. The 3 large bars are the function itself (euclidean_distances, check_array and check_pairwise here).As a confirmation, I tried adrin's suggestion and it did not improve@thomasjpfan here's what I use to profile:in this specific case:At a glance, it looks like the other parts of inner_f are all the other wrapped functions such as check_pairwise_array and check_array, the overhead of _deprecate_positional_args itself is ~ms in total:That being said I think we can speed things up here as well.That's exactly what I said in the previous comment :DI think I managed to fix the issue now in #17235. @PrimozGodec would you mind checking this out to see if you recover the performances of 0.22 ?@jeremiedbb thank you for your help. I tested the PR and it works now normally.Thanks @PrimozGodec ! Closing.",Performance
https://github.com/flatironinstitute/mountainsort/issues/72,"Hi,I've been trying to install mounstersort with the command 'pip install ml_ms4alg' in conda but I keep getting the following error:ERROR: Failed building wheel for isosplit5I then tried 'pip install isosplit5' but ended up getting the same error. I have the updated Visual Studio installed and I can't figure out what is the problem after googling around. Could you help me resolve this? I'm very new to coding so please forgive me if this is a silly question.Thank you!Try pip install wheel and then try with ml_ms4alg again.Thank you for your reply! I did pip install wheel but still get the following error while trying to pipi install ml_ms4alg (I apologize in advance for this really long spam):Sorry again about this long error message! I copy-pasted everything this time in case I didn't state my error correctly.MSVC (and Windows at all) is not officially supported by mountainsort. This particular error is easy to fix but (1) there is no guarantee you won't encounter other errors, (2) it might be difficult to quickly push the changes through. I will see what I can do however it might be easier for you to go with an officially supported platform (e.g. Linux) instead.For what it's worth, Windows 10 users can now run Linux on Windows: https://docs.microsoft.com/en-us/windows/wsl/install-win10 . I had encountered the same error before, but after installing WSL, it seems to install just fine.Thank you for your reply! I was able to install mountainsort through ubuntu now but I seem to be having trouble to import anything in jupyter notebook due to error :
ImportError: Cannot load backend 'Qt5Agg' which requires the 'qt5' interactive framework, as 'headless' is currently running
It seems like I cannot open new windows through a browser in jupyter notebook. Were you able to resolve this while running WSL?",None yet
https://github.com/NixOS/nixpkgs/pull/61253,"add deeplabcut, and fix derivation for wxPython40, which is broken and unlisted: #54235. Now working thanks to help from Phoenix team wxWidgets/Phoenix#1162 and @deliciouslytypedResolves: #54235It would probably be more appropriate to separate wxPython 4 changes into a separate PR. (but I'm not sure)@deliciouslytyped hmm not sure either, I couldn't find relevant guidance in the contribution docs. I put as one pull request as DeepLabCut needs WxPython4 to work, and separating could lead to a broken package if merged before WxPython, but my NixOS / open source experience is limited so happy for advice!Thanks, @risicle, made the suggested change!@tbenst Could you separate wxPython fix into a separate PR? We generally prefer orthogonal changes to be in separate commits. The changes appear to be significant enough to be reviewed separately, so doing it in two separate PR's should help.@tbenst On that note, since you're worried about breakage on separation you could probably wait for the wxPython4 to get merged and then do the deeplabcut.@veprbl @deliciouslytyped sure can do. will have to wait a bit due to looming deadlinesReviewed wxPython partupdated now that wxPython4 is merged. Blocked until #71282Also need to fix the my_X pattern by instead using something like the following from pytorch derivation:@GrahamcOfBorg build deeplabcut python27Packages.deeplabcut python37Packages.deeplabcut python38Packages.deeplabcut
@GrahamcOfBorg build python27Packages.imageio python27Packages.imageio-ffmpeg
@GrahamcOfBorg build python37Packages.imageio python37Packages.imageio-ffmpeg
@GrahamcOfBorg build python38Packages.imageio python38Packages.imageio-ffmpeg
@GrahamcOfBorg build python27Packages.sklearn
@GrahamcOfBorg build python37Packages.sklearn
@GrahamcOfBorg build python38Packages.sklearnAlso, please, consider reordering commits to move ""pythonPackages.deeplabcut: init at 2.1.6.4"" down the chain. This should prevent broken eval in the intermediate states that can mess with bisecting.thanks @veprbl. Reordered commits. Re:imgaug, I don't know yet, but my guess is it's hardware dependent. imgaug tests definitely stalls on my Intel i7-9800X, which supports AVX-512 instructions. I'm now building on an Intel i7-5820K, which supports AVX2 but not 512. My guess is it will succeed--I've encountered this issue before with scikit-learn.This is, actually, an important bit of information. It is, probably, outside of scope of this PR to fix this, but maybe we should mention this in a comment, so that a next person with an older cpu will not remove it. It would not hurt to move it inside the definition of imgaug something like doInstallCheck = lib.hasPrefix ""mkl"" blas.name;, so that this bug will be worked around for more users and will get more visible.Actually, I just errored out (no hang) on my i7-9800X with your example...3000+ tests failed due to aleju/imgaug#537.Edit: turns out imgaug hangs on my AVX2-only cpu as well on the branch of this PR. Seems like on latest Nixpkgs master, imgaug tests fail insteadOk, I fixed the imgaug issues. I think this is finally ready to merge! Verified working if cherry-picked onto 20.03.Note that numpy is currently broken on master. But fixing that is outside the scope of this PR (I hope...).Edit: forgot to mention, I figured out the hang. It's caused by an unknown bug in OpenBLAS. Hang does not occur with MKL. It's occurs in a call to np.dot, and cannot be replicated with the inputs alone, and most tests work fine, so I don't want to force users not to use openblas.Edit2: found a small issue, need to add wrapGapps, will push...ok, all the gapps & numpy issues are resolved, but now boto3 is broken on master.This is fixed in staging-next, please allow a few days for it to land in master",10.rebuild-darwin: 11-100 10.rebuild-linux: 11-100 11.by: package-maintainer 2.status: merge conflict 6.topic: python 8.has: package (new)
https://github.com/scikit-learn/scikit-learn/pull/16993,"Deprecation until version 0.25.The current approach in _precompute_metric_params
(Are there other metrics where we have a similar pattern?Good to go, @thomasjpfan?tagging for inclusion #17010",Waiting for Reviewer module:metrics
https://github.com/materialsinnovation/pymks/issues/369,"mks_structure_analysis : use of RandomizedPCA leading to errors, also assertion errors where model output is negative of target output - perhaps because behavior of some function has changed.Get the following deprecation warnings in fiber.ipynband",bug
https://github.com/vzhou842/profanity-check/issues/11,"Hi,Thank you for working on this project!As far as I can tell, scikit-learn–– which is a massive dependency, is not exactly required to make this package work. Maybe it's required for training a model, but it should not be required for using the model (which is what this package is for).You're using joblib, which is a transitive dependency on Skikit anyway. Even they suggest removing it:@iMerica that's a great point. Do you want to submit a PR for this? Should be pretty straightforwardTo assist this is the full messages we get for Python 3.8:As of now joblib is not in sklearn externals which makes the link above broken: https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/externalsDid not check on why but this is the same as #12 with tests passing.closed by #13",help wanted
https://github.com/flatironinstitute/mountainsort/issues/72,"Hi,I've been trying to install mounstersort with the command 'pip install ml_ms4alg' in conda but I keep getting the following error:ERROR: Failed building wheel for isosplit5I then tried 'pip install isosplit5' but ended up getting the same error. I have the updated Visual Studio installed and I can't figure out what is the problem after googling around. Could you help me resolve this? I'm very new to coding so please forgive me if this is a silly question.Thank you!Try pip install wheel and then try with ml_ms4alg again.Thank you for your reply! I did pip install wheel but still get the following error while trying to pipi install ml_ms4alg (I apologize in advance for this really long spam):Sorry again about this long error message! I copy-pasted everything this time in case I didn't state my error correctly.MSVC (and Windows at all) is not officially supported by mountainsort. This particular error is easy to fix but (1) there is no guarantee you won't encounter other errors, (2) it might be difficult to quickly push the changes through. I will see what I can do however it might be easier for you to go with an officially supported platform (e.g. Linux) instead.For what it's worth, Windows 10 users can now run Linux on Windows: https://docs.microsoft.com/en-us/windows/wsl/install-win10 . I had encountered the same error before, but after installing WSL, it seems to install just fine.Thank you for your reply! I was able to install mountainsort through ubuntu now but I seem to be having trouble to import anything in jupyter notebook due to error :
ImportError: Cannot load backend 'Qt5Agg' which requires the 'qt5' interactive framework, as 'headless' is currently running
It seems like I cannot open new windows through a browser in jupyter notebook. Were you able to resolve this while running WSL?",None yet
https://github.com/scikit-learn/scikit-learn/issues/10708,,None
https://github.com/udacity/ud120-projects/pull/79,,None
https://github.com/jingweimo/food-image-classification-/issues/1,,None
https://github.com/adriangb/scikeras/pull/67,,None
https://github.com/scikit-learn/scikit-learn/pull/17803,,None
https://github.com/banditml/banditml/pull/19,,None
https://github.com/scikit-learn/scikit-learn/pull/14898,,None
https://github.com/tslearn-team/tslearn/issues/221,,None
https://github.com/RaRe-Technologies/gensim/pull/2821,,None
https://github.com/inovex/justcause/issues/18,,None
https://github.com/Teichlab/cellphonedb/issues/153,,None
https://github.com/skorch-dev/skorch/issues/521,,None
https://github.com/equinor/gordo/issues/920,,None
https://github.com/podgorskiy/ALAE/issues/15,,None
https://github.com/librosa/librosa/issues/1156,,None
https://github.com/oddt/oddt/issues/108,"Hi! Thanks to the developer putting together the ODDT library of tools. It has the potential to be very impactful.I'm running into an issue with vina docking where it produces distorted ligand structures (see screenshot). I compared with running independently installed Vina 1.1.2 and it works expected. I'm using identical protein.pdbqt and ligand.pdbqt and xyz parameters.Command:
time oddt_cli --dock autodock_vina --receptor recep.pdbqt --center '(9.50,-53.29,2.64)' --size '(70,70,70)' --exhaustiveness 5 --seed 123 ligand.pdbqt -o pdbqt -O test.pdbqtOriginal ligand.pdbqtOutput:
Docked ligand.pdbqtAny thoughts would be much appreciated!Just to add, i'm using openbabel 2.4.1.Which version of the oddt are you using? I would recommend the latest git one, until we make 0.7 release. Could you also share the pdbqt files so that I can reproduce your error?Same here. I have checked and the problem is coming from this line that copies the coordinates from the docked ligand to the original source.It seems that assuming # ... the order of atoms match between ligands is not true, at least for openbabel 2.4.1.
Can't we just return the docked ligand as it is ?@mwojcikowski , is the reordering performed for openbabel > 2.4.0 necessary ?
Reading both the ligand and docked ligand from the tempfile directly is working fine:Unfortunately this is not so straightforward, basicaly PDBQT does not store information about the bond orders and molecules can be messed up in many different ways too, hence the mechanism to copy over the conformer back to the original molecule. I will look into this, but it is really weird that you have the same behavior with RDKit, as OB uses very different PDBQT writter. I would expect at least one of them not to fail.Could you at least share the lingand pdbqt file?@mwojcikowski I'm using version 0.6 from the conda channel. I'll try to install the got version and try again, perhaps with rdkit. Is it safer to do a pdbqt to pdbqt conversion rather than coord copy?I'll share ligand.pdbqt tmr.Here's the ligand:ligand.pdbqt.txtVersion:
I pulled and installed master branch, which is v 0.6. But:oddt_cli --version
0.5Is this right?Wrong reported version is a known issue, I will fix it with a new release.I will take a look at the ligand.@mwojcikowski quick update. I created a new conda environment and ran:conda install -c oddt oddt
conda install -c oddt openbabelThe output ligand looks fine!But the problem re-occurs if I then install via:
python setup.py installIt might be that particular change that broke it for you: 0208f21Need to investigate it.The issue seem to be pdbqt writing, I get correct molecules if I write out docking conformers to sdf files. You can use this strategy until I fix the underlying issue.Also RDKit PDBQT writer works fine with the file you sent.I get the same messed-up molecules even if they are written to sdf files by using oddt_cli. The program versions should be the latest as they were installed a month ago via conda.@parkc23 Could you please provide a ligand with such issue? Also please specify the toolkit you use and its version (OB/RDKit).Thanks for the prompt attention. Please find below for the input and output files.
in_out.sdf.tar.gzBy the way, how do I check the versions for OB and RDKit? I looked into the anaconda/pkgs folder, and found these below. (I installed these via conda recently)
openbabel-2.4.1-py27_3
rdkit-2017.09.1-py27_1I managed to reproduce the problem. To solve it just upgrade to OpenBabel 3.0 conda install -c conda-forge openbabel. In the meantime I will check if there is anything on our side that I can do to fix it.Hi,
Is it ok to ignore these warnings harmless?Thanks a lot.The first one you can ignore, the second one suggests that you should retrain your models. In general it should work, but there is no harm to retrain them to be extra cautious.Openbabel <3.0 was retired in #134, thus I'm closing this Issue. If you have further unrelated questions please open another issue.",None yet
https://github.com/scikit-learn/scikit-learn/pull/14241,"This should be easier once we have an n_features_in_ attribute #13603, and use a OneToOneMixin or something like that.Does this look good, and do we want it like this?
right now the implementation via the property is a bit weird because it means if someone is inheriting from BaseEstimator they might get an error if they don't implement any of the things that tell us the number of output features.
So it adds a weird thing to the API contract for 3rd parties that says ""if we can't figure out how to get the number of output dimensions you have to define _n_features_out.I also don't like that we can't just overwrite the property :-/Why not provide a setter??Why not provide a setter??Ok so I did everything with mixins. That seems a bit verbose right now, but I'm pretty sure once we add feature names in some way, this will pay off.
I'm not entirely certain if we should make the mixin public. We could leave it for now but by the time the release rolls around we need to have a plan. But I expect a couple of things will change before then.I'm really not sure why we'd use mixins here instead of base classes tbh. I would rather use base-classes@adrinjalali can you please review this? I think having this will be helpful for feature names, and this one is actually one of the easier parts.No it means I don't understand you saying you didn't see a test that you commented on ;)So is that approval from you @adrinjalali ? ;)@jnothman thoughts?@amueller merge conflicts in the meantime.@amueller, this is looking pretty good, but you wrote that ""This PR implements it for all estimators but FunctionTransformer, which sets it explicitly to None.""... but it doesn't include vectorizers.It also doesn't add n_features_out_ for Pipeline, FeatureUnion, GridSearchCV, etc., as far as I can see.updated to reflect changes from #14812.Correct, cause they are not covered in common tests :(Should I add this to meta-estimators in this PR as well?@jnothman they are tested in #9741 now ;) - at least somewhat.And yes, I agree that we want the attributes. I wasn't sure if I should add them here, but happy to do so.I guess since we're doing a SLEP for n_features_in_, we should also do one for this one.@adrinjalali we could. Or we could do one for both? The issues are basically the same. The API is pretty easy and obvious, I think the only questions are about backward compatibility of check_estimator.still needs pipeline (and XSearchCV? And CountVectorizer?).
Pipeline is a bit strange in that we need to find the last step that's not passthrough and if all of them are then we need n_features_in_...",Needs Decision
https://github.com/bitextor/bicleaner/issues/25,"Using bicleaner_train, bicleaner_train_lite, bicleaner_classifier_full or bicleaner_classifier_lite, scikit-learn throws a deprecation warning, suggesting that joblib should be installed directly.Joblib should be added to requirements.txt and those bicleaner scripts should be refactored to refer to the joblib package where appropriate.Yes, we are aware of that.It also comes with another sklearn warning, stating that trying to unpickle decission trees from different versions may lead to breaking code or invalid results. So, we need to re-train again all language packs.So, the fix for this will come after retraining.(Note to self: Internal #85)This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.This issue has been automatically closed because it has not had recent activity. Thank you for your contributions.How do I have to import in order to avoid that warning?Hi @javier171188 , Bicleaner 0.13 (released today in the master branch) has this dependency fixed.DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.i'm facing the same issue how to fix this ? what does this error saying ?Maybe you are using models trained with Bicleaner 0.12 and below, so the .classifier files were created with sklearn-0.21. You should use Bicleaner v0.13 and Bicleaner models v1.3, or in case you are using your own models train them again with Bicleaner 0.13.",staledByBot
https://github.com/ageron/handson-ml/issues/244,"could not read bytescan load_digits be used in place of fetch_ml data?Hi @elseagle ,
Thanks for your question. Could you please provide (much) more details? At least a copy of the code you ran (so I can try to reproduce the error) and the full exception stack trace. Also, please indicate your OS, python version and scikit-learn version.To answer your second question, the load_digits() function returns a different (simpler) dataset, it's not MNIST, so unfortunately the answer is no.I just run the following code, and it worked fine:Can you try this please?SKLEARN VERSION:
0.19.1OS:
KALI 2018.2ran this --->got this:Mmh, I guess there might have been an error while downloading MNIST, perhaps it's corrupted or empty. You can try deleting the following file: ~/scikit_learn_data/mldata/mnist-original.mat and trying again.wow, i just deleted it and tried it again.. and it worked! Thanks @ageronI'm getting a depreciation warning on fetch_mldata and then a timeout when trying to run the code. The SkLearn Source points to mldata.org, which is down. Maybe it'd make sense to host this dataset elsewhere?I understand that it's a pretty common dataset, but don't know what format, etc the book expects it to be in, the format of the dataset, etc.Hi @RogerTangos ,
Sorry for the delayed answer, I was traveling. It seems that mldata.org is down for good. It is now replaced with fetch_openml(), see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml .
I will update the notebooks accordingly.
Hope this helps,
AurélienTry using:
from sklearn import datasets
digits = datasets.load_digits()",None yet
https://github.com/pypeit/PypeIt/pull/950,"PR that addresses #948 and turns on median combination of PypeIt images.
While the parameter existed, no functionality did.Also turns on median combination as the default for bias combining.While the code changes are minor, this is fundamental enough I am running all tests and only submitting to develop.Unit tests have passed================ 229 passed, 579 warnings in 592.52s (0:09:52) ================Two minor comments, and one comment to consider a masked median. For example, what happens when all pixels need to be masked?Thanks for sorting this out Ryan!--- PYPEIT DEVELOPMENT SUITE FAILED 3/70 TESTS (Masters ignored) ---
Failed tests:
run_pypeit: keck_kcwi/bh2_4200
pypeit_tellfit: gemini_gnirs/32_SB_SXD
pypeit_coadd_1dspec: gemini_gmos/GS_HAM_R400_860hard to see how my changes affected any of the Dev suite failures, but am
making a quick re-redux.All good!Directory: /data/Projects/Python/PypeIt-development-suite/REDUX_OUT/gemini_gmos/GS_HAM_R400_860
Command line: pypeit_coadd_1dspec /data/Projects/Python/PypeIt-development-suite/coadd1d_files/gemini_gmos_gs_ham_r400_860.coadd1d
Writing the parameters to coadd1d.par
Running pypeit_coadd_1dspec on gemini_gmos/GS_HAM_R400_860COADD1D --- FAILED
File ""/data/Projects/Python/PypeIt/pypeit/pypmsgs.py"", line 267, in error
raise PypeItError(msg)
pypeit.pypmsgs.PypeItError: No matching objects for SPAT1508-SLIT1789-DET01. Odds are you input the wrong OBJID(base) profx> pypeit_tellfit /data/Projects/Python/PypeIt-development-suite/REDUX_OUT/gemini_gnirs/32_SB_SXD/pisco_coadd.fits --redshift 7.52 --objmodel qso
Writing the parameters to telluric.par
UserWarning: Selected configuration file already exists and will be overwritten! (parset.py:683)
FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison (arraysetops.py:569)
RuntimeWarning: invalid value encountered in greater (coadd.py:626)
RuntimeWarning: invalid value encountered in greater (coadd.py:627)
RuntimeWarning: invalid value encountered in greater (utils.py:940)
[INFO] :: telluric.py 1865 init() - Initializing object model for order: 0, 0/1 with user supplied function: init_qso_model
FutureWarning: The sklearn.mixture.gaussian_mixture module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.mixture. Anything that cannot be imported from sklearn.mixture is now part of the private API. (deprecation.py:144)
UserWarning: Trying to unpickle estimator GaussianMixture from version 0.20.0 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk. (base.py:318)
[INFO] :: telluric.py 1915 run() - Fitting object + telluric model for order: 0, 0/1 with user supplied function: init_qso_model
[WARNING] :: utils.py 1883 robust_optimize() - Maximum number of iterations maxiter=3 reached in robust_optimize
[INFO] :: telluric.py 1941 save() - Writing object and telluric models to file: pisco_coadd_tellmodel.fits
WARNING: VerifyWarning: Card is too long, comment will be truncated. [astropy.io.fits.card]
[INFO] :: io.py 586 write_to_fits() - File written to: pisco_coadd_tellcorr.fitsmerging. Travis appears broken right now.https://travis-ci.org/github/pypeit/PypeIt/builds/699037384",None yet
https://github.com/alexandrebarachant/pyRiemann/issues/75,"Importing pyriemann.clustering.Kmeans raises a deprecation warning for scikit-learnFutureWarning: The sklearn.cluster.k_means_ module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.I'll push an PR quicklyThanks for reporting this.Yes, please make a PR :) Also, don't forget to update the requirements.txt with the correct version number.Likewise, there is an error ModuleNotFoundError: No module named 'sklearn.cross_validation' for the import in plot_classify_EEG_tangentspace:from sklearn.cross_validation import KFoldCreated a pull request to fix.Fixed",None yet
https://github.com/automl/auto-sklearn/issues/784,"This was the script that resulted in an unusually low R2 -- the expected result was higher than multiple regression (.55) instead of R2 of .12. The question is whether this indicates a poor use case for auto-SKLearn, a need for parameter or hyperparameter adjustments, or some other error in use?15:11:48 PRIVATE python3 eluellen-sklearn.py
/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.
warnings.warn(message, FutureWarning)
Samples = 2619, Features = 40
X_train = [[4.96200e+04 1.71090e+04 3.44800e-01 ... 1.70000e+04 3.26200e+04
6.57400e-01]
[5.95000e+04 0.00000e+00 0.00000e+00 ... 5.95000e+04 0.00000e+00
0.00000e+00]
[4.65400e+04 4.65400e+04 1.00000e+00 ... 1.15400e+04 3.50000e+04
7.52000e-01]
...
[5.25800e+04 3.14100e+04 5.97400e-01 ... 2.25800e+04 3.00000e+04
5.70600e-01]
[6.46150e+04 6.27120e+04 9.70500e-01 ... 9.61500e+03 5.50000e+04
8.51200e-01]
[5.25800e+04 2.90390e+04 5.52300e-01 ... 2.22230e+04 3.03575e+04
5.77400e-01]], y_train = [1. 0. 1. ... 1. 1. 1.]
/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.
FutureWarning)
[WARNING] [2020-02-18 15:11:58,185:AutoMLSMBO(1)::cb28bbd020a0a08a3c17168f19c8aaae] Could not find meta-data directory /usr/local/lib/python3.6/dist-packages/autosklearn/metalearning/files/r2_regression_dense
[WARNING] [2020-02-18 15:11:58,212:EnsembleBuilder(1):cb28bbd020a0a08a3c17168f19c8aaae] No models better than random - using Dummy Score!
[WARNING] [2020-02-18 15:11:58,224:EnsembleBuilder(1):cb28bbd020a0a08a3c17168f19c8aaae] No models better than random - using Dummy Score!
[WARNING] [2020-02-18 15:12:00,228:EnsembleBuilder(1):cb28bbd020a0a08a3c17168f19c8aaae] No models better than random - using Dummy Score!
[(0.340000, SimpleRegressionPipeline({'categorical_encoding:choice': 'one_hot_encoding', 'imputation:strategy': 'median', 'preprocessor:choice': 'extra_trees_preproc_for_regression', 'regressor:choice': 'ridge_regression', 'rescaling:choice': 'quantile_transformer', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'preprocessor:extra_trees_preproc_for_regression:bootstrap': 'True', 'preprocessor:extra_trees_preproc_for_regression:criterion': 'mae', 'preprocessor:extra_trees_preproc_for_regression:max_depth': 'None', 'preprocessor:extra_trees_preproc_for_regression:max_features': 0.8215479502881777, 'preprocessor:extra_trees_preproc_for_regression:max_leaf_nodes': 'None', 'preprocessor:extra_trees_preproc_for_regression:min_samples_leaf': 11, 'preprocessor:extra_trees_preproc_for_regression:min_samples_split': 9, 'preprocessor:extra_trees_preproc_for_regression:min_weight_fraction_leaf': 0.0, 'preprocessor:extra_trees_preproc_for_regression:n_estimators': 100, 'regressor:ridge_regression:alpha': 4.563743442447699, 'regressor:ridge_regression:fit_intercept': 'True', 'regressor:ridge_regression:tol': 4.8339309027613326e-05, 'rescaling:quantile_transformer:n_quantiles': 572, 'rescaling:quantile_transformer:output_distribution': 'uniform', 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.022216999044307732},
dataset_properties={
'task': 4,
'sparse': False,
'multilabel': False,
'multiclass': False,
'target_type': 'regression',
'signed': False})),
(0.340000, SimpleRegressionPipeline({'categorical_encoding:choice': 'one_hot_encoding', 'imputation:strategy': 'most_frequent', 'preprocessor:choice': 'fast_ica', 'regressor:choice': 'extra_trees', 'rescaling:choice': 'minmax', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False', 'preprocessor:fast_ica:algorithm': 'parallel', 'preprocessor:fast_ica:fun': 'logcosh', 'preprocessor:fast_ica:whiten': 'False', 'regressor:extra_trees:bootstrap': 'False', 'regressor:extra_trees:criterion': 'friedman_mse', 'regressor:extra_trees:max_depth': 'None', 'regressor:extra_trees:max_features': 0.343851332296278, 'regressor:extra_trees:max_leaf_nodes': 'None', 'regressor:extra_trees:min_impurity_decrease': 0.0, 'regressor:extra_trees:min_samples_leaf': 14, 'regressor:extra_trees:min_samples_split': 5, 'regressor:extra_trees:n_estimators': 100},
dataset_properties={
'task': 4,
'sparse': False,
'multilabel': False,
'multiclass': False,
'target_type': 'regression',
'signed': False})),
(0.260000, SimpleRegressionPipeline({'categorical_encoding:choice': 'one_hot_encoding', 'imputation:strategy': 'mean', 'preprocessor:choice': 'no_preprocessing', 'regressor:choice': 'random_forest', 'rescaling:choice': 'standardize', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'regressor:random_forest:bootstrap': 'True', 'regressor:random_forest:criterion': 'mse', 'regressor:random_forest:max_depth': 'None', 'regressor:random_forest:max_features': 1.0, 'regressor:random_forest:max_leaf_nodes': 'None', 'regressor:random_forest:min_impurity_decrease': 0.0, 'regressor:random_forest:min_samples_leaf': 1, 'regressor:random_forest:min_samples_split': 2, 'regressor:random_forest:min_weight_fraction_leaf': 0.0, 'regressor:random_forest:n_estimators': 100, 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.01},
dataset_properties={
'task': 4,
'sparse': False,
'multilabel': False,
'multiclass': False,
'target_type': 'regression',
'signed': False})),
(0.040000, SimpleRegressionPipeline({'categorical_encoding:choice': 'one_hot_encoding', 'imputation:strategy': 'most_frequent', 'preprocessor:choice': 'fast_ica', 'regressor:choice': 'ridge_regression', 'rescaling:choice': 'standardize', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'preprocessor:fast_ica:algorithm': 'deflation', 'preprocessor:fast_ica:fun': 'exp', 'preprocessor:fast_ica:whiten': 'True', 'regressor:ridge_regression:alpha': 1.3608642297867532e-05, 'regressor:ridge_regression:fit_intercept': 'True', 'regressor:ridge_regression:tol': 0.002596874543719601, 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.00017348437847697216, 'preprocessor:fast_ica:n_components': 1058},
dataset_properties={
'task': 4,
'sparse': False,
'multilabel': False,
'multiclass': False,
'target_type': 'regression',
'signed': False})),
(0.020000, SimpleRegressionPipeline({'categorical_encoding:choice': 'no_encoding', 'imputation:strategy': 'median', 'preprocessor:choice': 'select_percentile_regression', 'regressor:choice': 'ridge_regression', 'rescaling:choice': 'quantile_transformer', 'preprocessor:select_percentile_regression:percentile': 82.56436225708288, 'preprocessor:select_percentile_regression:score_func': 'mutual_info', 'regressor:ridge_regression:alpha': 1.6259354959848533, 'regressor:ridge_regression:fit_intercept': 'True', 'regressor:ridge_regression:tol': 0.005858793476627702, 'rescaling:quantile_transformer:n_quantiles': 431, 'rescaling:quantile_transformer:output_distribution': 'normal'},
dataset_properties={
'task': 4,
'sparse': False,
'multilabel': False,
'multiclass': False,
'target_type': 'regression',
'signed': False})),
]
R2 score: 0.12086525801756198real 1m58.008s
user 2m17.253s
sys 0m12.919sHey, could you please increase the runtime a bit? 30 minutes or 1 hours should improve the performance of Auto-sklearn.Closing this due to inactivity.",None yet
https://github.com/Regexose/SmallData/pull/40,"@staudamm
ich wollte mal einen neuen Song ausprobieren aber beim starten des songs (run.py song) scheint es auf einmal probleme mit den sklearn modulen zu geben.
geht es bei dir auf?@Regexose kannst du die Fehlermeldung hier reinkopieren? Dann kann ich dit checken@staudamm
das ist es:
Boriss-iMac:SmallData_GitHub borisjoens$ python run.py song
/Users/borisjoens/.pyenv/versions/3.7.2/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.stochastic_gradient module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)
/Users/borisjoens/.pyenv/versions/3.7.2/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.sgd_fast module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)
/Users/borisjoens/.pyenv/versions/3.7.2/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator SGDClassifier from version 0.21.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)@staudamm Entwarnung
ach schon wieder. Ich musste den Trainer.py nochmal anschmeissen vorher. Da gab es zwar auch eine (future) warnung aber es gingBoriss-iMac:classification borisjoens$ python trainer.py
/Users/borisjoens/.pyenv/versions/3.7.2/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.stochastic_gradient module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)@staudamm
Lieber Benni, ich habe hier eine Klasse für die FX Regulierung geschrieben.
Ohne Live vielleicht schwer zu überprüfen, aber die aktuellen Werte werden in SongServer.sendFX angezeigt.
geht das bei Dir? weiss nicht, ob und worein man das mergen könnte.. Was meinst du?@Regexose kannst du mir nochmal in Worten beschreiben, was du in dem branch genau erreichen willst. Dann kann ich das besser reviewen...@staudamm
Ziel ist es, Werte für die Plugins (Delay, Distortion etc) nach Live zu senden. Davor war das ein zufälliger Wert. Jetzt soll eine ""Tonality"" class alle categories zählen und daraus eine Effekt-Chain ermitteln. Zusätzlich wird auch ein controller-Wert gesendet, der einen Parameter in der aktuell ausgewählten ""chain"" steuert.
Die SongServer.send_quittung Funktion sendet direkt Noten die in settings.category_to_note definiert wurden. Kann wohl auch eher in die Tonality class, nehme ich an..
hoffe, so ist es etwas klarer ..?Hi @Regexose, sieht jut aus soweit. Es gibt (natuerlich) ein paar Kommentare zur beauty. Kritisch ist nur die Benutzung von time.sleep - da lass uns drüber labern.@staudamm
du hast mir allem Recht gehabt.
somit habe ich es geändert",None yet
https://github.com/avinashsai/doc2vec/pull/39,This PR updates gensim from 3.6.0 to 3.8.3.,None yet
https://github.com/scikit-learn/scikit-learn/pull/10739,"Fixes #10736This PR improve the spectral clustering implementation.The first eigenvector is now dropt in spectral clustering. And the eigenvectors is weithted by their related eigenvalues for the embedding.Givng a simple affinity matrix like
[ (1, 2, 100), (1, 3, 100), (2, 3, 100), (3, 4, 1), (4, 5, 100), (4, 6, 100), (5, 6, 100) ]If the cluster number is set 2, now spectral clustering can stably resulting in clusters (1,2,3) and (4,5,6)This pull request introduces 2 alerts when merging eb7a35d into ec691e9 - view on lgtm.comnew alerts:Comment posted by lgtm.comYour PR broke quite a few existing tests, you will need to look at the failures (see this Travis build for example), and fix the tests.See this for example for how to run the tests on your machine.I'm sorry that I haven't given code of test at the beginning.For the original spectral_clustering, the result is unstable, no matter how many components used.
After setting drop_first = Truewould always give the right answer. But n_components must be 1.After weight eigenvectors according to their related eigenvalues, it works stably no matter how many components used. For exampleI use this as a test example because it's easy for us to figure out which result is reasonable.This pull request introduces 2 alerts when merging 741ec13 into 2aba6e2 - view on lgtm.comnew alerts:Comment posted by lgtm.comOh, it seems that I use rebase to update my PR and lost all the comments base on the last version……I conclude the opnions in the lost discussion here.I found that spectral embedding use a private function to check the connectivity of input.What do you think of these @lesteve @jmargeta @glemaitre @jnothmanThe issue appears to be fixed already, so may be the PR and the issue can be closed? Running the provided code gives the correct answer for any n_components<4 (unclear why one would want to use larger n_components?):@lobpcg I just saw you commented somewhere about dropping the first eigenvector if and only if the graph is connected. That sounds like the right thing to me but I can't find the comment right now. Are we implementing that?docs say:but it looks like we never drop it. But I guess the assumption is the discretization method doesn't mind a constant vector?
I agree we should close this and the issue, though.also eigensolver='amg' seems to give garbage but I figure that's related to #13393?with the above example doesn't give the result I'm expecting (which would be one constant eigenvector and one eigenvector separating the two clusters, which is what the other solvers give).The ""constant"" vector is actually needed in #12316 implements an alternative method for choosing the clusters from the eigenvectors, outperforming both k-means and discretization method.s The constant vector presence should surely make no difference in k-means and discretization methodsUnrelated to this issue, there is some mystery to me why there is this distinction in the code whether or not the graph is fully connected...Most likely - could you please give #13393 a try? The trouble may also be related to scipy/scipy#10621 since AMG is actually lobpcg with preconditioning.@lobpcg when you say #13393, do you mean adding a small value to the diagonal? That didn't seem to help. To me it looks more like the eigenvalues are inverted as you suggested in #10720 (comment) - the 5th eigenvector is as expected (wow that seems bad)Yes, in this case the issue is most likely just the same as in #10720 (comment) - thanks for reminding me my own recent post!@lobpcg yeah it's a bit tricky to keep track of all of these, thanks for all your input!Closing this following @amueller comment.
Thanks @lobpcg for checking this.",Waiting for Reviewer module:cluster module:manifold
https://github.com/shankarpandala/lazypredict/pull/139,This PR updates optuna from 1.5.0 to 2.0.0.Closing this in favor of #181,None yet
https://github.com/ScottfreeLLC/AlphaPy/issues/31,"I'm getting this error when running the examples.Any ideas?This was for the NCAAB example but seems to error in the same way across all examples.Operating system info below.platform: 'Darwin-18.7.0-x86_64-i386-64bit'
release: '18.7.0'
version: 'Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64'Sure, could you check which version of AlphaPy you're running? Run the following command:(base) localhost:AlphaPy markconway$ pip show alphapy
Name: alphapy
Version: 2.4.2You may have to update dependencies as well:$ pip install -U alphapyThen try running the examples again. Thank you!Will do! I think the problem here was that I had a virtual environment running but didn't actually install alphapy - it was working because I was in the folder when I cloned the repo and had all the dependencies installed.Nonetheless, I did install alphapy (version 2.4.2) and then got this error when running an example:Full error log below:Looking a little deeper - this looks like it may be something with XGboost:I'm using version 1.0.2 but will keep diggingI remember this issue. Try running with xgboost 0.90. We're going to fix this with dynamic import of these external libraries.Aight - sounds good will do now.Ok cool. I installed XGBoost 0.90 + re-ran. Still seems like the same error is happening.I'm hoping to extend alphapy a bit and work it into a general-purpose ML framework I'm building, so will keep trying to get this all working on my cpu - or try a linux OS.A log is below if it helps.Then I ranOkay, you got by the target encoding error, looks like a real bug when trying to access the XGBoost coefficients from sklearn.py. Thank you.np - will let you know if I figure it outIf this helps, these are the current requirements.txt I have after running pip freeze >> requirements.txtOkay, we're using the following:(base) localhost:AlphaPy markconway$ pip show scikit-learn
Name: scikit-learn
Version: 0.22.1ok changing to thatStill getting the same errorOkay cool - going to go grab some dinner. I'll keep trying to dig around here and post any updates if I figure it outWasn't able to figure this out, so going to try to work with a Linux OS over next few days to get stuff running on this repo.Thanks for making a rich library here; looks like a great tool among others in the AutoML field.Thanks for your kind words Jim, and let us know of any enhancements you'd like to see.Hey - trying again here.I loaded up Linux / Ubuntu on WIndows and I'm on Python 3.I got the first example to work again and have a project with the config files.This was with some custom data in a .CSV file (train and test). I have all these files attached
Battlecry_Cashregister_librosa_features_alphapy.zipWhen I cd into the folder and run alphapy I get stuck at this step:Any ideas on how to fix this? Is this a problem with the way I formatted the .CSV files?I'm running Python 3.6.9 on Ubuntu (shown below):Nevermind - I got it all working. Just updated a few dependencies here and the YAML file and it worked fine.",None yet
https://github.com/EmanuelGoncalves/dtrace/issues/1,"Hi Emanuel,When I tried doing ""python3 setup.py sdist bdist_wheel"", it prompted out that ""ImportError: cannot import name 'CrispyPlot' from 'crispy'"". I installed ""crispy"" from conda, but there is no ""CrispyPlot"" in this module. What ""crispy"" module are you using in your scripts? How can I install it? Thanks.The full error information is below:Traceback (most recent call last):
File ""setup.py"", line 14, in
exec(f.read(), version)
File """", line 7, in
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/dtrace/dtrace/init.py"", line 7, in
from dtrace.DTracePlot import DTracePlot
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/dtrace/dtrace/DTracePlot.py"", line 10, in
from crispy import CrispyPlot
ImportError: cannot import name 'CrispyPlot' from 'crispy' (/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.8/site-packages/crispy/init.py)Hi EmanuelI just found the ""crispy"" module in your repo. But it showed that I need ""model_list_20200107.csv"" to import crispy. Where can I find this file?Full error message:$ python3 setup.py sdist bdist_wheel/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.ranking module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.
warnings.warn(message, FutureWarning)
Traceback (most recent call last):
File ""setup.py"", line 14, in
exec(f.read(), version)
File """", line 7, in
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/dtrace/dtrace/init.py"", line 7, in
from dtrace.DTracePlot import DTracePlot
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/dtrace/dtrace/DTracePlot.py"", line 10, in
from crispy import CrispyPlot
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/crispy/init.py"", line 8, in
from crispy.QCPlot import QCplot
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/crispy/QCPlot.py"", line 15, in
from crispy.CrispyPlot import CrispyPlot
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/crispy/CrispyPlot.py"", line 14, in
class CrispyPlot:
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/crispy/CrispyPlot.py"", line 91, in CrispyPlot
natsorted(list(Sample().samplesheet[""cancer_type""].value_counts().index)),
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/crispy/DataImporter.py"", line 37, in init
pd.read_csv(f""{DPATH}/{samplesheet_file}"")
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/pandas/io/parsers.py"", line 676, in parser_f
return _read(filepath_or_buffer, kwds)
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/pandas/io/parsers.py"", line 448, in _read
parser = TextFileReader(fp_or_buf, **kwds)
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/pandas/io/parsers.py"", line 880, in init
self._make_engine(self.engine)
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1114, in _make_engine
self._engine = CParserWrapper(self.f, **self.options)
File ""/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1891, in init
self._reader = parsers.TextReader(src, **kwds)
File ""pandas/_libs/parsers.pyx"", line 374, in pandas._libs.parsers.TextReader.cinit
File ""pandas/_libs/parsers.pyx"", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File /u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/crispy/data//model_list_20200107.csv does not exist: '/u/project/eeskin2/k8688933/sanofi/crispr-screening/envs/lib/python3.7/site-packages/crispy/data//model_list_20200107.csv'Hi,Thanks for reporting this. Indeed Crispy (Cy) is a dependency and it's listed on the requirements.txt file so I expected it to be automatically installed. That file is outdated and was removed, I've updated Crispy, try to reinstall it.",None yet
https://github.com/mne-tools/mne-python/pull/3485,"closes #3484I still need to add some tests@kingjr if you want to cite a paper for the multiclass extension :
for the AJD and the ordering and selection of filters :Grosse-Wentrup, Moritz, and Martin Buss. ""Multiclass common spatial patterns and information theoretic feature extraction."" IEEE Transactions on Biomedical Engineering, Vol 55, no. 8, 2008.@alexandrebarachant I only made superficial changes to your code, so I'm not sure it's worth pushing to your toolbox; Let me know otherwise. Also, can you check whether my sample_weights for the mean covariance is adequate in the case cov_est is ""epoch"" ?I hadn't realized that CSP creates a number of filter independently of the number of classes (unlike Xdawn), can you confirm this?@Eric89GXL I changed epoch_data into X e.g. csp.transform(epochs_data) => csp.transform(X), to keep a consistent naming across sklearn-like objects. Does this need a deprecation warning and a catch param e.g.@kingjr Xdawn is a method that find spatial filters that maximize the SNR of an ERP, therefore you get one set of filters independently for each type of ERP. CSP, on the other hand, is a 'contrast' method, i.e. the spatial filters are extracted to maximize the variance of one classe while minimizing the variance of the other class. multiclass case becomes more complicated, but you still have one set of filter for all classes.That's one reason why CSP filters and pattern are sometimes tricky to interpret, you dont really know which class it correspond. But that's also very adapted to Motor imagery, because it match the principle of synchronization / desynchronization that you typically observes for hand movement.ok, we're on the same page then.@kingjr if you want to make a test for the AJD, here is a set of input matrices and the corresponding results for the matlab code
AJD_data.zip@agramfort @Eric89GXL What do you suggest for checking ajd matrices wrt matlab? Write them as plain text in the test function, or adding a npy file to the rep?How big are they? For like 4x4 ASCII is fine, but if you're talking 100x10000 you'd want to use binary. Maybe h5io-style so it's compressed and future compatible, added to the mne-testing-data repo.@Eric89GXL it's 10 5x5 matrices. right now it's a 2kb npy file10 5x5 doesn't seem so bad, that's only 50 lines of (probably) annoying ASCII. Any way you can make it smaller and still have it be useful? like 5 2x3 matrices or something?i will try once i got access to matlab again. i think the minimum is 3 3x3 matrices.3 3x3's are reasonable enough to put directly in the test_*.py file as ASCIIThere's no log in travisWeird, restarted@kingjr can you demo this in an example or in the decoding tutorial?Changing the example too multi class takes forever; @alexandrebarachant do you have an idea how to address this?@kingjr indeed, it is quite slow on high dimensional data, which is a bit surprising because the joint diag is only on 4 matrices.One way to speed up is to use another AJD algo, like uwegde, but pham's algorithm give sligtly better results.you should use it on motor imagery data (lower dimension too). i also suggest to keep the CSP example with 2 classes, and makes an extra exemple for the multiclass case.ok, do you want to open a PR for uwedge?I don't want to focus my time on examples for now. I first want to finish with the new decoding objects, and will do the corresponding examples at the end.i think we should keep pham's algorithm.About the examples, it is better to wait until the end of the decoding refactoring to modify/add examples. the CSP example need to be improved anyway.Ok, so in this case, I'll open an issue to keep the speed and CSP example in mind.Besides ready on my end if CIs are happy.thanks guys",None yet
https://github.com/scikit-learn/scikit-learn/pull/17225,"Follows from #6217
Addresses last item in #3450Add sample_weight to median_absolute_error.
Use sklearn.utils.stats._weighted_percentile to calculated weighted median as suggested here: #6217 (comment)
Amended _weighted_percentile to calculate weighted percentile along axis=0 (for each column) if 2D array input. This is to allow multioutput. Does not change behaviour of _weighted_percentile when array 1D. Not sure if this is best way to implement thus will wait for comment before fixing tests.Used np.take_along_axis to amend _weighted_percentile to work for 2D arrays. As np.take_along_axis was introduced in numpy v1.15, added the function in fixes.py that implements a simplified version of numpy.take_along_axis if numpy version < 1.15.test_scorer_sample_weight doesn't work for neg_median_absolute_error due to the binary targets (also noted in the original PR #6217 (comment)), so skipped for this scorer. The test works for neg_median_absolute_error if targets continuous (e.g., using make_regression to generate data). Not sure best way to add/amend to check neg_median_absolute_error as well.ping @glemaitre :pThanks @lucyleeow , made a quick (and incomplete) pass.I'm wondering whether it's worth extending _weighted_percentile, or if it'd be easier and faster to just have our own version of weighted_median? I would assume that computing a weighted median is simpler and faster than having a general algorithm that works for every percentile, though I haven't looked into the detailsping @jnothmanThanks for the thorough review @glemaitre. I've added the test for 2D array and 1D sample weight and split test_scorer_sample_weight into reg and clf versions.apart of style, it looks good to meThe last changes. I think that we strictly do not check this part of the string so the tests did not fail but we should not print the right error message (without the variable values)",module:metrics module:utils
https://github.com/scikit-learn/scikit-learn/pull/15396,"Fixes #14953
Related to #15050Adds categories='dtypes' option to OrdinalEncoder and OneHotEncoder. With this option enabled, the dtypes are remember during fit and checked during transform.CC @NicolasHug @jnothman @jorisvandenbossche @adrinjalaliNice! Added some comments!I think that #14984, #15050, and #15396 might not be blockers for 0.22 and I would move them for 0.23.I think that it could be great to have a single issue (superseded #14953, #14954) to discuss the overall behaviour for categories in OneHotEncoder and OrdinalEncoder and from there having several PRs which follows the discussed proposals.I am happy with moving this to 0.23@rth On master, we add the column of zeros if a user passes in the categories explicitly:In the above case 'd' is not in the training set.Indeed but a categorical dtype, does not indicate anything about user defined categories. Basic setup: load some dataset, filter it say by date (or do a train/test split) and the train can have some columns with categories that have 0 entries. That's an extremely frequent use case. That does not mean the user wants to have zero columns in the train output. Otherwise one might as well use pd.get_dummies, and the whole point of the OneHotEncoder class for me is to remove categories that are not in the train set.Bottom line if others find adding categories='dtypes' useful I won't oppose it, but personally I think it's a dangerous option for most users.On the other hand, I would be very interested in using categories=""auto"" and getting faster computation if the column dtype happen to be categorical without it changing the results from what they are currently. ohe_categorical.ipynb is a quick implementation of what I mean:Data: ~65500 4-char categories, n_samples=500k, n_features=10 with categorical dtypeBTW, for non categorical dtype, converting the same data to categorical internally produces,Not asking to change anything in this PR, but my overall point is that having a fast implementation with auto-detection of categorical dtype (or even an internal conversion to it in some cases) would be more interesting than in an explicit new option for categories IMO.I agree with @rth that this could just be part of the 'auto' behavior, especially since the current 'dtype' option reverts back to 'auto' for non-df inputs. Supporting this with dfs and 'auto' should be backward compatible?A few questions:I see this ultimately ending up to be the default behavior ""auto"". I am concerned with how we get there.Currently, if we add this feature into auto it will break backwards compatibility. My initial plan was to add ""dtype"" to behave as the new ""auto"", then transition ""auto"" to this new behavior and then deprecate ""dtype"". It is a very long path to get to the desired new ""auto"" state.The other option would be to add another parameter to the encoders , such as ""use_dtype_categories"" to enable this new behavior with ""auto"". With this path, we would ultimately want to deprecate this new parameter.can you explain why?Another option is to introduce the parameter and deprecate 'auto'.
But then 'dtypes' might not be an ideal nameOn master, 'auto' will use the lexicon ordering to encode anything including the pandas series with a categorical dtype. With this update, the integer encoding given by the pandas categorical will be used, which may be different from the lexicon ordering.'better_auto'? lolNot the most user friendly, sinceConsider the following:Both of these series have the same dtype, but the training set is all 'b's. If we only got by the dtype, this will create a column of all zeros. I believe @rth is suggesting we remove this column. With this, if we see 'a', we would treat it as unknown.Another option is to make a list of such minor but annoying things to change via a deprecation cycle and make those changes in 1.0.@thomasjpfan could you summarize the main selling points of categories='dtype'?So far I see:(BTW, these should be in the UG)Yes this should be in the UG.Is this still the current proposal @thomasjpfan? Thanksmoving to 0.24, I have a feeling these ones need a little bit of discussion still. Happy for us to wait for it if y'all think it can get in.Yea this needs more discussion, this is more of a ""quality of life"" update when using pandas categories.The ""quick"" thing to is to only do this for OrdinalEncoder, which in turn makes it nicer a little nicer for unknown categories, which makes it nicer for cross validation.Can we just warn for now: ""From version 0.26 'auto' will adopt the ordering from the pandas dtype."" and do so only if lexicographic and dtype order are inconsitent?But then I think my latest proposal was identical to #15050Maybe we do indeed need to workshop this set of encoding PRs again.",module:preprocessing
https://github.com/hackalog/cookiecutter-easydata/pull/108,sklearn.datasets.base is deprecated. Change reference to where it actually lives,None yet
https://github.com/Azure/azure-storage-python/issues/649,"ImportError Traceback (most recent call last)
in
----> 1 from azure.storage.blob import BlobServiceClientImportError: cannot import name 'BlobServiceClient'Hi @alla15747Thanks for reaching out.
Would you like to provide your pip freeze result?Yes, thank you.absl-py==0.8.1
adal==1.2.2
alabaster==0.7.12
alembic==1.3.0
anaconda-client==1.7.2
anaconda-project==0.8.3
ansiwrap==0.8.4
applicationinsights==0.11.9
asn1crypto==1.0.1
astor==0.8.0
astroid==2.3.1
astropy==3.2.1
atomicwrites==1.3.0
attrs==19.2.0
azure-common==1.1.23
azure-core==1.0.0
azure-graphrbac==0.61.1
azure-mgmt-authorization==0.60.0
azure-mgmt-containerregistry==2.8.0
azure-mgmt-keyvault==2.0.0
azure-mgmt-resource==6.0.0
azure-mgmt-storage==6.0.0
azure-storage-blob==2.1.0
azure-storage-common==2.1.0
azureml-automl-core==1.0.74
azureml-contrib-datadrift==1.0.74
azureml-contrib-interpret==1.0.74
azureml-contrib-notebook==1.0.74
azureml-contrib-opendatasets==1.0.45
azureml-contrib-reinforcementlearning==0.1.0.5919674
azureml-contrib-server==1.0.74
azureml-contrib-services==1.0.74
azureml-core==1.0.74
azureml-datadrift==1.0.74
azureml-dataprep==1.1.30
azureml-dataprep-native==13.1.0
azureml-defaults==1.0.74
azureml-explain-model==1.0.74
azureml-interpret==1.0.74
azureml-mlflow==1.0.74
azureml-model-management-sdk==1.0.1b6.post1
azureml-opendatasets==1.0.74
azureml-pipeline==1.0.74
azureml-pipeline-core==1.0.74
azureml-pipeline-steps==1.0.74
azureml-samples==0+unknown
azureml-sdk==1.0.74
azureml-telemetry==1.0.74
azureml-tensorboard==1.0.74
azureml-train==1.0.74
azureml-train-automl==1.0.74
azureml-train-core==1.0.74
azureml-train-restclients-hyperdrive==1.0.74
azureml-widgets==1.0.74
Babel==2.7.0
backcall==0.1.0
backports.os==0.1.1
backports.shutil-get-terminal-size==1.0.0
backports.tempfile==1.0
backports.weakref==1.0.post1
beautifulsoup4==4.8.0
bitarray==1.0.1
bkcharts==0.2
bleach==3.1.0
bokeh==1.3.4
boto==2.49.0
boto3==1.10.16
botocore==1.13.16
Bottleneck==1.2.1
cachetools==3.1.1
certifi==2019.11.28
cffi==1.12.3
chardet==3.0.4
Click==7.0
cloudpickle==1.2.2
clyent==1.2.2
colorama==0.4.1
configparser==3.7.4
contextlib2==0.6.0
cryptography==2.7
cssselect==1.1.0
cycler==0.10.0
Cython==0.29.13
cytoolz==0.10.0
dask==2.5.2
databricks-cli==0.9.1
decorator==4.4.0
defusedxml==0.6.0
dill==0.3.1.1
distributed==2.5.2
distro==1.4.0
docker==4.1.0
docutils==0.15.2
dotnetcore2==2.1.10
entrypoints==0.3
enum34==1.1.6
et-xmlfile==1.0.1
fastcache==1.1.0
filelock==3.0.12
fire==0.2.1
Flask==1.0.3
fsspec==0.5.2
fusepy==3.0.1
future==0.18.2
gast==0.2.2
gensim==3.8.1
gevent==1.4.0
gitdb2==2.0.6
GitPython==3.0.4
glob2==0.7
gmpy2==2.0.8
google-auth==1.7.0
google-auth-oauthlib==0.4.1
google-pasta==0.1.8
gorilla==0.3.0
greenlet==0.4.15
grpcio==1.25.0
gunicorn==19.9.0
h5py==2.9.0
HeapDict==1.0.1
horovod==0.16.4
html5lib==1.0.1
idna==2.8
imageio==2.6.0
imagesize==1.1.0
imbalanced-learn==0.6.1
importlib-metadata==0.23
interpret-community==0.1.0.3.2
interpret-core==0.1.18
ipykernel==5.1.3
ipython==7.8.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
isodate==0.6.0
isort==4.3.21
itsdangerous==1.1.0
jdcal==1.4.1
jedi==0.15.1
jeepney==0.4.1
Jinja2==2.10.3
jmespath==0.9.4
joblib==0.14.0
json-logging-py==0.2
json5==0.8.5
JsonForm==0.0.2
jsonpickle==1.2
jsonschema==3.0.2
JsonSir==0.0.2
jupyter==1.0.0
jupyter-client==5.3.3
jupyter-console==6.0.0
jupyter-core==4.5.0
jupyterlab==0.34.12
jupyterlab-git==0.8.1
jupyterlab-launcher==0.13.1
jupyterlab-server==1.0.6
jupytext==1.2.4
Keras==2.3.1
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
keras2onnx==1.6.0
keyring==18.0.0
kiwisolver==1.1.0
lazy-object-proxy==1.4.2
liac-arff==2.4.0
libarchive-c==2.8
lief==0.9.0
lightgbm==2.3.0
llvmlite==0.29.0
locket==0.2.0
lxml==4.4.1
Mako==1.1.0
Markdown==3.1.1
MarkupSafe==1.1.1
matplotlib==3.1.2
mccabe==0.6.1
mistune==0.8.4
mkl-fft==1.0.14
mkl-random==1.1.0
mkl-service==2.3.0
mlflow==1.3.0
mock==3.0.5
more-itertools==7.2.0
mpmath==1.1.0
msgpack==0.6.1
msrest==0.6.10
msrestazure==0.6.2
multimethods==1.0.0
multipledispatch==0.6.0
nbconvert==5.4.1
nbdime==1.1.0
nbformat==4.4.0
ndg-httpsclient==0.5.1
networkx==2.3
nimbusml==1.5.0
nltk==3.4.5
nose==1.3.7
notebook==6.0.0
numba==0.45.1
numexpr==2.7.0
numpy==1.16.2
numpydoc==0.9.1
oauthlib==3.1.0
olefile==0.46
onnx==1.6.0
onnxconverter-common==1.6.0
onnxmltools==1.4.1
openpyxl==3.0.0
opt-einsum==3.1.0
packaging==19.2
pandas==0.23.4
pandas-ml==0.6.1
pandocfilters==1.4.2
papermill==1.2.1
parsel==1.5.2
parso==0.5.1
partd==1.0.0
path.py==12.0.1
pathlib2==2.3.5
pathspec==0.6.0
patsy==0.5.1
pep8==1.7.1
pexpect==4.7.0
pickleshare==0.7.5
Pillow==6.2.0
pkginfo==1.5.0.1
pluggy==0.13.0
ply==3.11
pmdarima==1.1.1
prometheus-client==0.7.1
prompt-toolkit==2.0.10
protobuf==3.10.0
psutil==5.6.3
ptyprocess==0.6.0
py==1.8.0
py4j==0.10.7
pyarrow==0.11.1
pyasn1==0.4.7
pyasn1-modules==0.2.7
pycodestyle==2.5.0
pycosat==0.6.3
pycparser==2.19
pycrypto==2.6.1
pycurl==7.43.0.3
pydot==1.4.1
pyflakes==2.1.1
Pygments==2.4.2
PyJWT==1.7.1
pylint==2.4.2
pyodbc==4.0.27
pyOpenSSL==19.0.0
pyparsing==2.4.2
pyrsistent==0.15.4
PySocks==1.7.1
pyspark==2.4.4
pytest==5.0.1
pytest-arraydiff==0.3
pytest-astropy==0.5.0
pytest-doctestplus==0.4.0
pytest-openfiles==0.4.0
pytest-remotedata==0.3.2
python-dateutil==2.8.0
Python-EasyConfig==0.1.7
python-editor==1.0.4
pytz==2019.3
PyWavelets==1.0.3
PyYAML==5.1.2
pyzmq==18.1.0
QtAwesome==0.6.0
qtconsole==4.5.5
QtPy==1.9.0
querystring-parser==1.2.4
requests==2.22.0
requests-oauthlib==1.3.0
Resource==0.2.1
rope==0.14.0
rsa==4.0
ruamel-yaml==0.15.46
ruamel.yaml==0.15.89
s3transfer==0.2.1
scikit-image==0.16.2
scikit-learn==0.20.3
scipy==1.1.0
scrapbook==0.2.0
seaborn==0.9.0
SecretStorage==3.1.1
Send2Trash==1.5.0
shap==0.29.3
simplegeneric==0.8.1
simplejson==3.16.0
singledispatch==3.4.0.3
six==1.13.0
skl2onnx==1.4.9
sklearn-pandas==1.7.0
smart-open==1.9.0
smmap2==2.0.5
snowballstemmer==2.0.0
sortedcollections==1.1.2
sortedcontainers==2.1.0
soupsieve==1.9.3
Sphinx==2.2.0
sphinxcontrib-applehelp==1.0.1
sphinxcontrib-devhelp==1.0.1
sphinxcontrib-htmlhelp==1.0.2
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.2
sphinxcontrib-serializinghtml==1.1.3
sphinxcontrib-websupport==1.1.2
spyder==3.3.6
spyder-kernels==0.5.2
SQLAlchemy==1.3.9
sqlparse==0.3.0
statsmodels==0.10.1
sympy==1.4
tables==3.5.2
tabulate==0.8.5
tblib==1.4.0
tenacity==6.0.0
tensorboard==2.0.1
tensorflow-estimator==2.0.1
tensorflow-gpu==2.0.0
termcolor==1.1.0
terminado==0.8.2
testpath==0.4.2
textwrap3==0.9.2
toolz==0.10.0
torch==1.3.1
torchvision==0.2.1
tornado==6.0.3
tqdm==4.36.1
traitlets==4.3.3
typed-ast==1.4.0
typing-extensions==3.7.4.1
unicodecsv==0.14.1
urllib3==1.24.2
w3lib==1.21.0
wcwidth==0.1.7
webencodings==0.5.1
websocket-client==0.56.0
websockets==8.1
Werkzeug==0.16.0
widgetsnbextension==3.5.1
wordcloud==1.6.0
wrapt==1.11.2
wurlitzer==1.0.3
xlrd==1.2.0
XlsxWriter==1.2.1
xlwt==1.3.0
zict==1.0.0
zipp==0.6.0Hi @alla15747Thanks for the pip result!
It seems you are trying to use azure-storage-blob>=12.0.0 which is in another repo while the old version was installed: azure-storage-blob==2.1.0. The current repo only support azure-storage-blob<=2.1.0 which doesn't have BlobServiceClient.What was is recommended is:Let me know if you need any help!Hi,I'm having the same issue. Here is the result of pip freeze :
adal==1.2.2
aiohttp==3.6.2
alembic==1.4.1
apache-airflow==1.10.9
apispec==1.3.3
argcomplete==1.11.1
asn1crypto==1.3.0
async-timeout==3.0.1
attrs==19.3.0
azure-applicationinsights==0.1.0
azure-batch==4.1.3
azure-common==1.1.24
azure-core==1.2.2
azure-cosmosdb-nspkg==2.0.2
azure-cosmosdb-table==1.0.6
azure-datalake-store==0.0.48
azure-eventgrid==1.3.0
azure-graphrbac==0.40.0
azure-keyvault==1.1.0
azure-loganalytics==0.1.0
azure-mgmt==4.0.0
azure-mgmt-advisor==1.0.1
azure-mgmt-applicationinsights==0.1.1
azure-mgmt-authorization==0.50.0
azure-mgmt-batch==5.0.1
azure-mgmt-batchai==2.0.0
azure-mgmt-billing==0.2.0
azure-mgmt-cdn==3.1.0
azure-mgmt-cognitiveservices==3.0.0
azure-mgmt-commerce==1.0.1
azure-mgmt-compute==4.6.2
azure-mgmt-consumption==2.0.0
azure-mgmt-containerinstance==1.5.0
azure-mgmt-containerregistry==2.8.0
azure-mgmt-containerservice==4.4.0
azure-mgmt-cosmosdb==0.4.1
azure-mgmt-datafactory==0.6.0
azure-mgmt-datalake-analytics==0.6.0
azure-mgmt-datalake-nspkg==3.0.1
azure-mgmt-datalake-store==0.5.0
azure-mgmt-datamigration==1.0.0
azure-mgmt-devspaces==0.1.0
azure-mgmt-devtestlabs==2.2.0
azure-mgmt-dns==2.1.0
azure-mgmt-eventgrid==1.0.0
azure-mgmt-eventhub==2.6.0
azure-mgmt-hanaonazure==0.1.1
azure-mgmt-iotcentral==0.1.0
azure-mgmt-iothub==0.5.0
azure-mgmt-iothubprovisioningservices==0.2.0
azure-mgmt-keyvault==1.1.0
azure-mgmt-loganalytics==0.2.0
azure-mgmt-logic==3.0.0
azure-mgmt-machinelearningcompute==0.4.1
azure-mgmt-managementgroups==0.1.0
azure-mgmt-managementpartner==0.1.1
azure-mgmt-maps==0.1.0
azure-mgmt-marketplaceordering==0.1.0
azure-mgmt-media==1.0.0
azure-mgmt-monitor==0.5.2
azure-mgmt-msi==0.2.0
azure-mgmt-network==2.7.0
azure-mgmt-notificationhubs==2.1.0
azure-mgmt-nspkg==3.0.2
azure-mgmt-policyinsights==0.1.0
azure-mgmt-powerbiembedded==2.0.0
azure-mgmt-rdbms==1.9.0
azure-mgmt-recoveryservices==0.3.0
azure-mgmt-recoveryservicesbackup==0.3.0
azure-mgmt-redis==5.0.0
azure-mgmt-relay==0.1.0
azure-mgmt-reservations==0.2.1
azure-mgmt-resource==2.2.0
azure-mgmt-scheduler==2.0.0
azure-mgmt-search==2.1.0
azure-mgmt-servicebus==0.5.3
azure-mgmt-servicefabric==0.2.0
azure-mgmt-signalr==0.1.1
azure-mgmt-sql==0.9.1
azure-mgmt-storage==2.0.0
azure-mgmt-subscription==0.2.0
azure-mgmt-trafficmanager==0.50.0
azure-mgmt-web==0.35.0
azure-nspkg==3.0.2
azure-servicebus==0.21.1
azure-servicefabric==6.3.0.0
azure-servicemanagement-legacy==0.20.6
azure-storage-blob==12.2.0
azure-storage-common==1.4.2
azure-storage-file==1.4.0
azure-storage-queue==1.4.0
Babel==2.8.0
backcall==0.1.0
bcrypt==3.1.7
boto==2.49.0
boto3==1.9.162
botocore==1.12.163
cached-property==1.5.1
cattrs==0.9.0
certifi==2019.3.9
cffi==1.12.2
chardet==3.0.4
Click==7.0
colorama==0.4.3
colorlog==4.0.2
configparser==3.5.3
croniter==0.3.31
cryptography==2.6.1
cycler==0.10.0
Cython==0.29.6
decorator==4.4.0
defusedxml==0.6.0
dill==0.3.1.1
docutils==0.14
Flask==1.1.1
Flask-Admin==1.5.4
Flask-AppBuilder==2.2.4
Flask-Babel==1.0.0
Flask-Caching==1.3.3
Flask-JWT-Extended==3.24.1
Flask-Login==0.4.1
Flask-OpenID==1.2.5
Flask-SQLAlchemy==2.4.1
flask-swagger==0.2.13
Flask-WTF==0.14.3
funcsigs==1.0.2
future==0.16.0
graphviz==0.13.2
gunicorn==19.10.0
idna==2.8
ijson==2.6.1
importlib-metadata==1.5.0
ipykernel==5.1.0
ipython==7.4.0
ipython-genutils==0.2.0
iso8601==0.1.12
isodate==0.6.0
itsdangerous==1.1.0
jedi==0.13.3
Jinja2==2.10.3
jmespath==0.9.4
json-merge-patch==0.2
jsonschema==3.2.0
jupyter-client==5.2.4
jupyter-core==4.4.0
kiwisolver==1.1.0
lazy-object-proxy==1.4.3
lockfile==0.12.2
Mako==1.1.2
Markdown==2.6.11
MarkupSafe==1.1.1
marshmallow==2.19.5
marshmallow-enum==1.5.1
marshmallow-sqlalchemy==0.22.3
matplotlib==3.0.3
msrest==0.6.11
msrestazure==0.6.2
multidict==4.7.5
numpy==1.16.2
oauthlib==3.1.0
oscrypto==1.2.0
pandas==0.24.2
paramiko==2.7.1
parso==0.3.4
patsy==0.5.1
pendulum==1.4.4
pexpect==4.6.0
pickleshare==0.7.5
prison==0.1.2
prompt-toolkit==2.0.9
psutil==5.7.0
psycopg2==2.7.6.1
ptyprocess==0.6.0
pyarrow==0.13.0
pycparser==2.19
pycryptodomex==3.9.7
pycurl==7.43.0
Pygments==2.3.1
pygobject==3.20.0
PyJWT==1.7.1
PyNaCl==1.3.0
pyOpenSSL==19.0.0
pyparsing==2.4.6
pyrsistent==0.15.7
PySocks==1.6.8
python-apt==1.1.0b1+ubuntu0.16.4.5
python-daemon==2.1.2
python-dateutil==2.8.0
python-editor==1.0.4
python3-openid==3.1.0
pytz==2018.9
pytzdata==2019.3
PyYAML==5.3
pyzmq==18.0.0
requests==2.21.0
requests-oauthlib==1.3.0
s3transfer==0.2.1
scikit-learn==0.20.3
scipy==1.2.1
seaborn==0.9.0
setproctitle==1.1.10
simplejson==3.17.0
six==1.12.0
smart-open==1.9.0
snowflake-connector-python==2.2.1
SQLAlchemy==1.3.13
SQLAlchemy-JSONField==0.9.0
SQLAlchemy-Utils==0.36.1
ssh-import-id==5.5
statsmodels==0.9.0
tabulate==0.8.6
tenacity==4.12.0
termcolor==1.1.0
text-unidecode==1.2
thrift==0.13.0
tornado==6.0.2
traitlets==4.3.2
typing-extensions==3.7.4.1
tzlocal==1.5.1
unattended-upgrades==0.1
unicodecsv==0.14.1
urllib3==1.24.1
virtualenv==16.4.1
wcwidth==0.1.7
Werkzeug==0.16.1
WTForms==2.2.1
yarl==1.4.2
zipp==3.1.0
zope.deprecation==4.4.0Do you know what might be wrong ?Having the same issue with version 12.3.0. Any ideas?I think the best way to solve it is to create a new virtual envand install the packages anew. Make sure that within myenv you are using the local pip, not the global one: looking pip freeze and verifying that it is barren prior to installing any packages should be enough.Does this just not work?Like this isn't something conda or pip can solvehttps://github.com/Azure/azure-sdk-for-python/blob/master/sdk/storage/azure-storage-blob/samples/blob_samples_authentication.py#L110if you go here this whole piece of great work doesn't work. Where is the proper way of creating an SAS token via python or is that not develop?This is something we would like to use but doesn't seem to be something that can be used.I'm having the same problem from within Azure Notebooks. Unfortunately, Creating a venv inside the notebook doesn't make sense as I'm not using python in the terminal to execute code, but rather via cells in the Azure Notebook.Note: The venv workaround posted above will not work in Azure Notebooks (as the notebook itself is hosted by Microsoft, there is no option to open it in a venv).Even more basic @Jeremy-Demlow , why doesn't this work from within an Azure Notebook:There seems to be some kind of dependency issue between azure-storage, azure-storage-blob, azure-core. Please correct this issue.Is there an active solution to this issue without creating a new env ?I dont know of one yet, but I did solve this with the method being suggested because that was an option for me. In the end I want to use docker so creating a dedicated env wasn't an issue. But this makes development very difficult and has me looking for very different alternatives because of what is indicative of.Just want you guys to know, from within an Azure Notebook, BlobServiceClient is NOT reproducable. Its hit-or-miss whether it will import:

It seems to work, THE FIRST TIME, IN A BRAND NEW NOTEBOOK, then fail on subsequent opens of the notebook.
PITAAny update on this? It has become a blocking issue on several of our notebook demos. It works at first, then fails forever.it looks like the original issue is not for azure notebook, would you mind open a new issue?Hi @ksaur
did you install azure-storage? it's not supposed to be there.
Can you paste the pip result here?I see, it appears that pip shows azure-storage-blob==12.5.0 the first run, and 2.1.0 on subsequent runs. So not a storage issue! I'll coordinate with that team. Thanks!Hi @ksaurooh okay gotcha! thanks for finding that!",None yet
https://github.com/skorch-dev/skorch/issues/573,sklearn 0.22 + latest skorch github (and also PR bugfix/sklearn-0.22)Should be solved now,None yet
https://github.com/scikit-learn/scikit-learn/pull/17015,Towards #17010.Cleaning ups to whats_new.Some cases which need fixes:Not sure:Need entry:Looks good@adrinjalali maybe we can merge this one right now and address the listed entries in other PRs? This might ease the review given that lots of stuff are moved around in this onecreated #17057. We can merge this one and move forward as you suggest @NicolasHug,None yet
https://github.com/h2oai/h2o-3/pull/4816,"See description in https://0xdata.atlassian.net/browse/PUBDEV-7702No new feature here outside the fixes listed below + the support for regression as the change of numerator/denominator types were enough to get regression working after having fixed some bugs in the logic.
Goal is to get a ""definitive"" maintainable implementation with a standard quality level API.on all clients (Py, R, Rest, Flow):on Py, R, Java:To sum up:
old way:new way:The API changes look promising - a required step. Fully agree.@sebhrusen > Those renamings are backwards compatible on Py and R: will work to get them on SW as well.
We use rest api for target encoder as well, so should be fineJust started to review, noticed related test failures @sebhrusen .@Pscheidl thanks for the pointer, R failures are expected, I didn't fix those...
Will fix the java ones asap: didn't realize there ware TE tests outside the TE module...@jakubhava @mn-mikkeNo, you won't be fine :)
The REST API is not backwards compatible, the backwards-compatibility has been enforced on client-side.
Let's have a sync with Marek so that I can explain the changes in detail and show how to make it BC in your side.@michalkurka @Pscheidl
see c0aa606 fixing the previously failing GBMEncoding and StackedEnsembleEncoding after my changes in default domain ordering in TesrFrameBuilder.
It's interesting that some encoders don't mind about the domain order when others do...There is a file named api-changes.rst which should contain all the API changes (renamed parameters as I can see so far).Really like that k and f parameters were replaced with something more descriptive!In general, I believe this PR is very good. Noticed some bad stuff that's not reallly originating from this PR and I'd not try to solve that stuff in this PR as well.I have only one big question/issue with the parameter deprecation. The old k and f arguments were bad design, yet I believe we may keep them and explicitly state in our documentation those are the same.Is there something preventing us from mapping the old parameters to the new ones internally and keep them, while marking them as deprecated @sebhrusen @michalkurka ?@Pscheidl Do you mean on Java side? I tried to keep k/f backwards compatible on client side, but on Java, it adds complexity that I found unnecessary and will add confusion on the Flow UI where both k and inflection_point will appear.
I'm open to any suggestion though.Could we simply make Flow ignore some parameters marked to ignore ? Provided Flow is the only problem.I'd personally make Java compatible as well, but we can omit that. But the clients should remain compatible from my POV. That's the only point. Is this doable @sebhrusen ?Keep the client APIs backwards compatible by deprecating k and f, yet mapping them to their new counterparts.Python and R are backwards compatible -> Approve. Provided tests are fixed of course (they will be :) )Thank you for these improvements and clean-ups.I think that since we are now restarting the development with fresh minds and objectives, it would also be good to add a TE benchmark to the benchmark suite.@michalkurka do you mean microbenchmarks? Is it ok if I just create a ticket for those now and write them only when I'm done with the integration?Hi @sebhrusen Here's my first pass -- I will do a final review tomorrow to see if I missed anything and also look more closely at the tests.It looks great! Most of my comments are on existing conventions (not directly related to the code changes in this PR), but since we are doing some refactoring / API changing, it seems relevant to bring up now (vs later/never):Is there a reason that we are using different names for the data_leakage_strategy parameters in R vs Python? We typically use the same argument string values in R/Py in H2O and it makes it a lot easier to have consistency between the two (and there's really no reason not to). Is there a way that we can get these in sync right now? Or do you see issues with doing that due to backwards compatibility, etc. To me, it would make more sense to use the Python convention (since Python has stronger conventions around argument value names) -- I assume we can always keep supporting the ""old"" R names through some simple gsub code on the client side if need be?I'm a bit confused by the use of the term response_column in Python instead of the usual y to define this column? All of our demos/code/etc use y at the user-level -- the term itself is never use in the R API (anywhere) and response_column() in Python is a method to grab the name of the column from the model object, it's not used to define it (y is always used instead). The current TE docs only talk about response_column don't use the term y at all (which is the only name in R). I see y used in Python (and R) tests, so that's good.Another API question that's not related to current PR -- At some point long ago, we had talked about using a name other than x to represent the columns to be included (since x represents ""all"" predictor columns, and here we are only defining a subset) -- I assume using x here is required because of the fact that TE is an estimator, and thus requires it? I'm not suggesting we change it, I am just curious if the name x is required because it's an estimator.Some of the argument definitions in the user guide are missing default values (e.g. blending). We are pushing a new trend of always including defaults in the user guide parameter definitions now.Are all the R tests complete? It still says ""there are still test placeholder files for R clients: this means more tests to come.""@ledell, thanks for your feedback.Yeah, this is one oddity with the Python API, nothing specific to TE here, all algos expose those response_column and ignored_columns which imo should be kept (hidden) only for internal logic.
They should definitely be removed from the doc though, it can only confuse the user.I see your point, the main benefit would be for sklearn pipelines where the distinction between X and columns_to_encode is clear. Let me create a ticket for this.
Although for now, outside the sklearn/pipeline scenario, the current use of x doesn't limit what you can do with TE, it's just that by default, if x is not specified for specified or if includes all predictors, TE will encode all categorical columns included in x.Outside the fact that it's legacy and changing those values in R is annoying, no, there's no good reason for having different names.
Backend understands both k_fold and KFold for example, this is part of the conversion logic from the client parameter as a string to the internal representation as an enum.
So I guess that the easiest way to make them compatible is to support both cases on both clients... it should be trivial.Ok, need to go through the docs again.correct, my bad! I wanted to add tests for the backward compatibility logic after the renamings. Will do....Looks good! Since this needs to be merged (as to not block/slow down other TE updates), we can address the smaller changes in a follow-up PR.synced with @mn-mikke and agreed to merge this now even if it breaks SW dev build, given the small amount of changes needed on his side: cf. targetencoder.py for an example of required changes to adapt to the new REST API.",API BREAKING-JAVA-API R algos please review python
https://github.com/r9y9/wavenet_vocoder/issues/187,"Hi, when I try to train a new model using CUDA_VISIBLE_DEVICES=""0,1"" ./run.sh --stage 1 --stop-stage 2, I get the error:Does anyone know how I might fix this?My preset json file is:For reference, I am trying to train on the FMA dataset: https://github.com/mdeff/fmaThe log clearly says what was wrong there.",None yet
https://github.com/optuna/optuna/pull/1031,"This PR is related to #1004Continue to review full report at Codecov.LGTM. Thank you!I confirmed the PR with the following colab notebook:
https://colab.research.google.com/drive/1qUr_z8B2m6bKG5E3PxPL7yFRuMnfVZ9APlease let me keep #1004 open because I originally intended to resolve deprecation without accessing private methods of scikit-learn. Sorry for the less descriptive issue.LGTM!",code-fix
https://github.com/Telecommunication-Telemedia-Assessment/bitstream_mode3_p1204_3/issues/1,"Does this have to do with the scikit-learn frozen dependency?When I comment out the dependency, I can install Scipy, but then, when installing Scikit-Learn, I get:@stg7 @rakeshraor Updating to scikit-learn 0.20.0 or higher fixes the installation issue. Can this be safely updated?I tested both under macOS and Ubuntu, as well as Python 3.7 and 3.8.solved in ca63f8d",bug
https://github.com/scikit-learn/scikit-learn/issues/15902,"svm.SVR has no _n_support but has n_support_ as its property. The other part of the member functions still use _n_support.System:
python: 3.7.5 (default, Nov 20 2019, 04:09:21) [GCC 7.4.0]
executable: /home/ubuntu/.pyenv/versions/3.7.5/bin/python
machine: Linux-4.15.0-1056-aws-x86_64-with-debian-buster-sidPython dependencies:
pip: 19.3.1
setuptools: 41.6.0
sklearn: 0.22
numpy: 1.17.4
scipy: 1.3.3
Cython: None
pandas: 0.25.3
matplotlib: 3.1.1
joblib: 0.14.0Built with OpenMP: TrueI also encountered the same error: (Windows Server 2016 platform, python 3.6.8, scikit-learn 0.22, joblib 0.14.1):My SVC classifier was trained with sklearn 0.21.3. After upgrading to sklearn 0.22, using the previously trained classifier to perform predictions, the error above occurred.Please provide a runnable code snippet that reproduces the issue.I recall that we did some changes in #15099@liminai and @jinserk a small example would be really useful to be able to reproduce to find the corner case which is not catched by the test.@jnothman and @glemaitre , hello, the following is a very simple example, train using scikit-learn 0.21.3 and save the model, after upgrading to 0.22, execute the predictions, an error occurred.Train using sklearn 0.21.3Then upgrade to 0.22, execute predictionsThe error occurredWe can see the warning message, there will be risks after the upgrade, maybe incompatible. If I can't train the new model immediately after the upgrade, what can I do to avoid this error?I guess this is not a regression @jnothman
(1) While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results, see https://scikit-learn.org/dev/modules/model_persistence.html?highlight=pickle#security-maintainability-limitations
(2) _n_support is a private attribute, so we can change it without a deprecation cycle.Agreed. See the model persistence documentation for potential solutions@jinserk Could you confirm that you also were pickling a model from 0.21 and loading it to predict in 0.22?@glemaitre Sorry for late reply. I'm on the same page with you and used the model made from older sklearn (I don't know what the version is btw). Unfortunately most of the code I have was proprietary and it was very difficult to extract the code to public. I'm making another regression model using 0.22 and will test it if I have no issue. Thank you!Use this link https://ibex.readthedocs.io/en/latest/_modules/sklearn/svm/base.html and change the _base.py file in a directory C:\Users............................\site-packages\sklearn\svm_base.py .",Regression
https://github.com/jupyterlab/jupyterlab/issues/8738,"This is related to the PyRoot kernel at https://sft.its.cern.ch/jira/browse/ROOT-10958The issue with the kernel is that it is throwing an exception and return an error message
via stderr. The actual error is not related to JupyterLab. However, the issue is that when
the kernel fails because of a kernel error, the UI stays in the state busy, and gives the user
no feedback that the kernel has failed.Debugging this issue turns out to be relatively trivial using kernelspy, and kernelspy is able
to intercept the error but the issue is that there should be some user feedback that the
kernel has failed.Also note that there are no errors in the javascript log.The other thing is that this may be an error that should be addressed kernel side. but
since the issue is jupyterlab's response, it looks like a cllient issue.This also cThis particular error can be reproduced by the instructions the ROOT-10958However, this can be reproduced by having a remote kernel send a message via stderr.When the kernel fails because of an error, the UI should process the error and inform
the user that the kernel has issued an exception and output the exception.As it is, the UI will just report that the kernel remains busy and hangs the kernel. Also
in situations where there is a kernel exception, the kernel should not hang which is the
current situation. As it is the kernel requires a restart to continue to process.LinuxFirefox / Chromium - issue is not browser related2.2.2Yes, it does indeed look like a problem in jlab that the stderr message is not getting displayed.",None yet
https://github.com/jpmml/sklearn2pmml/issues/208,"When using scikit-learn 0.22.1 in an Amazon Sagemaker python3 interpreter (that I have updated for scikit-learn 0.22, I get the following warning:Is there a plan to update for better compatibility?Please add more context to such compatibility warnings.Right now, I'm assuming that it is related to this import statement:
https://github.com/jpmml/sklearn2pmml/blob/0.53.0/sklearn2pmml/__init__.py#L3The SkLearn2PMML package aims to be compatible with all Scikit-Learn versions 0.18 and up. Will have to see if from sklearn.feature_selection import SelectorMixin works with those older versions or not.These import statements are what cause the warning to appear.As to where in the code the issue arises. will leave that to you.",None yet
https://github.com/spacetx/starfish/issues/1887,"When running your Quick Start notebook, I got an error at the image registration step that says register_translation in skimages is deprecated.As shown in the Quick Start notebookI just ran conda update --all.Thank you for trying out the Quick Start tutorial. We are aware of this issue (#1885) and it'll be fixed as part of PR #1886. In the meantime, you can use the compatible scikit-image by doing pip install scikit-image==0.15.0.closed by #1886",bug
https://github.com/VowpalWabbit/vowpal_wabbit/pull/2301,"As per #2281, the current examples are not up to date with the latest scikit-learn version. The Pull Request implements the following changes in order to fix the issue and also some refactoring has been accommodated.fixes #2281Can you please clear the outputs?@jackgerrits Output has been removed.@jackgerrits The PR is ready for review.@jackgerrits Can you please review as you get time?The import looks good.",None yet
https://github.com/hyperspy/hyperspy/pull/2203,"Cluster analysis for Hyperspy.
Implementation of k-means clustering using scikit-learn.I looked at the docs and structure of a previous pull-request for a fuzzy clustering method to try and make it possible to add other clustering methods if needed in future.Methodology largely based around:
""Cluster analysis of soft X-ray spectromicroscopy data"". - Ultramicroscopy 100 (2004) 35–57
https://doi.org/10.1016/j.ultramic.2004.01.008Thanks for this @pquinn-dls !Note that #1353 was going in this direction too a few years ago but it never got finished and I suspect @bm424 is unlikely to finish it any more given he's moved on from academia.However, that PR did have tests and things so might be closer to completion than this.Do you think it might be better to get the branch from #1353 and add to that? I'm happy to help get that old branch back in service if that's a stalling point.I've added/updated tests from #1353
I've also added agglomerative clustering as well as kmeans.
Updated docstring and user guide.
I've also fixed some elements and changed some of the parameter names.
More or less ready for review@pquinn-dls, I have sent you a PR pquinn-dls#3 to fix the tests.Thanks @pquinn-dls for this PR. After review and merge of pquinn-dls#4, this should be in very good shape!Where are we at with this @pquinn-dls @ericpre - the version at c01b706 - looked pretty close?@pquinn-dls please could you deal with the merge conflicts and then I will review this because it needs to be someone other than Eric since he contributed to it.I've updated/merged , I've updated the method to evaluate the number of clusters and fixed the tests so think its ready for reviewThanks @pquinn-dls - the diff on this is looking a bit off. Seems you're unintentionally reverting some changes that have been made to the documentation and merged in to RELEASE_next_minor since you started this. Not sure why we're having this particular issue but had the same in #2262Could you tidy that up and also the tests that didn't quite get fixed.Continue to review full report at Codecov.@ericpre , @dnjohnstone -
The plot test passes on my laptop but fails on the CI.
I had a similar issue with the cluster tests - I can only think that the random seeding isn't sticking and producing different results ? Is there a way to get more details from the CI ?A couple of comments. I have fixed the tests of this branch in https://github.com/ericpre/hyperspy/tree/clustering-pquinn-dls. There are still some failures on win 32 bits related to the ordering of the clusters (and possibly related to the seed of the random values).To get the plotting test to work, it helps to have a similar version of freetype and matplotlib as the CI. These packages are installed from the anaconda defaults channel.Regarding the merging issue, it was very strange what happen and I don't understand. I solved the merge conflict with the original branch in https://github.com/ericpre/hyperspy/tree/clustering_good - I had to unstage a lot of changes during the merge conflict...
The inconvenience of this is that I am a bit lost with what is new and beside losing the history it is a bit more annoying to review (the history is also fairly useful for the review). But now it looks in fairly good shape.Except for one issue and a couple of suggestions, this looks pretty good to me.@pquinn-dls, it would be good to have this in v1.6. If you are unable to work on this at the moment I can try sending you a PR (as long as you agree with my suggestions).I'm in meetings for most of today and my wife's birthday tomorrow so I'm unlikely to get back to this until next week.In terms of the suggestions I added the median and closest as they were requested features but I must admit I don't see a strong case for them or for inspecting different centres. Different centers could just easily come from different scaling/pre-processing or clustering algorithm choice.I also see the broader benefit of clustering is grouping and statistical averaging of similar signals.
If you have low-SNR XANES or EELS maps for example and you cluster the decomposition results to find similar spectra the resulting mean cluster centers result are statistical averages which result in a representative spectra better than any of the individual spectra. If we took the ""closest"" we'd get just extract a single noisy spectral measurement.
Similarly with diffraction patterns - I might cluster based on a peak at some position but the peak statistics might be low but the resulting cluster would be the mean of all similar patterns rather than 1 individual pattern - which will improve statistics, peaks shapes etc. which you can then process further. Closest only outputs the closest signal so will only be useful in some cases so think mean is more generic and in keeping with sklearn outputs etc.In terms of closest - it's the point closest to all other points so I think the current implementation looking over the
distances is a reflection of this.If you wish to have the changes then yes please put in the PR and I can merge. Largely I just want it in and while I'm not so keen on them either way it doesn't affect how I will use it.Thanks @pquinn-dls . I'll work on it tomorrow and send you a PR. We'll wait until early next week to release.@pquinn-dls, I'll send you a PR this morning.It took a bit longer than expected, but here is the promised PR: pquinn-dls#8@pquinn-dls, would you like to solve the conflicts?Yes just looking at them now,",release highlight type: New feature
https://github.com/scikit-learn/scikit-learn/issues/3855,"Some data transformations -- including over/under-sampling (#1454), outlier removal, instance reduction, and other forms of dataset compression, like that used in BIRCH (#3802) -- entail altering a dataset at training time, but leaving it unaltered at prediction time. (In some cases, such as outlier removal, it makes sense to reapply a fitted model to new data, while in others model reuse after fitting seems less applicable. )As noted elsewhere, transformers that change the number of samples are not currently supported, certainly in the context of Pipelines where a transformation is applied both at fit and predict time (although a hack might abuse fit_transform to make this not so). Pipelines of Transformers also would not cope with changes in the sample size at fit time for supervised problems because Transformers do not return a modified y, only X.To handle this class of problems, I propose introducing a new category of estimator, called a Resampler. It must define at least a fit_resample method, which Pipeline will call at fit time, passing the data unchanged at other times. (For this reason, a Resampler cannot also be a Transformer, or else we need to define their precedence.)For many models, fit_resample needs only return sample_weight. For sample compression approaches (e.g. that in BIRCH), this is not sufficient as the representative centroids are modified from the input samples. Hence I think fit_resample should return altered data directly, in the form of a dict with keys X, y, sample_weight as required. (It still might be appropriate for many Resamplers to only modify sample_weight; if necessary, another Resampler can be chained that realises the weights as replicated or deleted entries in X and y.)I hear this positively after discussing this very same pb with @MechCodercan you write a few lines of code the way you would like to pipe something
like Birch with an estimator that supports sample_weights?I'm not sure about piping birch with sample weights, but BIRCH could be implemented asNot that it's so neat, but it gives an example of the power of the approach. (PredictorToResampler simply takes the predictions of a method and returns it as the y for the input X.)I think we should list a few use cases to come up with an API that does the
job. The code seems a bit too generic for a single use case, which again I
acknowledge the relevance given our work on birch.I think that this issue is a core API issue, and a blocker for 1.0.
Thanks for bringing the debate.Why conflating fit and resample? I can see usecases for a separate fit
and resample.Also, IMHO, the fact that transform does not modify y is a design failure
(mine). I would be happier to define a new method, similar to transform,
that modifies y (I am looking for a good name), and to progressively out
phase 'transform'.That way we avoid introducing a new class of object, and a new concept.
The more concepts and classes of objects there are in a library, the
harder it is to understand.Finally, I don't really like the name 'resample'. I find that it is too
specific, and that their are other usecases to the method than resample
(semi-supervised learning to propagate labels to unlabelled data, for
instance).Here are suggestions of names:The name transform is just too good, IMHO. In the long run, we could come
back to it, after a couple of years of deprecation of the old behavior.
The new behavior would be that it always return the same number of arrays
than it is given (and raises an error if only X is given for a supervised
method that needs y).Modifying y is not the fundamental issue here. Yes, that's something else that needs to be handled. The issue here is that the set of samples passed out of resample is not necessarily the set passed in. This sort of operation (of which resampling is emblematic, but I am happy to find it a better name) is frequently required for training, and is rarely the right thing to do at test time when you want the predictions to correspond to the inputs.Not just the ""mostly happens [in a pipeline context] at fit time"" (and yes, as above, there are cases where a fit model will be reapplied, especially outlier detection) sets this apart from transformers that must equally apply at fit and runtime, but the idea that the sample size can change.So never mind modifying y. A transformer that allows the sample size to change cannot be used in a FeatureUnion. A transformer that allows the sample size to change cannot be used in a Pipeline unless it modifies y also because score will break, but even so it seems a strange definition of scoring a dataset if it is modified as such.So as much as redesigning the transformer API may be desirable, there is value IMO in a distinct type of estimator that: (a) has effect in a Pipeline during training and none otherwise; (b) is allowed to change the sample size, where Transformers or their successors should continue not to.The idea of the name ""resample"" is that the most important job of this class of estimators is to change the sample size in some way, by oversampling, otherwise re-weighting, compressing, or incorporating unlabelled instances from elsewhere.That's the argument that I was missing. Thanks! Are there other cases?Based on your arguments justifying the need of the new class, I've been
thinking about the name. And indeed, it should revolve around the notion
of sample, and maybe even the term ""sample"", as this is what we use in
scikit-learn. The most explicit term would be ""transform_samples"", but I
think that this is too long (we might need things like
""fit_transform_samples"").One thing that I am worried about, however, is that if we introduce a
""resample"" and keep the old ""transform"" method, it will be ambiguous what
a pipeline means. Of course, we can introduce an argument to the
pipeline, or create a new pipeline variant. However, I am worried that
the added complexity for users and developers does not justify creating
the extra object compared to the Transformers. In other tmers, I think
that we would be better off saying that some transformers change the
number of samples (and we can create an extra sub-class for that).And would this subclass of transformers also only operate at fit time? I think this is different enough to motivate a different family of estimators, but I might be wrong.This type of estimator pipelining can also be easily modelled as meta-estimators. The only real problem there is the uncomfortable nesting of param names (although I did once play with some magic that allows a nested structure to be wrapped so that parameters can be renamed, or their values tied), and that flat is better than nested.Is there a reason why a fit_transform wouldn't solve that problem?fit_transform solves that component if fit_transform and fit().transform are allowed to have different results. I think transformers are confusing enough to many users even while more-or-less promising the functional equivalence of fit_transform and fit().transform.Quite clearly I agree with you that breaking this equivalence would be a
very bad idea.But I am not sure why it would be necessary (although I am starting to
get your point, I am not yet convinced that it is not possible to
implement a transform method that has the logic necessary to have the
match between fit().transform and fit_transform).I'm preparing some examples so that we have something to point at in
discussion, but it takes longer than writing quick responses!On 17 November 2014 20:45, Gael Varoquaux notifications@github.com wrote:Thank you. This is very useful!Thanks for restarting the discussion on this.
So with implementing something that, say, resamples the classes to equal sizes during training, there are three distinct problems:The first one might be solved by changing the behavior of transformers, for the other two it is not as obvious as to what to do.
I think we might still get away with the transformer interface, though.
I would not worry too much about 3). I think raising a sensible error when someone tries that would be fine. This should be pretty easy to detect.That makes me think: what are the cases when we want different behavior during fitting and predicting? Do we always want to resample during learning, but not during prediction? What if we want to do some visualization and want to filter out outliers for both?@jnothmanAs far as I understand this discussion, (sorry if I missed something, I just quickly skimmed through, especially just the parts that say Birch :P ), you mean to subclass Birch (and other instance reduction methods) from a new class of estimators, called Resamples, and whose fit_resample method we call during the fit of Pipeline, right?. Some naive questions for starters.@MechCoderFirstly, I'm not sure that reimplementing BIRCH is what I intend here. It's more that this type of algorithm can be framed as a pipeline of reduction, clustering, etc. There should be a right way to cobble together estimators into these sorts of things in scikit-learn, to whatever extent it is facilitated by the API. As for reimplementing BIRCH itself, the resampler could be pulled out as a separate component, and the full clusterer can be offered as well.Yes, using MBKmeans for the instance reduction is equally applicable; the fact that it happens to define transform with some different semantics means that however it is wrapped as a resampler needs to appear as a separate class (somewhat like how WardAgglommeration and Ward are distinct classes).Classifiers or clusterers or regressors that happen to implement transform are a little problematic in general because, as you suggest, the semantics of the associated transformation are not necessarily inherent to the predictor, are not necessarily described in the same reference texts as the predictor, etc. For instance, despite in #2160 suggesting that for consistency all estimators with coef_ or feature_importances_ should also have _LearntSelectorMixin to act as a feature selector, I later thought the approach of the now-stale #3011 would be more appropriate, where we replace this mixin with a way to wrap a classifier/regressor so that it acts as a feature selector; alternatively, a method of a classifier/regressor like .as_feature_selector() could perform the same magic. The idea is to more clearly separate model and function.@amuellerDid you mean (2)?I think this is a key question. Certainly there must be a way to reapply the fitted resampling where appropriate; visualisation is a good example of such. Yet perhaps this is no big deal to expect users to do without the pipeline magic.@jnothman yes, I meant (2).
Sorry, I'm not sure I understand your reply.
What do you mean by ""without the pipeline magic""? That users should not be able to use pipline in this case? Or that the heuristic of not applying resampling for predict, score or transform should be the default but there should be an option to not use this heuristic?Btw, this heuristic gives me no option to compute the score on the training set that was used, which is a bit odd.I'm not entirely happy with it, but I've mocked up some examples (not plots, just usage code) at https://gist.github.com/jnothman/274710f945e311697466I mean that currently there are cases where Pipeline can't reasonably be used. It's particularly useful for grid searches, etc., where cloning and parameter setting is involved, while requiring the visualisation of inliers to not use a Pipeline object probably doesn't hurt the user that much.I agree it's a bit upsetting that this model would not provide a way to compute the training score.To summarize a discussion with @GaelVaroquaux, we both thought that breaking the equivalence of fit().transform() and fit_transform might be a viable way forward. fit_transform would subsample, but fit().transform() would not.I think it's time to resolve this. We are already breaking fit_transform and transform equivalence elsewhere.But are you sure we want to allow fit_transform to return (X, y, props) sometimes and only X at others? Do we then require transform to return only X or is it also allowed to change y (I think we should not allow it to change y; it is a bad idea for evaluation).We also have a small problem in pipeline's handling of fit_params: any fit_params downstream of a resampler cannot be used and should raise an error. (Any props returned by the resampler need to be interpreted with the pipeline's routing strategy.) Indeed maybe it is a design fault in pipeline, but the handling of sample props and y there assumes that fit_transform's output is aligned with the input, sample for sample.I find these arguments together compelling to suggest that this deserves a separate method, e.g. fit_resample, not just an option for a transformer to return a tuple that results in very different handling. I do not, however, think we should have a corresponding sample method (and find imblearn's Pipeline.sample method quite problematic). At test time, transform should be called, or else we could consider all resamplers to perform the identity transform at test time. (On objects supporting fit_resample, fit_transform should be forbidden.)Let's make this happen.I think for now we should forbid resamplers from implementing transform, as the common use cases are identity transforms, and allowing transform is then possible in the future without breaking backwards compatibility.Proposal of work on this and #9630:ImplementationDocumentationI'm happy to open this to a contributor (or a GSoC) if others think this is the right way to go.Thanks Joel. I'm pretty sure I understand what you are after. Essentially:Some fit_params (eg sample_weight and perhaps others) are tied to the samples and must be accordingly modified by the resamplers for any downstream estimators. Let's call these sample_props.A backwards compatible implementation that would involve the least changes to the API (existing transformers, estimators, etc) could be as follows.Add an optional parameter to the pipeline fit routines called sample_props. This is a list of strings which correspond to keys of the fit_params that are sample_props and need to be modified by the resamplers. The relevant parameters can then be passed and updated by the resamplers. The pipeline _fit routine is modified to get the relevant fit_params at each step as follows::No rush on making this architecture decision, but I'd like to have a plan before moving forward with writing the code.edited to use pop to avoid recalculating sample_props for upstream transformersFair enough. I suppose modifying the pipeline was part I found most interesting. In any case, you're ok with this as the rough outline for the API?+1 for only having fit_resample defined in the Mixin. I don't recall any use case having only resample.Regarding the Pipeline implementation, I think that the changes done in the imblearn implementation should make the trick or at least a good start. It will remain the issue regarding the props handling.Regarding the handling of the sample_props in the resampler itself, it looks like @dmbee would go in the same direction than what we thought to handle sample_weight:
https://github.com/scikit-learn-contrib/imbalanced-learn/pull/463/files@dmbee do not hesitate to retake some of the code/tests of imblearn. You can also ping me to review the PR. I'm going to become active again in scikit-learn from next week.Above, it was not my intent to support resampling at test time. I understand that is out of scope here / niche.Allowing separate fit / resample methods (instead of just fit_resample) does not affect how the resampler is used in pipeline or the complexity of the pipeline in my view (see my pipeline code above). It came to mind that it may be useful to separate the fit and resample methods for some potential use cases (eg generative resampling).However, if this capability is not desirable we can use a very similar API to imblearn adding handling of sample_props which is straightforward if the resampler is just indexing the data. See rough example below.Let me know your thoughts?@glemaitre - thank you. I certainly have no desire to replicate what has been done in imblearn (I like and use that package by the way!). It seems the main thing lacking atm from imblearn is the pipeline / resampler changes required to support sample_props. Otherwise, it seems very compatible with sklearn.I don't really like that we're committing to a format for the sample props here in a sense, but I guess it's not that different from the handling of fit_params in the cross-validation code right now. So I think it should be good do go.Actually, you should take whatever works for scikit-learn from imblearn. Our idea is to contribute whatever is good upstream and remove from our code base. We are at a stage that the API start to be more stable and we recently made some changes to reflect some discussions with @jnothman.Bottom line, take whatever is beneficial for scikit-learn ;)OK good to know - thanks. I should have used re-implement rather than replicate. It seems most things from imblearn can be readily ported.Not sure what a SLEP is....A SLEP is an (under-used) Scikit-learn enhancement proposal. https://github.com/scikit-learn/enhancement_proposals/Regarding the API proposal in #3855 (comment), yes, that approach to resampling looks good... Not all estimators supporting fit_resample can do so from indices, but I think you are aware of that; sample reduction techniques will not, for instance.OK good stuff. Yes - I am aware that not all resamplers will sample from indices. Those that do not will have to either implement their own method of dealing with props (if there is a sensible option - hard to know for sure without knowing what will be in props) or otherwise raise a notimplemented error if props is not none.Is anyone in Paris working on this? I'd be happy to help (and the api would be useful for fitting semi-supervised classifiers, as discussed in this Review.)Sure, where can I find you?I started working on this (I'm not in Paris), but got too busy over the last couple of months. I am happy to share what I have done already, and continue working on it.I'm starting work on this now for the sprint. If you have existing work that you think could be useful, I'd be more than happy to build on what you've done.here it is - look at resample folder in sklearn
https://github.com/dmbee/scikit-learn/tree/dmbee-resampling",API Moderate New Feature help wanted
https://github.com/randaller/cnn-rtlsdr/issues/1,"It would be great to have some instructions how to install this on Linux.There are some DLLs mentioned in the instructions and also I think some stuff from requirements.txt might not apply to Linux.There are no differences in installation process except of CUDA dlls, you may wish to install linux versions of cuda following https://www.tensorflow.org/versions/r1.2/install/install_linux under ""NVIDIA requirements to run TensorFlow with GPU support"" section. If you decide to use cpu version, you may skip this.Not really all requirements.txt are required, i just did ""pip freeze"", including tons of my other installed libraries, then cleaned up some lines manually. All the rest should be in pip repo and easily installed on Linux or Mac with ""pip install"". If some lib still says ""it's unavailable"", you may try to skip it, may be it's not really needed. Only a few main key libraries are really needed, such as numpy/tensorflow/rtlsdr/keras, that's all exists for all of operating systems zoo.I have Tensorflow-gpu and a complete set of typically used libraries on Ubuntu 16.04 which works for pretty much anything machine learning related that needs TF. It did not work on this occasion. I rebuilt a new virtual environment with the requirements-text list. I still does not work and throws the same error so two different set-ups throw the same error. I don't think it is the RTL-SDR in my case either it seems to work. Tensorflow does not like reading the 1200000 byte length and throws error code -8. So I just used the sample rate as is, that has now lead to new errors with the numpy array reshape function. I am just going to need to plod through, but the predict_scan.py is not working on Ubuntu as of today. I have a 12Gb GPU so its not out of memory. If anyone has any fresh insights that would be welcome. But I suspect some major rework on predict_scan.py is required at a guess to satisfy my particular case.FYI the requirements-text: many of the file pointers are ahead in release numbers for linux so its asks for versions not yet released, just remove the version requirements where it breaks(all the ==x.x.x) and it will pull the latest available linux release. Some files are also windows only, just comment them out.Nightmare on Elm Street! DO NOT install requirements.txt if you have Anaconda already installed!As I was in a virtual environment after installing the requirements.txt, it wasn't apparent that it had 'smoked' Anaconda completely. The conda command refused to work and gave errors about being installed by pip. It was as I discovered irreversible. I have to destroy all my virtual environments and completely delete Anaconda owing to a bug with openssl so I could not write over the old install at all. I had a war with tensorflow-gpu and CUDA which I resolved eventually. I have now tensorflow-gpu running on 1.5.0 and CUDA running 9.0. Still it produced the same error. Apart from scipy and pyrtlsdr I think it didn't need much else to get it back to where I was previously. What a day!I have tried to install it on Ubuntu 16.04 x64 and stuck with the same error -8 while reading from rtlsdr. Probably it should work somewhere, as spoken in pyrtlsdr bugtracker, but not with my dongle.Its not a dongle thing as far as I can see. I am going through to see where it breaks. Before this its failed after the arguments. The dongle itself is a one shot thing it must be closed and opened again from what I have seen slowing things down. It could be pyrtlsdr, its on the list. I will read the issues. Thanks for getting back at least we see the same issue thats a start :)Did the same with latest Kali Linux, just with some modifications:And got the same:
OSError: Error code -8 when reading 1200000 bytesSo I'm pretty sure it's pyrtlsdr/my dongle error, because gqrx/gnuradio works well on both Ubuntu and Kali.ok. I have a HackRF unit as well but I only got the gear this week so I have had a steep learning curve ahead. I could try that unit but bending the code might take some time. I note the error we have is an 'OS' error not 'IO' error as reported in pyrtlsdr issues, at least that's what I read so far. I did an rtl_test it seems fine. I even got it to produce audio, but when it comes to using python and the RTL that's where the wheels are falling off using Linux.Finally I figured it out. Rtl-sdr driver under Linux won't return read_bytes(value), if the value is not divides to powers of 2 (or multiple to read buffer size) or so. Rewriting read_samples() a bit - solves the problem and then it works well.I have noticed, that the predicted signal class at frequencies was shown as 'tv', not as correct 'wfm'. It is ok here :), due to folders structure. I am recommending not to use outdated first version, but prefer keras one. It should run also well, just do not forget:Fabulous! I have mine working on the GPU at last. Thanks for the update. It had to be a data thing. I got nothing at first so I changed the antenna for something better. My high gain antenna will need moving, the number cruncher is too far from that, its raining, its probably not happening today. Running predict its absolutely convinced that every station it has received on the FM band is DMR which I interpret as Digital Microwave Radio, if thats what you mean, it will not be at ~100Mhz. I have some 'real' receivers I will take a listen as see what it has got mixed up on. But I suspect I will need to train this to local conditions, lets see how that goes. Keras is installed!But it's still finding wfm stations on it's correct frequencies, right? :) Yes, it's better to train new model using your local antenna etc. Ok, here are some quick steps to run keras version now:First, edit read_samples() function inside of [prepare_data.py] (to avoid driver errors under Linux): replacewithThen scroll down [prepare_data.py] to findlines. Edit all of them, set frequencies and labels according to your local ether, save the file and then run it. It should collect some training samples. Sample .npy files should be 200,080 bytes each.Before running [train_keras.py] you also need to edit a bit. Edit read_samples() function same way like we did with [prepare_data.py] :). Then scroll down file till the end, to findlines. Also edit all of them - type there also frequencies and correct labels, just use DIFFERENT frequencies, that are not stored in [prepare_data.py] - trained model should be evaluated using new data from frequencies, that neural network never seen before.I more or less did that I got it to create the .npy files, but I will do your method verbatim. I hit a snag. Given the time to preprocess, could tell me what the len(*.npy) should look like at the np.shape of *.npy. I had the training blow a gasket trying to reshape it to (128,2). It was saying it was a list of 54468 or something like that size not a numpy array during training. I took a look at the *.npy files they were complex number arrays and the iq_samples generated by preprocess were floats. But the *.npy were only half the size at around 25xxx bytes I don't have a baseline to compare fo what size and shape I should have but I know it will be 2D into keras. I will groom this again tomorrow with the above, we are out of sync in time zones, I suspect its on the second round now so I won't disrupt it. ....CheersI have been around in circles. I thought I had an SNR issue when I started training my models. It was all over the place. I have wripped all the training and testing files I did out and put yours back in. It seems there was only 1 *.npy file in each folder, I was creating dozens of them in my training for some reason. So now I am studying your original output again, to see what I can do about SNR or if its a FM format issue I think our RDS carrier is different to the U.S model in any case there must be minor differences or the prepare.py has placed the WFM files in the DMR folder on your end as it has located all the local FM stations but labeled them as DMR. There are stations on each freq I checked, so it seems the label is wrong. Still checking to see what is going on.ok This is what I have before I forget:step-by-stepafter fit().Dataset.py:94 error tells me, that you has a very little amount of training samples (less than 16). I repeat, you need a lot of, for example, 0.5-1 Gigabyte per each class, which is equal to 2000-4000 .npy files per class. Each npy file contains 12500 I and 12500 Q values interleaved, stored as 64-bit floats.Also, neural network does not cares about RDS or anything that are modulated inside, as we didn't extracting such features, network just learns for ""projected shape of cutted signal"". And yes, SNR is very important, especially while working with signal from rtl-sdr dongles, I recommend to always set dongle gain to 'auto', before you completely understand of what's going on in the software.ah right I see. It makes sense sure I have the model, but I assumed I had all the original *.npy too. So the missing ingredient is all the *.npy files. At no time has the GPU broken a sweat doing this stuff at all, super quick. Ok I will run preprocess tomorrow blow out the folders and check point. I just need to find some more interesting stuff the feed it. CheersOh, that's the great deal of work. After the whole things ran successfully, shrink bandwidth and classify different FSK/BPSK`s. Classify multiple signals presence at time. Feed CW samples to LSTM network and decode Morse. Decode voice from samples. :) Decode patterns from stars. Automaticaly choose the best modulation mode while communicating digitally...My 40m dipole is upstairs, that does well on WWV and HF but I am not lugging all of this upstairs. In any case I want to bend the code so I can do more QAM demod with the hackRF unit in the ISM bands. I could use the RSA306 but Tektronix never produced much info a driver of the IQ - too hard basket. So once I am comfortable with this RTL I will try and get the other working. I did voice using Wavenet I was feeding DTMF into it as a MNIST style test. Strange results not what I would have expected. But this is more predictable.I modified here on train_kerasdef read_samples(freq):
f_offset = 250000 # shifted tune to avoid DC
sdr.center_freq = freq - f_offset
time.sleep(0.06)
#iq_samples = sdr.read_samples(sample_rate * 0.25) # sample 1/4 sec
iq_samples = sdr.read_samples(1221376)
iq_samples = iq_samples[0:600000]
fc1 = np.exp(-1.0j * 2.0 * np.pi * f_offset / sample_rate * np.arange(len(iq_samples))) # shift down 250kHz
iq_samples = iq_samples * fc1
return iq_samplesand here onlycheck(88700000, ""wfm"")
check(492500000, ""tv"")
check(89600000, ""wfm"")
check(104900000, ""wfm"")
check(1080000000, ""tetra"")
check(100500000, ""wfm"")
check(120000000, ""other"")
check(592500000, ""tv"")
check(827500000, ""3G"")
check(662500000, ""tv"")and it just tests everything as 'tetra'88.7 tetra 99.95344877243042
492.5 tetra 99.93725419044495
89.6 tetra 99.94871616363525
104.9 tetra 99.9605119228363
1080.0 tetra 99.7898817062378
100.5 tetra 99.94792342185974
120.0 tetra 99.95473027229309
592.5 tetra 99.92196559906006
827.5 tetra 99.89325404167175
662.5 tetra 99.91363883018494still no check point files generated. I must have missed something. I changed prepare it created all the *.npy files ok and train_keras trained ok for 50 iterations it looked fine. Not sure what I missed as it not throwing errors.I am not sure what the role of data2 does btw there maybe an issue in there, it references iq_samples but nothing from the RTL_SDR specifically i could see.This is from prepare_data line 44 +collect_samples(940000000, ""tetra"")
collect_samples(92700000, ""wfm"")
collect_samples(872500000, ""3G"")
collect_samples(606500000, ""tv"")These are the directories in training _data
3G other tetra tv wfmand testing_data
3G other tetra tv wfmthere's 750 files in each directory e.g. ~/cnn-rtlsdr/training_data/wfmNow clearly thats not 2000 thats all it created so I must need to change collect_samples?Testing_data folder is not really needed for keras version, as it takes 30% from training_data to use as testing_data. What are keras latest statistic values after training? loss, acc, val_loss and val_acc? Did you set ppm value correctly?Sharing my training data, that should give: loss: 0.01, acc: 0.9971, val_loss: 0.0193 and val_acc: 0.9961. Just did a fresh train (on windows) and it has correctly predicted fresh signals from ether.
https://drive.google.com/open?id=1PuhzXkk6AVwXPPKjtFUCpQVsqOOlszu8My tv carrier is SECAM so it would differ, but network should predict tetra & wfm correctly on your ether......and the answer is...h5py! A casualty of requirements.txt disaster where I had to rebuild the system from scratch thus losing many libraries I had built in over time. So where does that leave us?So its a mystery during training that keras_train come up with half decent answers but train.py said its all tetra. But I can't run predict on the h5py model as is without changes to truly run keras out on its own.I removed compile and fit and run it, its a bit all over the place with predictions. SECAM is an old analog european standard we have digital TVB, also gsm is gone here I think, its wideband 3g or 4g so I am sampling multi carrier or cdma wideband QAM systems not analog. So I will hit it with more samples today which takes hours to do. I am not sure I have the gain right. I have a preamp on the rtf-sdr it may be causing intermod on some signals, hard to say with digital sounds ok on fm. So if that fails to predict accurately I will just concentrate on getting fm right as a baseline, then attempt to adjust for QAM based signals. The decimation maybe to harsh with signals spanning more than 5 Mhz, I may need more sample bandwidth.Hi
I did the same thing that you do and didn't get any output from my dongle. I used UBUNTU 16.04 and install Anaconda. after that I installed the Requirement.txt packages and RTL-SDR driver. after that I changed the read_samples function and run the predict_scan.py but as you see nothing happened and get none in result.
What's can be wrong ? Can somebody please help me?
@randallerThose are just info and deprecation warnings. It may not have detected a signal at all.First I would do a sanity check as follows on the command line:rtl_fm -f 92.7e6 -M wbfm -s 240000 -r 48000 - | aplay -r 48k -f S16_LEChange 92.7e6 for a local station you have. Do you hear sound? If so proceed next. I can see it has PLL warnings so it should be ok. If not sound card issues not our problem with raw samples but you should get the radio working its essential. Check you have 'aplay' check 'stack overflow' for more details.I would insure you have h5py installed. Use 'pip freeze' on the cmd line, look for h5py==2.7.0.Are you using Python 3.6? I can't speak for 2.7. can't say it will or won't work but given the number of print statements in code you would hit some errors very quickly. Did you install Tensorflow for Python 3+, I see its cpu only no issue with that, certainly easier. You could be running Py 3.6 on the wrong tensorflow. Fire-up python interpreter and issue 'import tensorflow'. Did that work? If not, sort that out.I would did a 'pip freeze' check against requirements.txt. Also you may want to use 'conda list' and see what anaconda installed.That should keep you busy for a while. Also I have a weak signal I would take the RTL stick off 'auto' in the prepare_data.py code and replace with 60 which as far as I can tell is max gain (line 42 in my code).Try that!I see python 3.6 being used, signal.decimate() tells this. After the check, ensuring that rtl-sdr works well with other applications, and if cnn-rtlsdr still ends silently, try one of my ways to install environment on a clean Ubuntu or Kali, edit the function, and check again, it really should detect at least wfm at it's places. Otherwise, I don't know more what could help, may be inserting debug print( ) after each block of code to see what's happening line by line.So I have followed all the steps, when I run Python_predicts this is where I am stuck..~/cnn-rtlsdr$ sudo python3 predict_scan.py --start 850000000 --stop 860000000 --threshold 0.9955
Detached kernel driver
Found Rafael Micro R820T tuner
[R82XX] PLL not locked!
2018-11-15 07:54:51.714785: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
/usr/local/lib/python3.6/dist-packages/scipy/signal/_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use arr[tuple(seq)] instead of arr[seq]. In the future this will be interpreted as an array index, arr[np.array(seq)], which will result either in an error or a different result.
b = a[a_slice]
/usr/local/lib/python3.6/dist-packages/scipy/signal/signaltools.py:3463: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use arr[tuple(seq)] instead of arr[seq]. In the future this will be interpreted as an array index, arr[np.array(seq)], which will result either in an error or a different result.
return y[sl]
Reattached kernel driverthe sdr works as I have ran it through GQRX but I cant seem to get past this..any help please?
any help would be appreciated also is the instructions for ubuntu all the commands that are needed?It has just found nothing in ether. Commercial FM is usually at 88-108 MHz. Try with --start 88000000 --stop 108000000It is much better to use fresh keras version of neural network.Awesome, thank you for your help. So with Keras will it be able to reconize dmr. Also when I run it I get this error.etached kernel driver
Found Rafael Micro R820T tuner
[R82XX] PLL not locked!
Traceback (most recent call last):
File ""train_keras.py"", line 112, in
check(92900000, ""wfm"")
File ""train_keras.py"", line 76, in check
iq_samples = read_samples(freq)
TypeError: read_samples() missing 1 required positional argument: 'freq'
Reattached kernel driverthank you for your helpCommands for fresh installed Ubuntu 18.04.1 LTS, desktop version:Detached kernel driver
Found Rafael Micro R820T tuner
[R82XX] PLL not locked!
2018-11-26 15:26:04.158066: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
/usr/local/lib/python3.6/dist-packages/scipy/signal/_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use arr[tuple(seq)] instead of arr[seq]. In the future this will be interpreted as an array index, arr[np.array(seq)], which will result either in an error or a different result.
b = a[a_slice]
/usr/local/lib/python3.6/dist-packages/scipy/signal/signaltools.py:3463: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use arr[tuple(seq)] instead of arr[seq]. In the future this will be interpreted as an array index, arr[np.array(seq)], which will result either in an error or a different result.
return y[sl]
87.500 MHz - dmr 99.99%
88.000 MHz - dmr 100.00%
88.900 MHz - dmr 100.00%
89.700 MHz - dmr 100.00%
90.100 MHz - dmr 100.00%
90.500 MHz - dmr 96.31%
90.600 MHz - dmr 100.00%
91.100 MHz - dmr 99.88%
91.500 MHz - dmr 99.99%
92.900 MHz - dmr 99.61%
95.000 MHz - dmr 100.00%Which is pretty ok, just showing dmr instead of correct wfm label due to training_data/ folder contents.And to continue to keras version:To use developer data:or, to use own dataNow we are ready to run keras version:I followed the instructions but it doesn't work, getting Illegal instruction. please help. Thanks
I have latest version of kali linux
root@kali:~/cnn-rtlsdr/cnn-rtlsdr# python3 predict_scan.py
Illegal instructionHi,
having had so many errors in windows and even more errors in kali linux I just installed ubuntu to make a fresh start but still having problemsroot@udd-K73E:/home/udd/cnn-rtlsdr# python3 predict_scan.py
/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
return f(*args, **kwds)
Detached kernel driver
Found Rafael Micro R820T tuner
[R82XX] PLL not locked!
2019-03-05 18:02:32.018359: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2I googled the issue and found that I should install an older version so I installed tensorflow1.4 but still getting errors...root@udd-K73E:/home/udd/cnn-rtlsdr# python3 predict_scan.py
/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
return f(*args, **kwds)
Detached kernel driver
Found Rafael Micro R820T tuner
[R82XX] PLL not locked!
2019-03-05 17:22:15.758252: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2
^CTraceback (most recent call last):
File ""predict_scan.py"", line 59, in
iq_samples = read_samples(sdr, freq)
File ""predict_scan.py"", line 14, in read_samples
iq_samples = sdr.read_samples(1221376)
File ""/usr/local/lib/python3.6/dist-packages/rtlsdr/rtlsdr.py"", line 497, in read_samples
iq = self.packed_bytes_to_iq(raw_data)
File ""/usr/local/lib/python3.6/dist-packages/rtlsdr/rtlsdr.py"", line 514, in packed_bytes_to_iq
iq /= 127.5
KeyboardInterrupt
Reattached kernel driverwould appreciate some help. thanks newbeeHiWhat remains to be sorted out for it to work under Ubuntu 16.04/18.04?",None yet
https://github.com/scikit-learn/scikit-learn/issues/12401,"Our tSNE implementation uses squared Euclidean distances by default, but does not square the distances when other metrics, or precomputed data, are provided. We had no certainty about whether the theory underlying tSNE was even valid for non-Euclidean metrics (which was raised in the partial rewrite before version 0.19) but if it is valid I would think using a matrix of squared pairwise distances would be more appropriate than raw distance matrices. I think we should at least facilitate squaring the distance matrix, if not deprecate the default behaviour.I propose:I'd like to work on this.I am not sure if tSNE should be used for non-Euclidean distances. Could not find anything
in the literature. Nevertheless, if there is agreement to the steps you suggested, then you can take a look at my PR.
I have marked it WIP until a common consensus is reached. Will add pytests and more documentation if the approach seems okay.Thanks!I agree.Hi Joel,I have made the respective changes as per your recent comments, but I am struggling with unit tests as there are a few unit tests failing in K-means which are completely unrelated to my changes. I have been trying to solve them for last week but couldn't get any luck.Could you suggest me what is the required course of action here? Should I just upload my changes and wait for the other unit tests to be fixed?Thanks,
AnuragI have no idea why unit tests might be failing in k-means. There are no such failures in master on our continuous integration servers. I do think you should upload your changes so that we can see what fails on Travis and can help you resolve them. (Please discuss further pull request related discussion at #12524, not here.)Seeing that #12419 is stalled, I'm interested in picking this up.I see that the help wanted tag was removed by @adrinjalali, though. Is this enhancement still wanted? Thanks! :)Feel free to take it @joshuacwnewton . The label was removed in 2018, and this issue was just never triaged after that :DTakeI have a naive question here but why would squaring the distance matrix be useful ?
For euclidean it makes sense because we first compute the squared distance matrix and then take the square root.We don't. You're right, for euclidean we compute probas given squared distance matrix and for other metrics we compute probas given vanilla distance matrix. I'm not familiar with the maths but at some point t-sne calls _binary_search_perplexity which seems to expect a matrix of squared distances.Thanks for the clarification. Now I'm not sure we even want to keep the current behavior but replace it by always square :)I'm having trouble coming up with a justification for including square_distance=False.square_distance='legacy' already covers the legacy behaviour, so having False as an additional option implies that there's a valid reason for someone to intentionally not square the distances. But, is there?See also this related comment by @jeremiedbb.I'm fine to have square_distance='legacy' and then change default after deprecation period.Indeed, we could rename it to sqrt_distance reflecting the implication of the current behaviour.Or maybe users wouldn't mind too much if we just changed behaviour as a bug fix???That's where my thoughts were heading too! If we don't see a reason for False, and the default eventually becomes True, then why give a parameter at all? :)Thank you for sharing your perspective! It's been helpful for understanding sklearn's philosophy on adding/removing parameters. :)The only thing I'm concerned about is the code written by users in the interim period. With the current FutureWarning as included in #17662, users are nudged into explicitly setting square_distance=True to suppress the warning.Say in a future version, sklearn removes square_distance and switches to squaring by default. That would require users to then go back and remove the parameter they set.Is this an issue? Or, is this back-and-forth okay as long as enough time is given? (e.g. through 2 periods of Warning -- 1 for the initial change, and 1 for the deprecation.)Based on your feedback, then, I think I like keeping the proposal for introducing (then later removing) a square_distance parameter with options {True, 'legacy'}.I've only just realized, one other upside of a slow rollout is that once a version with a FutureWarning is deployed, users will have a chance to provide feedback if they object to the change. Perhaps a researcher intimately familiar with niche use-cases of t-SNE sees the warning and has insight to share, e.g. ""for [insert metric], I need the ability to keep the distances un-squared.""Thanks much for all of the replies. :)Actually, this could be a reason to have 'True', 'warn', and 'legacy'. In that case, explicitly setting legacy would be another way to suppress the warning.With just True and 'legacy', then there would be no way to suppress the warning and still keep legacy behavior.That's a good point!I guess what I meant was, this hypothetical person might be able to better prove why/why not squaring is necessary if they had an opinion to share, and happened to raise an issue about it. Perhaps that's wishful thinking, though! :)Here's the part in the original paper where the distance matrix is converted into a similarity matrix:The squaring comes from the gaussian distribution. I'm pretty convinced that the current behavior is a bug, so I agree that only have square_distances={True|'warn'|'legacy'} is best.According to the author, instead of computing the similarities based gaussian distribution you can directly provide your own similarity matrix. Maybe we could add this possibility if users really need more flexibility.I'm thinking that we should deprecate 'legacy' right away. Then the plan isWhat do you think ?Just to demonstrate, then, from 0.24-0.26 the warning would look something like:And from 0.26-0.28 the warning would look something like:I think that seems reasonable. But, as a new contributor, happy to hear other opinions. :)Good to hear. I've made the changes to remove'legacy' by 0.26 in 923c90d. :)@NicolasHug had some thoughts about the 0.26-0.28 deprecation plan which were shared in this PR review comment. Referencing here so that the comment doesn't get buried inadvertently in the rest of the review.",API Easy Enhancement
https://github.com/microsoft/LightGBM/issues/967,"Hi Team,This package is awesome, and we use it a lot. It's become our default gradient boosting implementation at work. The training speed in particular is astounding.I was disappointed to find that the latest release had two changes that broke our existing code: The type of data inputs accepted changed, throwing ValueErrors, and the .best_iteration attribute disappeared, throwing an AttributeError. Unfortunately, I remember something pretty similar from the previous patch release, though I forget exactly what the issue was now.I'm all for progress! One of the things I like about this package is how rapidly everyone's developing on it. But it's marked as 'Development Status :: 5 - Production/Stable', and that means that code that's written using lightgbm one patch ago (2.0.6) should definitely work with this patch (2.0.7).Generally, I'd expect Deprecation warnings, or FutureWarnings, or UserWarnings for at least 2 minor versions (2.0.x - > 2.2.x), and ideally one major version, before a release that breaks existing codebases.I know that it's a bit more work. It possibly means setting up a test environment that runs your tests on multiple releases of your code. It definitely means a couple more if statements to check for things like if the user is trying to access a soon-to-be-removed attribute, and returning to them the new attribute, and a warning.But that's what you signed up for with 'Development Status :: 5 - Production/Stable'. If you want to be able to release breaking changes, there are other Development Statuses for that, and we'll adjust our code to be more cautious accordingly.Again, thanks for the great package! It's vastly sped up our iteration speed at work; it's as accurate as anything else out there that we've found; it's got great use in analytics projects too with .feature_importances_. I'm a loyal evangelist.Thanks for your suggestions. We don't have much experience on this and will be careful in the future.Btw, what's exactly the two changes you refer to (The type of data inputs accepted changed & the .best_iteration attribute disappeared)? Do you mean in sklearn interface?yeah, we should define and follow versioning rules like PEP 440 and Semantic Versioning.Because of lacks of experience and human resource, we couldn't take care it, but we wlll do our best in the future.
Thank you.Sorry that you had problems with LightGBM's backward compatibility. From my side I confirm thatAs a typical user, I can remind you that you can fix versions of dependences of your software. It's very useful when you have dozens of them. And then you periodically do review of dependences and in case of ""all OK"" increment the version of the corresponding dependence. Of course, you know it, just reminder.Thanks for the responses, y'all!@wxchan you nailed it- i'm using the sklearn interface.as always, with open source software, it's all a bit buyer beware, but i thought i'd mention it so y'all knew how much people were using and depending on your project.thanks always for the great work. we're very glad to be using it :)",None yet
https://github.com/mitre-attack/tram/issues/33,"TRAM version - 0.5
Python version - 3.7.6
OS - Ubuntu 18.04 LTS(TRAM-0.5) warhawk@lab:~/tram$ python tram.py
INFO:root:Welcome to TRAM
DEBUG:asyncio:Using selector: EpollSelector
INFO:root:Downloading ATT&CK data from STIX/TAXII...
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cti-taxii.mitre.org:443
DEBUG:urllib3.connectionpool:https://cti-taxii.mitre.org:443 ""GET /stix/collections/95ecc380-afe9-11e4-9b6c-751b66dd541e/ HTTP/1.1"" 200 249
DEBUG:urllib3.connectionpool:https://cti-taxii.mitre.org:443 ""GET /stix/collections/95ecc380-afe9-11e4-9b6c-751b66dd541e/objects/?match%5Btype%5D=attack-pattern HTTP/1.1"" 200 994431
DEBUG:urllib3.connectionpool:https://cti-taxii.mitre.org:443 ""GET /stix/collections/95ecc380-afe9-11e4-9b6c-751b66dd541e/objects/?match%5Btype%5D=intrusion-set HTTP/1.1"" 200 198576
DEBUG:urllib3.connectionpool:https://cti-taxii.mitre.org:443 ""GET /stix/collections/95ecc380-afe9-11e4-9b6c-751b66dd541e/objects/?match%5Btype%5D=malware HTTP/1.1"" 200 429055
DEBUG:urllib3.connectionpool:https://cti-taxii.mitre.org:443 ""GET /stix/collections/95ecc380-afe9-11e4-9b6c-751b66dd541e/objects/?match%5Btype%5D=tool HTTP/1.1"" 200 68856
DEBUG:urllib3.connectionpool:https://cti-taxii.mitre.org:443 ""GET /stix/collections/95ecc380-afe9-11e4-9b6c-751b66dd541e/objects/?match%5Btype%5D=relationship HTTP/1.1"" 200 5644298
INFO:root:Finished...now creating the database.
INFO:root:[!] DB Item Count: 630
INFO:root:[] Found punkt
INFO:root:[] Found stopwords
INFO:root:server starting: 0.0.0.0:9999
INFO:aiohttp.access:127.0.0.1 [29/Jan/2020:02:19:21 +0000] ""GET / HTTP/1.1"" 200 9084 ""http://localhost:9999/?"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36""
INFO:aiohttp.access:127.0.0.1 [29/Jan/2020:02:19:22 +0000] ""GET /theme/images/favicon-blue-m.ico HTTP/1.1"" 200 23171 ""http://localhost:9999/?"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36""
INFO:aiohttp.access:127.0.0.1 [29/Jan/2020:02:19:28 +0000] ""GET / HTTP/1.1"" 200 9084 ""http://localhost:9999/?"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36""
INFO:aiohttp.access:127.0.0.1 [29/Jan/2020:02:19:28 +0000] ""POST /rest HTTP/1.1"" 200 160 ""http://localhost:9999/?"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36""
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.fireeye.com:443
DEBUG:urllib3.connectionpool:https://www.fireeye.com:443 ""GET /blog/threat-research/2020/01/saigon-mysterious-ursnif-fork.html HTTP/1.1"" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.fireeye.com:443
DEBUG:urllib3.connectionpool:https://www.fireeye.com:443 ""GET /blog/threat-research/2020/01/saigon-mysterious-ursnif-fork.html HTTP/1.1"" 200 27970
[#] Loading models from pickled file: model_dict.p
/home/warhawk/anaconda3/envs/TRAM-0.5/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.1 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)
/home/warhawk/anaconda3/envs/TRAM-0.5/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)
/home/warhawk/anaconda3/envs/TRAM-0.5/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.1 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)",None yet
https://github.com/EpistasisLab/tpot/issues/981,"It runs with FutureWarning while from tpot import TPOTClassifier, TPOTRegressorJust below.as the context above.It seems a compatibility issue with the latest scikit-learn 0.22 since there is no warning message with scikit-learn 0.21.3. But TPOT should work with those warnings. We will fix it in next version of TPOT.The issue was fixed in the latest version of TPOT, thus I closed this issue. Please feel free to reopen this issue if you have any questions.I believe that, with the newer version of scikit-learn, the public API is the only one supported, and that corresponds only to the top-level modules.
Thus, for example, on file gp_deap.py, line 33, where I see
from sklearn.model_selection._split import check_cv
should be
from sklearn.model_selection import check_cvThere may be some other places on the code you may want to check for future-proofing your project.",bug
https://github.com/alan-turing-institute/MLJBase.jl/pull/155,"addsall can generate their output as either MLJ-friendly (table, categorical) or just matrix/vector, all can generate points with a given type; in the case of the regression, only the points have said type, response is float64.also@davidbp I changed quite a few things (apologies for that), a few comments for you:Thanks again David for your effort! I'll merge that soon and close #136Continue to review full report at Codecov.Well, since MLJ already has a way to import Sklearn models I thought it was already ""cited"" so I did not thought this could be a problem.No problem! Next time I will make pull requests with code that is less prone to provide extra work. Definetly a set of guidelines for MLJ would be a good start (I understand that there are other priorities though).I will check the tests with care.",None yet
https://github.com/freqtrade/freqtrade/issues/2859,"If you have discovered a bug in the bot, please search our issue tracker.
If it hasn't been reported, please create a new issue.Hyperopt file was created out of the new templateerror during hyperoptsee log belowsee log below@Prossi79 Is there a particular reason why you're using trailing_stop_positive_offset_p1 as a param in AP_hyperopt_BbandRSIV4? Using --spaces trailing should be enough to optimize trailing parameters for you. You don't have to specify them in the AP_hyperopt_BbandRSIV4 file.I like to hyperopt the stoploss and trailing space, thats the code out of AP_hyperopt_BbandRSIV4 file:Yeah, the sample in sample_hyperopt_advanced.py was not aligned with the changes made into hyperopt_interface.py, I'll fix it, thanks!",Bug
https://github.com/scikit-learn/scikit-learn/issues/16391,"fit_grid_point is not used in examples or internally.Happy to see it deprecated then removed+1 for removalTo the person who will pick up this issue, please checkout our deprecation guidelines: https://scikit-learn.org/stable/developers/contributing.html#deprecationmay I work on this?Sure go ahead",Easy good first issue help wanted module:model_selection
https://github.com/MOAISUS/pycaret/pull/41,Bumps catboost from 0.20.2 to 0.24.Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting @dependabot rebase.Superseded by #43.,dependencies
https://github.com/tensorflow/tensorflow/issues/38850,"I get the following error:System informationDescribe the current behavior
The error only occurs only if mixed precision and noise is applied.When mixed precision is applied, then:and data are converted to float16.Converting the output of the previous dense layer to float16 couldn't solve the problem.I cannot update to a higher TF version.@Arktius
we see that the code shared is not sufficient for us to replicate the issue, could you please share a simple stand alone code to replicate.With casting the data to float16, an error during backpropagation occurs...Even if I cast the gradients in the custom training loop, the training doesn't get through.TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.And If I now cast the trainable_variables to float16, I'll get this:
AttributeError: 'Tensor' object has no attribute '_in_graph_mode'Any suggestions?@Arktius
Can you please provide simple stand alone code such that we can replicate the issue, if possible share a colab gist, with the error the code shared in incomplete, the error faced is not same as yours. please find the gist herehttps://colab.research.google.com/gist/Saduf2019/3d02bd5e25f035a97a4393df156e2763/untitled162.ipynbYou need !pip install tensorflow-gpu==2.0.I am able to replicate the issue reported, please find the gist hereThis is fixed latest tf_nightly_gpu-2.2.0.dev20200503Could you please show me what has changed? I've just looked in the release notes, but I couldn't find something related to Gaussian noise.I know that TF 2.1 also works as well, but I don't know why and I cannot use versions higher than 2.0.Take a look at commit. A bunch of other fixes were made as well so its a but tricky to pin point exactly.The link is probably broken. Did you want me to see the release notes?@ArktiusI am not seeing any issue with TF 2.2 version.Please, find the gist here.Please, verify once and close this issue.Thanks!Thanks for your comment, but I cannot work with versions higher than 2.0 at the moment due to driver issues on the server.Sorry for the delay in response. Please refer this commit f9e8998 for the changes.This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.Closing as stale. Please reopen if you'd like to work on this further.Are you satisfied with the resolution of your issue?
Yes
No",TF 2.0 comp:keras stalled stat:awaiting response type:bug
https://github.com/JasonKessler/scattertext/issues/49,Scattertext now will trigger the following future warning message.C:\Users*.virtualenvs\django-tally-QTYVOJb0\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.This is fixed in 0.0.2.57.,None yet
https://github.com/calpoly-csai/api/pull/162,"f it git pushYour Render PR Server URL is https://nimbus-pr-162.onrender.com.Follow its progress at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.Looks good so long as tests pass.Your Render PR Server at https://nimbus-pr-162.onrender.com is now live!View it on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.send itwaitYour Render PR Server at https://nimbus-pr-162.onrender.com is now live!View it on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.Your Render PR Server at https://nimbus-pr-162.onrender.com is now live!View it on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.requestYour Render PR Server at https://nimbus-pr-162.onrender.com is now live!View it on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.Your Render PR Server at https://nimbus-pr-162.onrender.com is now live!View it on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.Your Render PR Server at https://nimbus-pr-162.onrender.com is now live!View it on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.Kudos, SonarCloud Quality Gate passed!0 Bugs
0 Vulnerabilities (and 0 Security Hotspots to review)
5 Code SmellsNo Coverage information
0.0% DuplicationYour Render PR Server at https://nimbus-pr-162.onrender.com is now live!View it on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.Your Render PR Server at https://nimbus-pr-162.onrender.com is now live!View it on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.A deploy for your Render PR Server at https://nimbus-pr-162.onrender.com just failed.View details on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.Your Render PR Server at https://nimbus-pr-162.onrender.com is now live!View it on your dashboard at https://dashboard.render.com/web/srv-br7k3qg60jaqq47hqvb0.",None yet
https://github.com/kexinhuang12345/clinicalBERT/issues/7,"We are attempting to use custom pretrained parameters for the MIMIC III 30-day readmission task, and getting some odd behavior, namely that the model predicts consistent logits across all eval examples (see screenshot below). We running run_readmission.py with the start command below. We attempted the same task with the bert-base-uncased parameters, and got slightly different logits (by 0.01), but still consistent across all eval examples. The only thing we changed about the run_readmission.py script was from sklearn.utils.fixes import signature to from inspect import signature based on deprecation issues.Has anyone run into this issue before?Hi, sorry for the late reply! Could you share more details? Are you using the same model checkpoints and running the same data preprocessing scheme? Or are you using your own pre-trained checkpoints? Is it after finetuned? If you can, could you examine some sample inputs? It is likely that the classifier on top of BERT is not learning anything.Closed for now, reopen if you still have questions.",None yet
https://github.com/IQTLabs/NetworkML/issues/481,,bug
https://github.com/timsainb/AVGN/issues/5,"When importingI getHi @yoavram, you can import joblib directly instead of importing from sklearn if you pip install it. If you look at the examples from my newer repo from the paper (which I would recommend using if you're just starting out) you can see how to import joblib directly. https://github.com/timsainb/avgn_paperOK. The import that caused the warning is in a notebook in this repo, I'll look at the paper repo.
Thanks.",None yet
https://github.com/stellargraph/stellargraph/pull/707,"The converter.py file was reproducing a lot of functionality that's available in pandas and sklearn, where it is likely more general, more efficient and more reliable (since it's had more use). We've switched to doing using those libraries in all our examples, and others should too.Specifically:This PR is a ""hard deprecation"", since it removes all the functionality but leaves the NodeAttributeSpecification class, emitting an error on construction. I think we can remove it entirely after a single release.There's some support functions that have been removed too.See: #706Code Climate has analyzed commit d3a8ade and detected 0 issues on this pull request.View more on Code Climate.",None yet
https://github.com/pliablepixels/zmeventnotification/issues/188,"Event Server version5.0Hooks version5.2.1The version of ZoneMinder you are using:1.33.16What is the nature of your issueBugDetailsWith new version of scikit-learn 0.22.0, face recognition throws sklearn.neighbors.classification deprecation warning
and
AttributeError: 'KNeighborsClassifier' object has no attribute 'n_samples_fit_' :Downgrading to 0.21.3 solves the issue.Debug Logs (if applicable)$sudo -u http /var/lib/zmeventnotification/bin/zm_detect.py -c /etc/zm/objectconfig.ini -e 5891 -m 5Thanks!Delete faces.dat in /var/lib/zmeventnotification/known_faces/ and try again with 0.22That fixed it, thanksfixed, thanksTraceback (most recent call last):
File ""speaker_id.py"", line 783, in
test_video(args['video_file'], args['shape_predictor'], args['model'],args['fmodel'])
File ""speaker_id.py"", line 511, in test_video
character_name = fmodel.predict(face_features.reshape(1,-1))[0]
File ""C:\Users\Soof\Anaconda3\lib\site-packages\sklearn\neighbors_classification.py"", line 173, in predict
neigh_dist, neigh_ind = self.kneighbors(X)
File ""C:\Users\Soof\Anaconda3\lib\site-packages\sklearn\neighbors_base.py"", line 612, in kneighbors
n_samples_fit = self.n_samples_fit_
AttributeError: 'KNeighborsClassifier' object has no attribute 'n_samples_fit_'this is my error message, can you please help me with it?Traceback (most recent call last):
File ""speaker_id.py"", line 783, in
test_video(args['video_file'], args['shape_predictor'], args['model'],args['fmodel'])
File ""speaker_id.py"", line 511, in test_video
character_name = fmodel.predict(face_features.reshape(1,-1))[0]
File ""C:\Users\Soof\Anaconda3\lib\site-packages\sklearn\neighbors_classification.py"", line 173, in predict
neigh_dist, neigh_ind = self.kneighbors(X)
File ""C:\Users\Soof\Anaconda3\lib\site-packages\sklearn\neighbors_base.py"", line 612, in kneighbors
n_samples_fit = self.n_samples_fit_
AttributeError: 'KNeighborsClassifier' object has no attribute 'n_samples_fit_'this is my error message, can you please help me with it?",None yet
https://github.com/scikit-learn/scikit-learn/issues/10778,"sklearn.metrics.mutual_info_score [Source] does not calculate the mutual information correctly. For example: mutual_info_score([0, 0, 1], [0, 1, 1]) yields 0.17442 while it should have been 0.25163.Blame: Commit a3dacb6 replaced np.log2 with np.log.Fix: replacing the log_e with log2 repairs sklearn.metrics.mutual_info_scoreFound a SO issue about this (https://stackoverflow.com/questions/24686374/pythons-implementation-of-mutual-information). I assume it is by design that sklearn.metrics.mutual_info_score uses a non-conventional natural logarithm.Could some please mention this in the documentation of the method. [I'm really curious who would need this method.]It should probably be log2. I suspect it was only accidentally natural logarithm, but I'm not now looking back to check. I assume log and log2 would both give the same ranking for feature selection?A PR to mention this in the documentation, at least as a temporary solution, is welcome.Yes, the ranking is not changed (for non-trivial situation).I realised that a lot of code depends on the current implementation of sklearn.metrics.mutual_info_score and sklearn.metrics.entropy which uses a natural logarithm for at least 5 years, thus I suspected that this is by design.sklearn.metrics.adjusted_mutual_info_score (Source) also uses loge, but the referring documentation eg Information Theoretic Measures for Clusterings Comparison does not depend on a choice between base 2 or e.I'm not sure what would be the best solution to support log2 without breaking anything.I would like to contribute to this one, should I mention that natural logarithm was an accident after changing it? with an edit tag? or just mention it discreetly that the function uses log2Sorry I got confused between mutual info in metrics and feature selection. In metrics I think this should be regarded a bugBut yes, we could use a deprecation process to avoid backwards compatibility issues. I.e. add a parameter log_base whose default is to use e, but warn that the default will change in v0.22@jnothman I need help: I cannot find an example in which such a deprecation process is used.A PR for entropy would look like the following code: but I don't know what to raise as a deprecation warning, see TODO below (original entropy here)What would be better: pass the log_base to the log function which toggles between 'e' and '2' (this will hurt performance), or as done below with an if and duplication of code?Confused by problem due to this default setting for a long time.
If it is not the bug, at least it should be mentioned in the doc, and can be changed the log base by passing parameters.Actually, the solution to get log2 information is easy.
Just compute like@zyong812 Is there still a problem? I would like to work on it there is.Yes, sklearn/metrics/cluster/supervised.py makes no mention that it is using the natural logarithm.PR welcome, @zyong812.@jnothman Where do you suggest I add that info then?Unless we allow the user to configure which log base they use, it should just be documented in relevant functions' docstrings in sklearn/metrics/cluster/supervised.pyHi, I know that this is closed but I think that the user should choose the base of the log... I would like to contribute to this one.",Easy
https://github.com/scikit-learn/scikit-learn/pull/12599,"This is the continuation of #5653. Lots of tests and checks have been added, with a few fixes. More detailed changes from #5653 can be found here #5653 (comment).Closes #5653
Closes #4405This PR implements support for partial dependence plots with any regressor or classifier that has predict_proba.Partial dependence plots were only supported for gradient boosting trees and the functions were in ensemble.partial_dependence. All backward compatibility is preserved, but those functions are deprecated in favor of the sklearn.partial_dependence module.The grid param has been removed in the new implementation.
The ax param was replaced by fig. The claim that the new plots were plotted on the ax argument was wrong.In separate PRs:Can you give me a sense of what else has changed? It's hard to get from comparing commits since files have moved etc.Apart from moving files to inspection, not much has changed since you last saw it.I'm happy to merge after those three comments are addressed.Thanks and congrats, @NicolasHug and @trevorstephens!!Thanks a lot for the reviews @glemaitre @jnothmanWow, congrats @NicolasHug on getting this done, bravo!!Yay!!Which breaks my package sklearn-gbmi (which computes Friedman-Popescu H statistics to identify interactions between features in gradient boosted regression trees).Looking at the source code, I don't understand why you did this. It seems to me the function could still accommodate the specified-grid operating mode.Anyhow, is there some simple work-around I'm not seeing? I'd appreciate your advice.GitHub doesn't seem to be making it obvious, so I'll clarify that the preceding comment is directed to @NicolasHug.Honestly I don't exactly remember.I assume it's because with the additional support of the brute method, the interactions between method, X and grid were becoming too intricate to be reasonably intuitive.Unless something went wrong, you should have been warned long ago about the deprecation and ultimate removal of these previous functions.You can still pass X and the grid will be automatically computed from there. Though I'm not sure that solves your issueI wasn't. I was aware of your efforts to support a broader class of models, which I applaud. I wasn't aware of the removal of the specified-grid operating mode until users of my package informed me the package was broken. (I haven't been using it myself recently.)It doesn't.I could work around the problem by writing my own version of the partial_dependence function, supporting exactly my use case. However, it would depend on functions that aren't parts of the public API and that therefore could be changed at any time. It would be safer to create my own complete implementation. I don't have time to deal with either option right now, but I'll decide what to do and do it early next month.",None yet
https://github.com/JakeKandell/NBA-Predict/issues/3,"Hello, first of all thank you for your work, a masterpiece.Can you help me troubleshoot and solve this.Firstly, this error shouldn't cause the program to break and will still allow it to run perfectly. However, the reason for the error is that you installed the wrong version of scikit-learn. To resolve this, you should uninstall scikit-learn version 0.22 and install version 0.21.2. Let me know if you have any more questions.Hello @JakeKandell first of all sorry for reopening the issue, although lately I haven't been managed to run the script because of this:Do you know how to fix it?This issue is occurring because it appears that something has changed with the stats.nba.com api that I used for this project. There is an issue thread over on the page for nba_api where many people seem to be having the same problem as you. The creator of the api mentioned that he will work on fixing the issues this weekend, so if it's able to fixed I will let you know.The issue should be fixed now. You just should redownload the updated headers file and download the updated version of nba-api.Confimed! Working again!",None yet
https://github.com/JakeKandell/NBA-Predict/issues/3,"Hello, first of all thank you for your work, a masterpiece.Can you help me troubleshoot and solve this.Firstly, this error shouldn't cause the program to break and will still allow it to run perfectly. However, the reason for the error is that you installed the wrong version of scikit-learn. To resolve this, you should uninstall scikit-learn version 0.22 and install version 0.21.2. Let me know if you have any more questions.Hello @JakeKandell first of all sorry for reopening the issue, although lately I haven't been managed to run the script because of this:Do you know how to fix it?This issue is occurring because it appears that something has changed with the stats.nba.com api that I used for this project. There is an issue thread over on the page for nba_api where many people seem to be having the same problem as you. The creator of the api mentioned that he will work on fixing the issues this weekend, so if it's able to fixed I will let you know.The issue should be fixed now. You just should redownload the updated headers file and download the updated version of nba-api.Confimed! Working again!",None yet
https://github.com/scikit-learn/scikit-learn/issues/10890,"metrics.pairwise functions (rbf_kernel for instance) throw error due to unexpected 1-D array instead of 2-D array when called from _pairwise_callable.I discovered the bug when I have been using sklearn.kernel_approximation.Nystroem with sklearn.metrics.pairwise.rbf_kernel. When Nystroem wants to compute the pairwise metric between the examples it asks for _pairwise_callable to do it and inside this function there is this thing:if metric is a metric from sklearn.metrics.pairwise (I mean, the function, not the string) then it crashes because X[i] and Y[j] are both of shape (d,) instead of (1, d) and those metrics check if the input is 2D array.So there is a problem. But it may be multiple fold:What do you think?Example:There are a few things in this issue. Here is what I could spot (I may have missed a few things because there is a lot of text in-between ...):Thank you for your answer. Sorry for not have been clearer. You have not missed anything.I am not used to contributing on open-source projects, I wanted to share my point of view before doing anything. I put this on my todo list. You are right for Nystroem this is how I achieved my goal.kernel='rbf' definitely works but I feel like what you want should also work and we should fix it.I've run into this issue as well. I believe the main issue is with _pairwise_callable trying to be clever and only build half the kernel matrix (and then replicating it for the full matrix), by calling the metric function in a loop over rows of X and Y. The already implemented kernels, like rbf_kernel expect X and Y as matrices and are optimized to produce the full kernel matrix faster than any looping. Thus I believe all optimizations should be in the kernel functions themselves and removed from _pairwise_callable. And all user created kernel functions passed in would then have to function like the built in kernels, ie expecting matrices. I think this is the cleanest solution, but not sure what it might break...I've run into the same issue as well. A simple solution I can think of that won't break anything is to check if the metric is in PAIRWISE_KERNEL_FUNCTIONS.values() in addition to PAIRWISE_KERNEL_FUNCTIONS.keys().I would argue that the actual problem is that we don't have an API that allows a user to pass a matrix-matrix distance function.
If we allowed the 1d case with the reshape as suggested by @lucgiffon that would make things much slower if passing the function instead of passing the string.
If we did the fix of @HanGuo97 that would mean some callables are treated different from other callables, which seems weird.Unfortunately there is no way for us to know whether the callable the user passes allows matrix-matrix distances or only vector-vector distances.@lucgiffon saidI think the docs already say that it needs to be a string, but we can be more explicit in saying that the callables from sklearn are not supported, because supporting them and supporting the current interface would lead to computations that are much slower (unless we come up with a way to specify if a metric is vector-vector or matrix-matrix).Another possibility would to go to matrix-matrix callables everywhere in scikit-learn, but that sounds like a rough deprecation...",None yet
https://github.com/vaexio/vaex/pull/524,This PR renames vaex.ml.sklearn.SKLearnPredictor to vaex.ml.sklearn.PredictorI agree with this plan.,None yet
https://github.com/scikit-learn-contrib/imbalanced-learn/issues/680,"HelloI'm using imbalanced-learn==0.6.1.When running the following code throws an error just because some parameters with similar behavior are named differently across different classes:The problem is that ADASYN's parameter for neigthbours is named n_neighbors but SMOTE's parameter is named k_neighbors.I know this can be workarrounded but it is far better to normalize them. Not sure if this happens with other methods.If you agree, I may write those changes and make the PR, but will need to know the process for this.Thanks in advanceCorrect. That has been discussed in the past and has also been reported in #601.
It needs a deprecation cycle. That means that we should keep the parameter, along with the new one, for two subsequent minor releases.I am closing the issue as a duplicate of #601. However, be aware that we are opened to changes following the normal deprecation path.",None yet
https://github.com/deephyper/deephyper/issues/12,"sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. This PR adds joblib as an explicit dependency.joblib 0.10.3 is used as the minimum version since it is the version included in scikit-learn 0.18.1.
https://github.com/scikit-learn/scikit-learn/blob/a5ab948cbc366d705b1f8db8687c7162f51de22d/sklearn/externals/joblib/__init__.py#L119",warning
https://github.com/franchg/pycon9/pull/125,This PR updates Nuitka from 0.5.30 to 0.6.9.1.,None yet
https://github.com/skorch-dev/skorch/issues/570,"When calling GridsearchCV I got :This is with scikit-learn 0.22rc3 version.Regards,Can you provide a reproducible code example where the warning appears?We use sklearn's check_is_fitted with that parameter here:Since the parameter was deprecated here, we should also remove it.The question now is if we want to deprecate it as well or just flat out remove it. My guess is that nobody uses skorch.utils.check_is_fitted directly, which would make it safe for immediate removal. If we deprecate, people will see two deprecation warnings while just using normal skorch code, and they cannot even easily fix it.It still has a use case in skorch by raising skorch specific exceptions. Since it is not being used, we can change its behavior without much consequences.A quick fix would be to check the sklearn version and make sure to call check_is_fitted with the correct arguments (depending on version).I just saw that the changes to check_is_fitted make it unusable for use. E.g.:Instead, the function now only checks if there are any attributes that end on _. This is the case for skorch even before fitting.There is also :Should I make a separate issue for this?Thanks for that. Yes, please open a new issue for that.@fradav This issue can be closed, right?I think it can be closed. @fradav please re-open the issue if you object.",None yet
https://github.com/wesm/dataframe-protocol/pull/1,"Based on https://discuss.ossdata.org/t/a-dataframe-protocol-for-the-pydata-ecosystem/267, we are discussing a ""protocol"" method (potentially called __dataframe__) similar to __array__ for data frame-like data. The consensus so far is that this protocol should not force conversions to a particular data frame memory model like pandas. Instead, it may provide access to its metadata and data in the form desired by the user of the __dataframe__ protocol.Questions:The APIs proposed here are not intended to be to the exclusion of others -- more APIs can be proposed and added later.@wesm I really like this, there are some minor differences between the model here and Modin's model worth discussing. Some are noted inline, others I will add here:Slicing/Masking/Projecting/Selecting
There is only one method to extract a subset of columns and rows in Modin: called mask. It accepts either labels or position, based on keywords provided. This interface follows software engineering best practices of keeping external interfaces narrow and implementations deep.I think it's cleaner than treating all four as separate methods, and all calls will be the same.Types
Modin does not enforce types or have its own system of types and does not place requirements on types. I think this is better.At a high level, I think the interface should be as narrow as possible.Maybe not surprisingly I think types and schemas are good, and libraries like pandas being a bit ""loosey goosey"" about type metadata has IMHO caused a lot of problems over the last 10 years.If the producer of a data frame can expose type metadata without requiring a potentially costly conversion, then it seems reasonable to me to permit this. If you know that a column contains integers you might pass different arguments to to_numpy then if it were strings.As far as this interface having its ""own system of types"" -- what would be the alternative, to return the metadata of the internal data representation (e.g. a NumPy dtype)? That seems contrary to the goals of this project, which is to avoid exposing details of the data frame producer to the data frame consumer.I think the requirements for df[col_name].type can also be relaxed so that a data frame producer can return a ""type not indicated"" value.Like above I'm interested to see what consumer projects (that don't want to depend on pandas, say) think about this.Regarding conversion between different data frames, how would libraries convert between the types? It'd be nice to have an API similar to what __array__ does. What I mean is to have this work, with reasonable overhead, minimal memory copy, and assuming they all implement the proposed DataFrame API:Not sure how easy/challenging it is to make it happen though.@adrinjalali that is effectively what we are discussing here (starting from https://discuss.ossdata.org/t/a-dataframe-protocol-for-the-pydata-ecosystem/267) -- i.e. thinking about what is the API of the object returned by a __dataframe__ method, so you would haveThe __array__ method returns a numpy.ndarray, but we have to determine what kind of object __dataframe__ returns and what behavior that object hasEDIT: I just added some comments about this to the PR summary for clarityWith respect to a __array__ analog for dataframe, doesn't that necessitate more dedicated methods like a __pandas_dataframe__, __modin_daframe__, etc? The intent of these methods is to give objects control over producing a concrete ndarray. So the concrete dataframe (say pandas.DataFrame's constructor) would need to check the object for a __pandas_dataframe__ implementation and hand of control of the construction to the object with the __pandas_dataframe__ method.I'd be curious to know if there's value in a more generic __dataframe__ method, and if so what it would produce. Or do we think that consumers of this protocol fall in one of two camps:This is exactly what is being proposed here and what I understood to be the spirit of the discussion in https://discuss.ossdata.org/t/a-dataframe-protocol-for-the-pydata-ecosystem/267. We need to determine what object is returned by __dataframe__ though -- what is in this PR is a proposal for that object's interface.Both camps of users are served by this.I'm still not sure how we can have a unified object returned by __dataframe__ in this scenario. To me, this proposal has two aspects to it, and both are equally important:The first category is a unified interface for libraries to depend on, which can be used to extract information from the dataframe. Reading feature names, dtypes, etc, is in this category.The second one is enabling the ecosystem to easily work with one another in an efficient way. If I compare __dataframe__ to __array__, then I'd imagine that the details of how the object is converted to a specific dataframe, should be done in the __datafram__ method, and not the [pandas, xarray, ...] constructor which reads the output of __dataframe__; the same way that numpy expects the output of __array__ to be an ndarray. And to me this needs to be done by the object which implements __dataframe__, since it's the one who knows how to efficiently convert its internal data structures into a given dataframe. So I'd expect for the __dataframe__ to either accept an argument such as pandas, xarray, etc, or to have specialized methods such as __xarray_datarray__, __pandas_dataframe__, etc.That said, for the dataframes to work nicely with one another, they don't have to implement any of these specialized __*__ methods. They can start by implementing the interface in the first category, and then if such a dataframe is given to pd.DataFrame, the constructor will first check if it has a __pandas_datafram__ implemented, and if not, it will use the public methods to extract the information it needs to create a dataframe.I hope this makes it a bit more clear on what I meant before.To me it would simplify the discussion if we split the discussion between:One thing that occurred to me is that the interface is completely column-oriented. But if you have a large file or database with lots of rows, and you read it in row by row, you won't have complete column vectors until you have read everything in, and then you might have some issues with memory.So my question is whether the interface should define a protocol for creating a DataFrame from row-oriented data.This can be done separately, but we should consider adding an .attrs attribute to the interface as a way of propagating metadata about the dataset (cc @philippjfr). Pandas, xarray, and h5py all implement the same interface.I made another pass on this per feedback here. I removed all the dunder methods in the interest of being as conservative / explicit as possible. Take a lookI relaxed hashability of column names and changed column_names to return Iterable. PTALIf someone would like write access on this repository to help lead this effort please let me know. I'm juggling a few too many projects so need to step away from this for a while",None yet
https://github.com/scikit-learn/scikit-learn/issues/15741,"sklearn.utils.deprecated modifies the docstring of a function or a class to warn users for the deprecation. However it does not seem to render very well with the documentation because it adds a definition description which adds a lot of indentations.https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.partial_dependence.partial_dependence.html#sklearn.ensemble.partial_dependence.partial_dependencehttps://scikit-learn.org/dev/modules/generated/sklearn.ensemble.partial_dependence.partial_dependence.html#sklearn.ensemble.partial_dependence.partial_dependenceI think that there could be two solutions:@johannfaouzi since this is about deprecated stuff and they'll be removed anyway, we may not have the time to work on it, but we can gladly have a PR doing the fix you suggest, if you're kind enough to create one :)",module:utils
https://github.com/AllenInstitute/mouse_connectivity_models/issues/47,"Hello, I got an error during pytest. Do you know how to solve the following error?
I am using python3.7 on MacOS Catalina.Hi, I'm afraid we do not support OS X. I suggest trying on a Linux machine and seeing if it works, or starting from a fresh python install/environment. Best of luckThanks for your answer. I will try with another computer.",None yet
https://github.com/pylablanche/gcForest/issues/2,"The data is from UCI.Here is the link.http://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29
Here is my code.`
data_dir = '../census_income.data'df = pd.read_table(data_dir,sep=',',header=-1)df[41][df[41]==' 50000+.']=1
df[41][df[41]==' - 50000.']=0y_tag = 41
pos_value = 1
neg_value = 0
y = df[y_tag].values
y = y.astype(float32)
del df[y_tag]for c in df.columns:
if df[c].dtype == 'object':
lbl = preprocessing.LabelEncoder()
lbl.fit(list(df[c].values))
df[c] = lbl.transform(list(df[c].values))mmsc = MinMaxScaler()
for i in df.columns:
df[i] = mmsc.fit_transform(df[i])df = df.astype(float32)df = df.fillna(df.median(axis=0))X = df.valuesX_train, X_test, y_train, y_test =train_test_split(np.nan_to_num(X),y,test_size = 0.3,random_state=123)gcf_param={'shape_1X': X.shape[1],
'window':[1],
'n_mgsRFtree':30,
'stride':1,
'cascade_test_size':0.2,
'n_cascadeRF':2,
'n_cascadeRFtree':101,
'cascade_layer':100,
'min_samples_mgs':0.1,
'min_samples_cascade':0.05,
'tolerance':0.0,
'n_jobs':1
}gcf=gcForest(**gcf_param)start_time=datetime.datetime.now()gcf.fit(X_train, y_train)end_time = datetime.datetime.now()cost_time = end_time-start_timecost_time = int(cost_time.seconds)
`
The error raises when it comes to the 'gcf.fit(X_train,y_train)',but there is no NA and inf in the data,so I wonder where the problem is.@chibohe I just wanted to let you know that I haven't had much time to look at your issue lately but I'll be able to look at it next week.
Have you been able to solve it or is it still a problem?Hey @chiboheI have tried to reproduce your results and here is my code :And it runs smoothly without any error.That make me think it is either a problem of libraries a bit too old or a hardware problem, i.e. running out of memory or something similar. How much memory available do you have ?Thank you for your reply.
Now it can run smoothly on my PC,maybe I didn't fill the NA before:)@chibohe Glad I could help.
Honestly I can't really spot where there was any problem in your code except maybe here :but I'm not even sure.
Feel free to contact me again if you face any more difficulties!@pylablancheHello,I am having same issue and wondering if you can help me with code. I am getting same error:ValueError: Input contains NaN, infinity or a value too large for dtype('float64').Any idea what could be the issue? I really appreciate your help. My full code is below:import os
import numpy as np
import pandas as pd
import pickle
import quandl
from datetime import datetime
import seaborn as sns
import plotly.offline as py
import matplotlib.pyplot as plt
import plotly.graph_objs as go
import plotly.figure_factory as ff
py.init_notebook_mode(connected=True)
import quandl
lags = 5
start_test = pd.to_datetime('2017-03-01')
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.svm import SVC as SVC
from math import sqrtdef computeClassification(actual):
if(actual > 0):
return 1
else:
return -1#DATA IMPORTING#quandl.ApiConfig.api_key = os.environ[""daQg3mGnaMeP2JDH5swh""]
quandl.ApiConfig.api_key = ""daQg3mGnaMeP2JDH5swh""df = quandl.get(['BCHARTS/BITSTAMPUSD.4'], start_date = ""2011-09-13"", end_date = ""2017-12-18"")df.rename(columns={'BCHARTS/BITSTAMPUSD - Close': 'Close'}, inplace=True)print(df.head())df['Stdev'] = df['Close'].rolling(window=90).std() # calculate rolling 90 day std
df['SMA'] = df['Close'].rolling(50).mean() # calculate 50 day SMAdf['returns'] = np.log(df['Close'] / df['Close'].shift(1))
df['returns'].fillna(0)
df['returns_1'] = df['returns'].fillna(0)
df['returns_2'] = df['returns_1'].replace([np.inf, -np.inf], np.nan)
df['returns_final'] = df['returns_2'].fillna(0)
print(df['returns_final'])ts = df
ts.index = pd.to_datetime(ts.index)
tslag = ts.copy()for i in range(0, lags):
tslag[""Lag_"" + str(i + 1)] = tslag[""Close""].shift(i + 1)
tslag[""returns_final""] = tslag[""Close""].pct_change()for i in range(0, lags):
tslag[""Lag_"" + str(i + 1)] = tslag[""Lag_"" + str(i + 1)].pct_change()
tslag.fillna(0, inplace=True)tslag[""Direction""] = np.sign(tslag[""returns_final""])X = tslag[[""Lag_1"", ""Lag_2""]]
y = tslag[""Direction""]X_train = X[X.index < start_test]
X_test = X[X.index >= start_test]
y_train = y[y.index < start_test]
y_test = y[y.index >= start_test]pred = pd.DataFrame(index=y_test.index)svc = SVC() # import SVC # import RFC
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test) # predict y based on x_testpred = (1.0 + (y_pred == y_test)) / 2.0
hit_rate = np.mean(pred)
print('SVC {:.4f}'.format(hit_rate))
print(pred)regressor = SVC()
regressor.fit(X_train, y_train)from sklearn.metrics import mean_squared_errormse = mean_squared_error(y_pred, regressor.predict(X_test))
print(""MSE: %.4f"" % mse)df['strategy'] = pred * df['returns_final'] # however cumulative performance of the strategy
df[['returns', 'strategy']].ix[lags:].cumsum().apply(np.exp).plot(
figsize=(10, 6))plt.show()sorry resent code in proper format for you to view easily. thanks.........NEver mind you can disregardfigured it out thanks@maysam19 ,
Great to hear that (I was about to look at your problem).If I may ask, what was the problem and at what line in your code ?I believe the code has something to do with quantitive trading.
i.e. classifying a sequence of time series data into buy or sell.
(since he imported the quandl module, and a lot of technical indicators such as SMAs )@kingfengji
That was my guess too! The classification tag made it pretty obvious.
I was more curious to know if it was an error like missing values or wrong data type. :)@pylablanche haha...you are probably right,I think...
@maysam19 for these kind of data, class imbalance is an important issue (most of the time the stock price wont rise sharply) needed to be taken care of, so you need to set the sample weights for base estimators so as to take the imbalance issue into account.Hello,I am having the same issue and wondering if you can help me with code. I am getting the same error:ValueError: Input contains NaN, infinity or a value too large for dtype('float32'). i am using gcforset classifier. The error is found end of code, i just simple pass rowXcolum(row=number of sample, column=feature data). When i pass data to gcforest classifier give me that issue.
any idea what could be the issue? I really appreciate your help. My full code is below:import sys
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_curve, auc
#from gcForest.GCForest import gcForest
from GCForest import gcForest
from sklearn.preprocessing import scale,StandardScaler
#from load_data import load_data
#from baseline import TitleFinder
#from baseline_author import AuthorFinder
#from constants import *iRec = 'SVM_CBR_bestfirst_Hspeice.csv'
df = pd.read_csv(iRec, header=None).fillna(0) # Using pandasdata=scale(df)
from sklearn import preprocessing
data=preprocessing.normalize(data)
label1=np.ones((495,1)) #Value can be changed
label2=np.zeros((495,1))
label=np.append(label1,label2)
X=data
y=labeldef main():
#X = data.iloc[:, :-1].values
#y = D.iloc[:, -1].values
#X, y, tfidf = load_data()if name == 'main':
main()
*********************error
File ""C:\Users\saeed\Miniconda3\lib\site-packages\sklearn\utils\validation.py"", line 56, in _assert_all_finite
raise ValueError(msg_err.format(type_err, X.dtype))
ValueError: Input contains NaN, infinity or a value too large for dtype('float32').Process finished with exit code 1@pylablanche please check my issue i data is attached with this messageSVM_CBR_bestfirst_Hspeice.xlsximport pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split as tts
from sklearn.model_selection import KFolddata=pd.read_csv(r""C:\Users\ISHIKA\4th\household_power_consumption\household_power_consumption.txt"",delimiter="";"")#print(data.head())
data.drop([""Date"",""Time""],axis=1,inplace=True)cols=data.columns
data[cols]=data[cols].replace([""?""],[None])
data=data.fillna(data.mean(axis=1))#data.replace([""?""],[data.mean()],inplace=True,axis=1)
#print(data.head())
x=data.drop(['Global_active_power'],axis=1)y=data[['Global_active_power']]x_train,x_test,y_train,y_test=tts(x,y,train_size=0.7,random_state=200)
x_train=x_train.to_numpy()
x_test=x_test.to_numpy()
y_train=y_test.to_numpy()
y_test=y_test.to_numpy()
from sklearn.preprocessing import StandardScaler
#x_train=x_train.to_numpy()
#x=x.astype(float)
scaler=StandardScaler()
scaler.fit(x_train)
scaler.fit(y_train)
scaler.fit(x_test)
scaler.fit(y_test)
x_train=scaler.transform(x_train)
y_train=scaler.transform(y_train)
x_test=scaler.transform(x_test)
y_test=scaler.transform(y_test)
#t=Normalizer()
#x=t.transform(x)
np.nan_to_num(x_train)
np.nan_to_num(x_test)from sklearn.preprocessing import PolynomialFeatures
pr=PolynomialFeatures(degree=4,include_bias=True)
x_poly=pr.fit_transform(x_train)
pr.fit(x_poly,y_train)
print(""checklist2"")
from sklearn.linear_model import LinearRegression
lr=LinearRegression()
lr.fit(x_poly,y_train)
plt.scatter(x,y,color=""Red"")
plt.plot(x_test,lr.predict(pr.fit_transform(x_test)),color=""black"")
plt.show()please help me I have the same error with no huge data values in my data set",None yet
https://github.com/RTXteam/RTX/issues/720,"Tried to run the following ambitious query:
add_qnode(curie=DOID:14330, id=n00)
add_qnode(is_set=true, id=n01)
add_qnode(type=disease, id=n02)
add_qedge(source_id=n00, target_id=n01, id=e00)
add_qedge(source_id=n01, target_id=n02, id=e01)
expand(edge_id=[e00,e01])
overlay(action=compute_jaccard, start_node_id=n00, intermediate_node_id=n01, end_node_id=n02, virtual_edge_type=J1)
resultify(ignore_edge_direction=true)
filter_results(action=sort_by_edge_attribute, edge_attribute=jaccard_index, direction=descending, max_results=50)The query poops out at resultify() with:
DEBUG: Processing action 'resultify' with parameters {'ignore_edge_direction': 'true'}
ERROR: The two nodes for KG edge 1281618, DOID:0050890 and DOID:1289, have no corresponding edge in the QG
DEBUG: Applying Resultifier to Message with parameters {'ignore_edge_direction': 'true'}Unclear if this is aproblem with resultify() or if resultify is legitimately unhappy because expand() has a bug.Think I know what's causing this error:During expansion of your query graph, node DOID:14330 (corresponding to n00) is also being returned from neo4j as a result for n02 (generic disease). Therefore, this node technically has multiple qnode_ids, but we only store one of those on the node. (This was a corner case we discussed in the past but decided not to worry about at the time, as it seemed unlikely to be encountered.)I adjusted expand to throw an error when this happens (for now at least). Not sure if we want to come up with a way to allow multiple qnode/qedge IDs?Ah, excellent, good sleuthing!
Definitely a good topic for discussion.
For this type of query, including the n00 node as a n02 is sort of a degenerate solution (i.e. the disease most similar to DOID:14330 is of course DOID:14330, but not interesting)
Seems like we want to force DOID:14330 to remain a n00 and be excluded from being a n02.True - for that query, it doesn't seem helpful to have DOID:14330 appear as an n02...Though for a query like this, for example:it seems these would both be relevant results:which means synuclein alpha would technically have two qnode_ids (n01 and n03).Not sure how often/if such a query would be run in practice though.Which proteins interact with drugs that interact with proteins that are related to fanconi anemia?
Seems like a reasonable question.I think our data model will break down with this. Having the same node with the same CURIE play two different roles isn't going to work so well. This is something that we should consider with the whole team and probably even Translator-wide. One can imagine a hack of adding an underscore to the end of the CURIE to make it different in the different roles.So, probably not often, but it seems like something that we should plan for.Sounds good to discuss with the team, though I'm not sure this is a Translator-wide issue? I think the ReasonerStdAPI already allows for this, since the mapping of qnode_id -> node doesn't happen within the knowledge graph, but in the result bindings. Meaning, different result bindings could refer to synuclein alpha for different query nodes, but synuclein alpha still only exists once in the knowledge graph.So I think the challenge is more how we want to deal with this internally, since we've opted to decorate nodes with their qnode_ids for our own purposes.yeah, I think you're right. So maybe internally we just need to allow qnode_id be a map of multiple ids so that Expand() can communicate to Resultify() when the came CURIE plays two roles.It sounds like we are contemplating a change to the Reasoners Standard API object model such that a Node object can have a one-to-many relationship to QNodes (correct?). Here is where we are at right now: currently, the _get_results_for_kg_by_qg function (which is the actual internal function that does the ""resultifying"") in ARAX_resultify.py(line 369) requires that any Node object have one (and only one) qnode_id. If not, the function raises a ValueError. If that mapping becomes one-to-many, the internal map node_bindings_map would have to become one-to-many. This will complicate the procedure for ascertaining whether or not a given set of KG nodes can define a subgraph that matches the QG (previously, with a one-to-one mapping, it was trivial). But I think that it should be doable, provided that the cardinality of the number of nodes in the QG never gets too big. Tagging @dkoslicki for his graph insights here.Why can't this query be handled with a query graph like this?QNodes:QEdges:MONDO:0019391 has to be a node, so like n03and there's no change to the standard, but rather just our internal convenience feature qnode_id in KG Node()Yeah - and I think overlay relies on QG IDs as well (not just expand and resultify).One crazy alternative idea to annotating nodes/edges with their list of QG IDs is to instead (internally) organize them by their QG IDs, like:So you can see Node1 appears under both n00 and n02.This is really nice/more performant for expand, as I almost always am trying to grab nodes/edges with a particular qnode_id/qedge_id, but not sure how well it would work for other modules. (Maybe help? Maybe hurt?) Doubt it's something we'd choose to pursue, but thought I'd throw it out there.That's crazy!And I love it!But not for me to decide since I don't have much to do with code that would be affected by this. The writers of Expand, Overlap, and Resultify would need to jointly decide whether this is a useful/better internal representation.I'm afraid I don't understand this. The node object clearly has a qnode_id property as shown in this YAML file:https://github.com/RTXteam/RTX/blob/master/code/UI/OpenAPI/python-flask-server/RTX_0.9.3_Swagger2.0.yamland isn't this YAML file the embodiment of the reasoner's API standard?Or are you saying that this YAML file has extra stuff that is not specified in the standard?This is our implementation of the standard. Note that the official ""standard"" is still at 0.9.2, is on OpenAPI 3.0.1, not Swagger 2.0, and has both a core schema as well as an ""experimental/optional"" branch that individual sites may implement. Plus we added some things for our own convenience. Like qnode_id and log[ ].
So, that YAML file that you reference is our embodiment of what we chose to implement that is mostly the core plus the experimental plus some new spicy things that we've thrown into the soup. The real standard is here:
https://github.com/NCATS-Tangerine/NCATS-ReasonerStdAPI/blob/master/API/TranslatorReasonersAPI.yaml
It's missing a lot of fun stuff.
Everyone is more or less doing the same things, which is why the discussion around this topic is so complex.@edeutsch thank you for the very helpful explanationAmy and Eric, I had always assumed that a node in the KG cannot match both a fixed node in the QG (e.g., ""DOID:14330"") and a non-fixed node in the QG (or more generally, two distinct nodes in the QG at the same time). Consider a simple example with query graph:please correct me if I am wrong, but it sounds like in response to this issue we are proposing to allow matching the above QG with a KG subgraph like thisIs that correct? If so, I don't see why the proposed KG should be allowed to match the QG. It's not a subgraph isomorphism (right, @dkoslicki ?). The above KG subgraph has two nodes and the QG has three nodes. If I as the user request a query graph with three nodes, I feel like it is reasonable to expect each result to have three QG-mapped nodes in it. Also, allowing this kind of match would require some changes in ARAX_resultify, and I'm not so clear on why we want to allow it (see below).As for this query (I will call it ""Query 2""):I'm not sure why we would want results where n01==n03. But suppose that for some reason we did. And suppose we had modified Expander so that Query 2 does not error out and does not return any results where n01==n03. It seems to me that we could easily get results where n01==n03 (along with all the results where n01<>n03) by constructing the union set of proteins in the following two queries:If there is a demand for such a workflow, I think we should consider the possibility of implementing a ""results union"" in the DSL to handle it as described above. But if that is not possible and if we really need n01==n03 type results as described above, I think we should make this a conditional mode in resultify (i.e., we should have a resultify DSL parameter option strict_subgraph_isomorphism=bool, for which the default value could be a matter of discussion). My reasoning is is in part that anyone using the system who has used SPARQL before might find the n01==n03 result quite surprising; thus having a strict_subgraph_isomorphism=true mode of operation would probably be best for people who want to think of their query graphs with a ""SPARQL""-like mental model.Sorry to be a sticky wicket about this, but ARAX_resultify was carefully constructed under the interface contract that each KG node maps to one and only one QG node. To change that means (I think) a fairly significant alteration of the resultify module. So while that is not necessarily an argument for not ever changing it, it is an argument for carefully considering what has changed in our use-cases to warrant breaking the interface's data model.I don't have an answer, there are several possible approaches here. But I think this is a very important topic for discussion. In an ideal world I would suggest that @saramsey , @dkoslicki , @amykglen , and I (and anyone else) all sit together in a room with a whiteboard for 2 hours and hash this out and make decisions.OK next week works for me. This week is impossible.Just to clarify about this part of your response, @saramsey:I think with ""Query 2"" I was pointing out a slightly different issue than this - in my example, both results have four unique nodes (so within each result n01<>n03), but one result uses a particular protein (synuclein alpha) as n01, and the other result uses that same protein as n03.Ah, I missed that. Thank you @amykglen. OK, I am starting to come around to the idea of making the qnode_id field of the Node object a list. No doubt a bunch of code in resultify would need to be fixed to accommodate this.If we do make qnode_id a list, now we'll have to loop over all possible KG to QG mappings. And they can't vary independently; i.e., if we have proteins foo and bar in the KG and both can map to n02 or n04 in the QG, then we have to enumerate the two casesandwhile disallowing cases such asok, so I was able to merge master into issue720 with no conflicts, but when I run pytest -v in ARAX/test/, 9 Overlay tests are failing for me... all complain about sqlite3.OperationalError: unable to open database fileI doubt this is related to the qnode_ids/qedge_ids stuff.. maybe a local problem for me? do those tests pass for anyone else in issue720 (after pulling)?The output is huge, but here's the first several failing tests:I am testing on main server. But is taking a very long time. presumably populating NGD cache. 35% done all PASSED so far.All tests passed on the main server. But there were a bunch of warnings about using deprecated modules:test_ARAX_overlay.py::test_predict_drug_treats_disease_virtual
/mnt/data/python/Python-3.7.3/lib/python3.7/site-packages/sklearn/externals/joblib/init.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
warnings.warn(msg, category=FutureWarning)test_ARAX_overlay.py::test_predict_drug_treats_disease_virtual
/mnt/data/python/Python-3.7.3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)test_ARAX_overlay.py::test_predict_drug_treats_disease_virtual
test_ARAX_overlay.py::test_predict_drug_treats_disease_attribute
test_ARAX_resultify.py::TestARAXResultify::test_example2
test_ARAX_resultify.py::TestARAXResultify::test_issue680
test_ARAX_workflows.py::test_example_2
/mnt/data/python/Python-3.7.3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)ok, great, thanks @edeutsch. well I suppose I'll finish up the merge then and merge issue720 into master since apparently the failures are just a problem in my local environment.Merge is complete! No conflicts.Now I'm just going to merge the expander and resultify branches into master for #722 and #800 - will let you know when that is done and we're ready for a roll-out, @edeutsch.@edeutschYeah, these warnings can be ignored. They don't impact the ML models we are using (and will be depreciated soon with the work @chunyuma is doing on #758)all merged code rolled out to /test and /beta but not production. Please test /test and /beta and let us know if anything is amiss.No issues discovered in the testing I've done on /test or /beta so far - have mostly tested Expand/Resultify since I don't know Overlay/Filter very well, but this is one example of DSL that produces nodes with multiple qnode_ids (some nodes act as both n01 and n03), and all modules seem to handle it as expected:https://arax.rtx.ai/test/?m=2284@finnagin There appears to be an issue with sklearn with joblib being move/depreciated/etc.See:I triedbut unfortunately:So it appears to be an issue with the *.pkl file.Note: this is on a fresh install (new conda env, new KGNodeIndex, etc.).So maybe we need to peg sklearn to a certain version? All other tests pass except for those involving the drug-disease prediction.@amykglen just so I know, what version of sklearn are you using?@finnagin note that downgrading to scikit-learn 0.22 makes the test pass via conda install scikit-learn==0.22@dkoslicki - the environment that test passes in for me has scikit-learn 0.22.1. it fails for me in a newer environment that has scikit-learn 0.23.1.@amykglen yeah, that seems to be the issue. Trying to peg it in requirements.txt now....@amykglen ok, pegging sklearn to 0.22 makes all test pass.
Note, after @chunyuma addresses #758 , we may need to change this in the requirements.txt fileThis is what I see on the main server:root@1d9f9e25e40a:/# pip3 list | grep learn
scikit-learn 0.22.2.post1@edeutsch , I think that should be fine! Just avoiding 0.23Feels good man!Then test:So now that requirements.txt is now updated, I think we're good to go!I still see these warnings:
test_ARAX_overlay.py::test_predict_drug_treats_disease_virtual
/mnt/data/python/Python-3.7.3/lib/python3.7/site-packages/sklearn/externals/joblib/init.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
warnings.warn(msg, category=FutureWarning)test_ARAX_overlay.py::test_predict_drug_treats_disease_virtual
/mnt/data/python/Python-3.7.3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)test_ARAX_overlay.py::test_predict_drug_treats_disease_virtual
test_ARAX_overlay.py::test_predict_drug_treats_disease_attribute
test_ARAX_resultify.py::TestARAXResultify::test_example2
test_ARAX_resultify.py::TestARAXResultify::test_issue680
test_ARAX_workflows.py::test_example_2
/mnt/data/python/Python-3.7.3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)Should I downgrade the on the server? OR do we address another way?
Note the last message is ""pickled with 0.19.1"". Presumably that would not going away even if I downgraded to 0.22?@edeutsch I don't think we should be too concerned about the warnings. (as they show up in other tests too on different machines)Things still work even with those warnings. The predict drug treats disease is in flux per #758 , and until that's finished we can probably ignore theseFrom what I've seen on my end, using sklearn 0.2.2.x works just fine, so if you're running something else on prod, you might test that (though you will still see the warnings). If it ain't broke Ii.e. tests still passing), don't fix is is my motto atm.So yes, even if you downgraded, I figure you would still see the warnings.okay, will stand pat, then, thanks.well, from my perspective it's safe to close this issue I guess! when we're ready I can remove the issue720 branch as well.",None yet
https://github.com/freqtrade/freqtrade/pull/2848,"Bumps scikit-optimize from 0.5.2 to 0.7.1.Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting @dependabot rebase.Coverage remained the same at 97.196% when pulling 3938418 on dependabot/pip/develop/scikit-optimize-0.7.1 into 5c20311 on develop.checked the new version, comparing it to 0.5.2:Later, that workaround should be removed, since this version implements proper own mechanism for controlling the size of the models history queue...",Dependencies Python
https://github.com/NervanaSystems/nlp-architect/issues/54,"nlp_architect command fails if dev req aren't installed
detect is libraries required are install and print a msg that states the error + how to installHere's some more details about the issue. A few weeks ago, everything worked perfectly fine with version ""v0.4.post1"". I was able to execute ""nlp-architect -v"" to see the current version, and also execute the server, and use a web browser to try examples.After I updated to ""v0.4.post2"" then nothing works.I tried on both Ubuntu 16.04 LTS and 18.02 LTS. On both systems, I'm using the Intel Distro for Python. CPU backend.@javaspaces
That's true, we've split the required packages needed to operate the library and for development (changing the library).
However, we haven't addressed this on the nlp-architect command.
For the meantime please install those packages manually: pip install -r dev-requirements.txt and the command should work.
We'll push a fix in the next days.Ok, unfortunately, the installation still fails. Here's my environment:Next, I installed the release version 0.4 with this command
$ pip3 install -e .Here's the result (notice the failure in pybind11 and fasttextmirror) :So, when I try to execute the nlp-architect command, it still fails (even for the 0.4 release on a clean system. Here's the command and the result below:Now, at this step, I try to install the packages manually as you instructed, the command still fails:Now, I try to execute the command again:@javaspaces see fix that was merged today.
the correct cmd command is nlp_architectI just tried the installation process today. Again with no successHope this issue is not resolved yet. Not able to find dev-requirements.txt currently. Instead after PIP install requirements.txt, still not able to execute nlp_architect or nlp-architect command. Any help?",bug
https://github.com/tensorflow/lucid/pull/221,"It seems that sklearn.decomposition.base.BaseEstimator does not exist anymore. I've replaced it with sklearn.base.BaseEstimator and all the tests seem to pass now.I feel that it could be safer to explicitly specify versions of the packages that are required to run lucid, either in setup.py or by creating a requirements.txt file. Do you think it would be feasible?For backwards compatibility, would it make check for the existence of one BaseEstimator and fall back to the other? Many colab instances still come with the old version of sklearn pre-installed.CC @michaelpetrov for updating setup/requirements. My main concern is how this will interact with other packages and with platforms like colab that pre-install particular versions of packages.@colah, sounds doable, sure! I have a question: even if colab comes with some sklearn preinstalled, wouldn't it be possible to install appropriate packages in the very first cell from the requirements file?Hi @bmiselis -- I hope you had a great time over the winter break!I was vaguely concerned there might be further version issues with colab (eg. maybe other packages installed would break if we forced an upgrade / or conflict) but I don't know much about python packaging. If you know more and think this is fine, I'm happy to go with that. :)@colah thanks!Regarding potential conflicts I've checked that Colab comes with sklearn preinstalled with version 0.22.1 which works with the proposed update without having to fix any package versioning.It seems to me that adding the line you suggested (if there's no sklearn.decomposition try the other option) and then I'd consider being more explicit with package versions (sounds like another PR, though).By the way, check out the FutureWarning here:sklearn.base.BaseEstimator will be used in the future but I'll still leave the if statement that @colah suggested.This is great, @bmiselis! Thank you so much for the PR.",None yet
https://github.com/boudinfl/pke/pull/95,"Upon importing pke, there is a DeprecationWarning from sklearn:Importing joblib directly is now the recommended approach:
https://scikit-learn.org/stable/modules/model_persistence.html@boudinfl can you please take a look at this and approve if possible?Thanks @TimRepke",None yet
https://github.com/equinor/gordo/issues/767,TL;DRchangefrom sklearn.decomposition.pca import PCAtofrom sklearn.decomposition import PCAWarningThis was closed with #771.,None yet
https://github.com/equinor/gordo/issues/767,TL;DRchangefrom sklearn.decomposition.pca import PCAtofrom sklearn.decomposition import PCAWarningThis was closed with #771.,None yet
https://github.com/scikit-learn/scikit-learn/issues/15005,"SLEP009 is all but accepted.It proposes to make most parameters keyword-only.We should do this by first:We might along the way establish rules of thumb and principles like ""are the semantics reasonably clear when the argument is passed positionally?"" As I noted on the mailing list, I think they are clear for PCA's components, for Pipeline's steps, and for GridSearchCV's estimator and parameter grid. Other parameters of those estimators seem more suitable for keyword-only. Trickier is whether n_components in TSNE should follow PCA in being positional... It's not as commonly set by users.Realistically, this should be in 0.23. We already have too much on our plate, I think.I'm wondering whether it's easy to decide which patameters should be positional, e.g., do we think PCA(2) is reasonable? At least I don't like it.And how shall we make the decision? I guess we don't want to open a vote for every class/function, right? So is +2 enough, or do we need +3?It's about leaving some of this to the user discretion, or forcing them to use what we think best. Yes, PCA(2) is bad, while PCA(n_components) is reasonable. At least I wouldn't object to a PR using it, would you? Users can resent a limitation of their freedom (when to use or not position args) in cases when what there is no overwhelming reason for it.I would say +2 since the SLEP was accepted but wait at bit before merging to give the possibility for feedback?I don't like it but I'm not opposed to it.Hmm, not sure, but the SLEP is passed?Let's put in another way. As a user, imagine currently I have a few 1000 lines of perfectly fine code that uses PCA(n_components) (and other comparable use cases). If tomorrow it's going to start raising warnings to change it to PCA(n_components=n_components) and require me to do maintenance work without good reason, personally I would be unhappy and will complain about it to whatever project did that.@agramfort explicitly argued for accepting PCA(n_components).I really think the point made on the mailing list about allowing users to have clear expectations is important. If we can't write down a simple rule it's hard for users to have clear expectations.Doing some quick stats:There's 61 different first arguments in our estimators.Maybe having a white-list of those that we allow would be useful?
Say, 'n_components', 'alpha', 'estimator', 'base_estimator', 'kernel' (this is not for SVC), 'n_clusters', 'n_estimators', 'n_neighbors', 'C', 'steps', 'regressor', 'transformers'?Though the C is a bit of an outlier and I think having 'store_precision' be positional would not be very useful so I didn't list it. Generally I think for all meta-estimators the first argument should be positional.Agreed.I prefer narrowing the list down to just clustering, decomposition, and meta estimator parameters: 'n_components', 'estimator', 'base_estimator', 'n_clusters', 'n_neighbors', 'steps', 'regressor', 'transformers'Hm that's deprecating LogisticRegression(0.01) and RandomForestClassifier(100)`` ... I'm not super opposed but also could see some resistance?LogisticRegression(0.01) isn't a thing: the first parameter is penaltyA major company recently did a giant github scrape and analyzed sklearn usage. I asked them whether they can share their results.Hi guys,
I did some preliminary analysis for @jnothman using AST and these are the results for the aggregated data and analysis related to the repo/file.Let me know if you have any questions. I intend to upload the code (need a bit of cleaning!) I used to run this analysis soon, so you guys can have a look.Briefly a piece of code downloads the repos from a list of repos (grabbed from the dependents tree or by getting a list of repos using Google BigQuery on the GitHub public dataset - in the latter I run out of free usage credits) and convert the *.ipynb to *.py using nbconvert, then another code runs through all repo files in parallel and search, using AST, for callables on objects imported from all_estimators.
The code does not handle case where the import is of this form: from sklearn.ensemble import RandomForestClassifier as RFC.@srggrs Thank you for the analysis!It looks like nr_pos_args is bounded below by 1. Does this mean that all estimators uses at least one positional argument?Also, nr_pos_args max seems to be 1 for many estimators, which looks pretty odd to me.Because the analysis here is limited to class constructors (next version should not, I think, have this limitation), the first arg is always self, and so the lower bound of 1 makes sense.I'm happy to have in 0.22 if you still think we can have it in 0.22 @jnothman@adrinjalali I've added a list of the subpackages we've resolved this for, and those still to go, in the PR description.Once this is done, and included in the RC, we should heavily advertise the RC to make sure people discover potential issues before the final release. If there are many complaints, we might need to relax some of the most common positional arguments.Should we leave utils alone?I'll try to open a PR but I would agree there's no strong need to get it in for the releaseOkay, let's see what it looks like. In general, it would be ""nice to have"" since it would make the library more consistent and promote the usage of * in future util functions.I think this one is now complete. There may be missing ones, which we can deal with later with delayed deprecations.Thanks for all your effort to everyone involved. This is a great thing for making the parameters more findable in a year's time.",API
https://github.com/MOAISUS/pycaret/pull/29,,None
https://github.com/bharat-b7/MultiGarmentNetwork/issues/26,"I am running on python3.5, with tensorflow-gpu==1.13.1, DIRT as well as MESH.I installed dirt, and could successfully run the following code:OUTPUT:During the installation of DIRT I set the path to OpenGL and OpenEGL via Cmake Setting.But when I ran the test_network.py, I got:Hi~all.
After searching for the solution to this problem. I found:If you wanna install dirt in a developer mode in your virtualenv, when you run the code you will meet such a problem:So I just uninstalled DIRT.
And just installed it viaThen, I met the old problem:Then I just modified CMake:That means I added:Then pip install . worked well for me.
And after installed this new DIRT.The problem finally fixed.(Mayed the above process is really naive, since I am not very familiar with this. But hope this experience can help somebody.)The whole CmakeLists.txt for DIRT installation:closed.",None yet
https://github.com/optuna/optuna/pull/1090,"A minor refactoring of the StudyDirection class to increase affinity with the Study class.
By making the StudyDirection class independent with structs.py, it is easy to understand that the StudyDirection class is related to Study class.I made study_direction.py which includes the same implementation of StudyDirection with structs.py. I also added the warning of using structs.StudyDirection.This generally looks good to me. But I would say that study.py and trial.py are now so large that they are worth splitting, for example, optuna/study.py into optuna/study/base.py and optuna/study/study.py.But separating files will hide the commit history in general, so leaving them as they are is also reasonable.@crcrpar Thank you for your kind comment. I totally agree with your claim. Actually, study.py and trial.py are so large, but I think splitting them should be on another line of work. BTW, I think your proposal is worth to investigate.How about making optuna/study_direction.py private by renaming it to optuna/_study_direction.py. The StudyDirection is preferably exposed as optuna.study.StudyDirection. I think that'd make the refactoring suggested by @crcrpar less painfull as well.Continue to review full report at Codecov.Thanks for the PR. I left some minor comments if you could take a look.LGTM!I updated the title of this PR to match the format of our release notes.LGTM except for a trivial message change suggestion.Hat tip.",compatibility
https://github.com/blue-yonder/tsfresh/issues/437,"In the https://github.com/blue-yonder/tsfresh/blob/master/notebooks/robot_failure_example.ipynb
there is a deprecation error on line: from sklearn.cross_validation import train_test_split
Suggest replacing it with: from sklearn.model_selection import train_test_splitYes, those warnings are annoying. However, If we want to get rid of it, we have to update our scikit-learn dependency, even though there is no immanent need for that. I will check later how old the versions are and see if we can justify an updateThe line was replaced.",enhancement
https://github.com/scikit-learn/scikit-learn/pull/17406,"Sometimes it can be convenient to allow values (categories) in the transform of OrdinalEncoder that were not present in the fit data set. For example, a machine learning method, which is able of setting such an unknown sample of the corresponding feature to a neutral value (i.e. non-informative), could use the information from all other features of the sample and still output a prediction (instead of no prediction at all).Closes #13488
Closes #15108
Closes #16959
Closes #14534
Closes #12045
Closes #13897Lots of discussion about this in here: #16959Hi @scikit-learn/core-devs, this discussion was addressed during the last core-dev meeting.
I'm commenting here to draw some attention on this PR, as @FelixWick kindly proposes a simplified version that perhaps could help to move forward. Do you mind having a look before the next meeting? Thanks!@thomasjpfan
Please see changes as discussed in #16959.Thank you for the PR @FelixWick !Otherwise LGTM!Please add an entry to the change log at doc/whats_new/v0.24.rst. Like the other entries there, please reference this pull request with :pr: and credit yourself (and other contributors if applicable) with :user:Sorry, a few more things...LGTM (besides @jnothman's comments above). Thank you very much @FelixWick.I am not @thomasjfox :) but I wouldn't mind either option. We can always allow later if we want without breaking backward compat while the opposite would require a deprecation cycle.I agree that the future use_category option (in the input space) is more natural to map unknown values to known categories observed on the training set.I'm thomasjfox, but I'm not @thomasjpfan :) I wish I wouldn't often get mentioned on scikit-learn ^^Sorry @thomasjfox, github autocomplete fail...Thanks a lot @FelixWick for your consistent work!Made some minor comments but LGTM overall.Thanks @FelixWick , LGTM! The test failure seems unrelated (codecov upload fail)Let's ship it :)Thanks @FelixWickCan you clarify in which scikit version it will be available?
Meant we use. jdraines/cardinal_encoder: Implements a Scikit-Learn CardinalEncoder which differs from OrdinalEncoder in that it handles unknowns.great thanks
and in what version will be available unknowns in predictions for categorical nive Bayesian ,
meaning values not met in train data.
we wait it for long time ..",module:preprocessing
https://github.com/mathurinm/celer/issues/90,sklearn 0.24Importing clar gives:#92,None yet
https://github.com/scikit-learn/scikit-learn/pull/16585,"Fixes #16552
Closes #16554 (as superseded)Adds a check on the value of drop_idx_ elements: None is the value for no dropping.Thanks for the PR! I haven't entirely followed the original issue so I might be missing something. Generally it would be good to have a regression test, i.e. a test that shows that now the generated feature names are as expected with your fix.Cheers,
A.@amueller @jnothman
As I discussed in the issue, I think that it would be best to have None as a sentinel when not having any index. Basically, we could have:instead ofThe reason being that -1 could be thought to be negative indexing in Python.So if we should agree with any solution and then we can go ahead.ping @ogrisel @jeremiedbb @NicolasHug @thomasjpfan as well.Indeed, see also #16593 (that I would rather label as a New Feature, but maybe I'm missing something...). If we want to use indexes in get_feature_names then the None solution should be preferred. Could someone give me a green light on that, please? Thanks!As I would like to move forward in #15706, I have implemented drop_idx_ as array of objects, as long as I cannot use masked array I honestly prefer this solution. Some core-dev available for review? Thanks!I'm ok with this change but since it will change the result of a public attribute, shouldn't it pass through a deprecation cycle ? unless we consider it a bug :)The convention of setting -1 in drop_idx_ has not been released yet so we can still change it. I am okay with using None.@rth @thomasjpfan this was discussed during #16245 right? Could you please remind us the pros and cons of each approach?The sentinel value itself shouldn't matter, I think, other than for readability. -1 allows to keep the dtype as int. I also have a slight preference for None as was done at some point in that PR.Hi @rth, hope you don't mind I've just tried the ""Request for reviewers"" button ... :)
Thanks for you patience!Thanks @thomasjpfan !I can't really judge how many users will think -1 means negative indexing. I would want to go with the route that is less confusing for users. (If only we can do a poll)@jnothman @thomasjpfan , I will not try to persuade you , just want to add a clarification.
The reason I'd rather the None solution is that -1 and negative indexes have a specific meaning in python and I don't like the idea of ""overloading"" it in the OneHotEncoder: one day this -1 will maybe be useful (eg a last drop option ... ok, this is a stupid example, but who knows?). Now I stop bothering you. Thanks for listening.Don't hesitate to use that button on reviewers @cmarmo :) The code LGTM.Indeed, but I guess the issue here is that array index is not necessarily a positive integer, and that we are introducing a different meaning from what negative index commonly means in python. Another possibility for the sentinel could have been np.iinfo(np.int32).max () == 2147483647. Anyway any of these would likely be OK.@rth, @thomasjpfan , after discussion with @glemaitre I've finally understood that self.drop_idx_ is set to None after fit so I can use it in the checks... :)
The code is a bit different wrt the version you approved, I have added a test for this particular situation.
Maybe you can find some time to check if you are ok anyhow? Thanks a lot for your patience.LGTM apart of this small issueOh and we will need an entry in whats new:Please add an entry to the change log at doc/whats_new/v0.23.rst under bug fixes. Like the other entries there, please reference this pull request with :issue: and credit yourself (and other contributors if applicable) with :user:No need for the what's new the bug was only introduced in dev.Someone available for merging? :) Thanks a lot!Thanks @cmarmo !",module:preprocessing
https://github.com/scikit-optimize/scikit-optimize/pull/777,"Closes #762This allows CI to pass on master again and allows other PRs to carry on with the review process (e.g. this one #776).This is in my opinion a temporary fix to the problem. sklearn has deprecated fit_params in the class constructor in favor of passing keyword parameters to the fit methods. scikit-optimize should probably follow suit. However that will require a chain of deprecation action.If the maintainer agrees, I think there should be a separate issue for deprecating fit_params in BayesSearchCV.__init__ and exposing fit_params in BayesSearchCV.fit.Ah I see #772 is already on it :)Is this ready to merge or #772 is the active PR for this issue?Merged on advice of @holgern",None yet
https://github.com/Spencer-Weston/NBApredict/issues/11,"Hello and thanks for open sourcing your project,I can seem to make it work, any thoughts?C:\Users\sergi\AppData\Local\Programs\Python\Python38\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API. warnings.warn(message, FutureWarning) Season is up to date; Returning without performing an update. C:\Users\sergi\AppData\Local\Programs\Python\Python38\lib\site-packages\numpy\core\fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead. return ptp(axis=axis, out=out, **kwargs) Traceback (most recent call last): File ""C:\Users\sergi\AppData\Local\Programs\Python\Python38\lib\runpy.py"", line 193, in _run_module_as_main return _run_code(code, main_globals, None, File ""C:\Users\sergi\AppData\Local\Programs\Python\Python38\lib\runpy.py"", line 86, in _run_code exec(code, run_globals) File ""C:\Users\sergi\Desktop\NBApredict-master\run\all.py"", line 34, in <module> run_all() File ""C:\Users\sergi\Desktop\NBApredict-master\run\all.py"", line 26, in run_all predict.predict_all(db, session, year) File ""C:\Users\sergi\Desktop\NBApredict-master\predict\predict.py"", line 509, in predict_all odds_tbl = database.get_table_mappings(""odds_{}"".format(league_year)) File ""C:\Users\sergi\Desktop\NBApredict-master\database\database.py"", line 70, in get_table_mappings self.metadata.reflect(self.engine, only=table_names) File ""C:\Users\sergi\AppData\Local\Programs\Python\Python38\lib\site-packages\sqlalchemy\sql\schema.py"", line 4259, in reflect raise exc.InvalidRequestError( sqlalchemy.exc.InvalidRequestError: Could not reflect: requested table(s) not available in Engine(sqlite:///outputs//nba_db.db): (odds_2019)Hi, sorry i'm just getting to this! Long story short, I'm doing a pretty significant reorganization of the project which should increase the usability and stability. That reorganization has it's own branch. And it'll be the first place you'll get a working version. In a few weeks, that branch should be up and running at which point I'll push it to the master branch.",None yet
https://github.com/saidsef/ml-classifier/pull/8,In this PR we've:,enhancement
https://github.com/paris-saclay-cds/ramp-workflow/issues/196,Fix deprecation warnings in starting_kit of titanic (simple fixes):and this warning with the starting_kit of iris (not sure about this one):The changes should be copied over to 'ramp-kits' as well.Happy to tackle this later unless someone else is interested.,None yet
https://github.com/nusdbsystem/singa-easy/issues/44,"After the backend upgrade of the singa-auto, the singa-easy models are not able to run.@naili-xing is fixing the issue#45",None yet
https://github.com/scikit-learn/scikit-learn/pull/14815,"Closes #14807
Closes #11952Alternative to #10665This makes sure that it's possible to use a custom activation function registering new ones in sklearn.neural_network.multilayer_perceptron.{ACTIVATIONS,DERIVATIVES}.It was almost working before, except for a manual check of supported activation (instead of using the ACTIVATIONS variable), so this mostly adds a test.Not sure if we want to document it (and if so where).A minimal example would be something like,changing public variables in the scikit-learn namespace is not great, but as a somewhat experimental solution it should work (also it doesn't cost much).Sure, why not?I'm very worried about this one. Which one of the following two are we doing?I kinda don't like not documenting it, cause we'll force ourselves to deprecate the behavior if we wanna change it anyway. Which means why not document and let people use it then?We don't have to document anything if we don't judge it necessary. The one line change to the MLP file could be done just under the justification of removal of redundant code unless you think it's controversial. It happens that you can then provide custom activation by adding to these dict variables. Is it worth explicitly disallowing that? I don't know. Technically ACTIVATIONS, DERIVATIVES are defined in sklearn.neural_network._base which is private it's just that I can't monkeypatch it there (that doesn't work).I can locally import those inside MLPClassifier and remove top level imports in multilayer_perceptron.py (which would also allow using this mechanism by only changing private variables) if you think that would be preferrable?We could pass a function that returns the activation function and the derivative. The problem is that activation and derivatives are not computed at the same time (meaning it would be slower) and also add a bit more complexity to code.I would prefer that.I meant passing two functions, but I'm happy with the private variable solution as well.Thanks @rthAre we going to start recommending users to do this in our issue board?Allowing to do this via a private API, implying that we are not officially supporting it and it can change in the future, is not such a bad compromise. But it still leaves people the ability to experiment if they are so inclined. Documenting it would make this mechanism a bit more public, I'm not sure that is something we want. How do you think we should handle it?CutomThis is not helping me to work with custom activation functions in Sklearn .
Please suggest what process should I do.@ArijitN5 The PR isn't even merged yet so it's normal that the code snippet doesn't work yet.Also note that even though we are making this possible, we do not recommend it. You would be using a private API that is subject to change without deprecation.Could we consider (privately) accept a tuple of callables instead of hacking the dict?I'd be happy to publicly accepting a tuple of callables, why not?The problem is that tuple of callables doesn't generalize well to the case when more than 2 callable are needed (e.g. GLM) #14720 (comment) so it's not a general solution to this problem.hence the idea of keeping it private. it's possible, but not recommended.OK so I reverted latest changes, and can remove the test if it bothers you. So there would be only a 2 line code simplification PR. The example in the PR still uses the private API.To summarize, if we were to do this as part of the public API, we coulda) add a activation_derivative=None parameter, and accept a callable there and for activation.
b) have some registration mechanism for ACTIVATION (e.g. register_activation('name', activation, derivative)
c) accept a tuple of callable with (fn activation, fn derivative)
d) accept a callable returning a tuple
e) Accept an instance (or a named tuple) of the form .Can't say I'm too enthusiastic about any of them. Putting all constraints aside I would probably go with e) or c) (with a named tuple) .I'd be fine with not having the test, and I'm also fine with C or E. After these discussions I'm also just happy with the PR as is, but ideally I'd open an issue to have one of your suggestions implemented after this one.I'm OK with the PR as-is too.If we ever write an example we might want to use _base.ACTIVATIONS instead of the public path.I removed the test and changed the PR title. Should be good to go.Will try to open an issue about possible public API implementation later today.Merging, thanks @rthI'm OK with a public API but I think we decided not to add more features to the NN code during the meeting? So might just not be worth it",None yet
https://github.com/scikit-learn/scikit-learn/pull/13146,"Resolves #11187Adds permutation importance to a model_inspection module.Do we want to provide a meta estimator giving feature_importances_ for use the local where that's expected?Please also consider looking at eli5 for feature parity, and perhaps testing ideasHmmmm... By conducting cross validation over multiple splits, this determines feature importance for a class of model, rather than a specific model. If we are trying to inspect a specific model, surely we should not be fitting cv-many different models, but merely assessing the importance of features to prediction accuracy for the given model.This is correct. I will add a prefit option to inspect a specific model (turning off the cross validation).The CV mode isn't inspecting the model, it is using a multiple models to find the importance of the features. It is ""inspecting the data"". If the scope of the inspect module is for data and model inspection, then this CV feature could be kept in.+1 for focusing first on a tool used for the single (fitted) model inspection use case. Here are alternative implementations:Then we could think of a tool for automated feature selection using a nested cross-validation loop that can be used in Pipeline as the SelectFromModel does. However, to me, it's less of a priority.Because it's so cheap to resample the individual predictions (on the permuted validation set), we should take advantage of this to recompute the mean score on many resampled predictions (bootstrap estimates of the importance). I think it's very important that the default behavior of this tool makes it natural to get bootstrap confidence intervals on the feature importance (e.g. a 2.5%-97.5% percentile interval in addition to the median importance across resampled importances.Also, the feature importance plot in the example should use horizontal mustache/ box plots to highlight the uncertainty of this feature importance estimates:https://matplotlib.org/gallery/pyplots/boxplot_demo_pyplot.html#sphx-glr-gallery-pyplots-boxplot-demo-pyplot-pyWe could even set the opacity of feature boxplots where 0 is outside of the 2.5%-97.5% range to highlight that those features are not predictive (given the others).Here are other interesting references that I have not carefully read yet:@ogrisel Thank you for all the suggestions! I will focus this PR on inspecting a single fitted model and tune the API to make it easy to get bootstrap results.It’s a prefix I use to mean “REFACTOR”.Can you please check my and guillaume's suggestions and address the remaining comments? I'd really like to merge this.lgtmmy browser is working great for me these days..I think there were only nitpicks after @ogrisels approval, so merging.Hooray! Great work guys! :) @jph00, check it out.The things that happen while you're on the ski slopes. Congrats, @thomasjpfan!Hi Everyone,Thanks for improving the usability for feature selection through ML. I have been trying to use from sklearn.inspection import permutation_importance but it throws an error: ImportError: cannot import name 'permutation_importance'
What am I missing?
Any help would be really appreciated!
PS: Python version: 3.7.3 and sklearn version: 0.21.2Thanks for your response @jnothman, I am planning to use it for a critical project. Is it safe to use the nightly-build yet and has it been tested for all the bugs? If not, I'll wait to use it for my next project.It has been tested for all the bugs... that we encountered so far.After a major version release, users may find edge-case bugs that we couldn't catch.This doesn't have a what's new entry!!Added what's new in 9a6f05eFeel free to tweak it.This permutation importance is giving me only zeroes no matter how I choose the settings. Everything else works fine, including the default importance.@kool7d please open an issue with code to reproduce the issue. It's likely that you have strongly correlated or uninformative features. Saying that ""default importances work fine"" means that they don't detect the issue.Does the X, y arguments of this function take into account the transformations done within a pipeline setting if a pipeline is passed as the estimator?Are there plans for drop-column importance implementation?@jjakenichol Could you open an issue with the feature request. You will probably have no answer by posting on a merged PR. Thanks",None yet
https://github.com/bharat-b7/MultiGarmentNetwork/issues/25,"Hi~
I make a py3(python3.5) version of MGN on my Ubuntu16.04. (I will explain at the end.)
And successfully installed Mesh as well as Dirt. All necessary data are loaded.
But When I ran the code, I got this error:To make a py3 version, what I have done is:fromtoAlso:toAbove are all functions I modified.(That's all. I also noticed someone using 2to3 to transfer, but I think the modifications described above are enough.)Device:
I only have one GPU, thus, all my device is gpu:0
The size is about 8GB.Thanks in advance!Plus, my pkgs are as follows:It seems like to be the exact division problem caused by converting py2 to py3 from your bug report:So you should change some / to // to make sure exact division, which has different meaning between py3 and py2.For example in file YOUR_PATH_TO_MGN/MultiGarmentNetwork/network/custom_layers.py there are two places you need to change like followings:There are some others places you need to change / to // but I don't rermenber them all , may you should check it by yourself , good luck to you!Thanks! I make it! Thanks for your kind help and guidande!
Yet, after fixing this problem, I met another one:
#26
Actually, I also tested on Python2.7 before with DIRT for py2.7 successfully installed. I also got this error (At first I think it is caused by python's version, that's why I choose py3.5. But now it comes again...).Anyway, thank you again for your help!
Best,
Frank",None yet
https://github.com/stellargraph/stellargraph/issues/419,"In the Cora link prediction demo,__getitem__ in node2vec_feature_learning.py is deprecated. This results in deprecation warnings 5 times in the demo. The demo runs smoothly but doesn't look pretty due to the warnings.I'm not able to reproduce this, although I'm seeing some other deprecation warnings - has this been resolved, or can you reproduce the full log? @habiba-hGetting the same running the notebook instead of the scriptMy python environment has the following:@kjun9 the warning from Node2VecFeatureLearning is gone. And I am getting the same warnings as you. The tensorflow and tensorboard warnings are ubiquitous. But the convergence warnings from sklearn are specific to this demo.Thanks for confirming! I've created #565 for the new warnings",enhancement
https://github.com/scikit-learn/scikit-learn/pull/17062,"CC @scikit-learn/core-devs please review or directly edit and add new entries as you pleaseCC @cmarmo tooI think the positional deprecation also deserves to be here.I'd be fine with this focusing on enhancements and new features.HTML vis belongs here when merged.sample_weight in Lasso and ElasticNet?Changing away from Boston in examples? (a good media piece?)Otherwise I suspect we're not going to list much more for this release. A lot of work behind the scenes for this one!!Thanks Joel, I added a section about Lasso and ENet and separated the mnt_cstRe positional stuff: I agree with Joel that this isn't really a feature. I feel like the top entry in the what's new would be enoughRe usage of Boston dataset: same, I feel like this would be a nice top-entry in the what's new, but not sure about including it in the highlights?@NicolasHug, apparently people prefer the # %% syntax wrt ########... (#17063, and #17068), maybe it is worth it to start using it in new PRs?yes, same for #16648I'll update when / if the PR gets merge. I personally prefer the current formI'd also happy to have the boston dataset change as a highlight here.Stories are always better told in a blog than in a software documentation, even if it is the best documentation in the world like yours ... ;)
Anyway, after the meeting I was under the impression that core-devs only want one reference for the 'what's new' communication, am I wrong?My understanding was that if the foundation website is to have an article similar to this one for 0.23, then it should make clear that the ""official"" highlights are in the gallery, and link to itI personally agree a blog post is better suited both for Boston and for the kwonly.Small comment on the ordering of the sections:Comments addressed and added entry for html repr@ogrisel @thomasjpfan @rth @adrinjalali @glemaitre @jeremiedbb @TomDLT I think we can get this in nowOtherwise +1.Thanks @ogrisel @TomDLT , your comments were addressedWill merge when green.Looks good to me.Tiny nitpicks: I wouldn't doon the same line. It's not advised in Python styling.@adrinjalali I'm not sure what PR I should ping anymore but this should be in the release ;)I guess we can continue pinging #17010",None yet
https://github.com/scikit-learn/scikit-learn/issues/14781,"probA_ and probB_ are attributes inherited from the base class libsvm in the SVR class. However SVR does not allow for a probability = True as an input like the libsvm class does, therefore these attributes always return an empty array. I'm not sure they should be included here.Link to code: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/svm/classes.py#L824They should indeed be removed / deprecated. I think someone might be working on that already (or did I talk to you about that earlier?)
See https://scikit-learn.org/dev/developers/contributing.html#deprecationdo you want to work on this?
You can talk to @meiguan who does something similar for #14766.Ok sounds goodWe are working on the same issue but for OneClassSVM @eugeniaftI think this is closed.@MZeej I don't think so, can you double check? There was no fixed merged, right?Looks like it's still there on master:@thomasjpfan can you remind me why these were closed without getting merged? The deprecation still seems reasonable and necessary.@amueller They were closed by the contributors.",Easy good first issue help wanted
https://github.com/mitre-attack/tram/issues/17,"After submitting a new url the review is empty, however source button has content (url submitted) and when i click Analyze button retorned '404: Not Found'.
Not sure if i've missed something or it's something that have not been implemented.URL tested: https://www.fireeye.com/blog/threat-research/2017/12/targeted-attack-in-middle-east-by-apt34.htmlI just ran the report myself and was able to find some techniques. 1) Are you behind a proxy that might not allow TRAM to reach out to the internet? 2) Are there any other error messages besides the 404? 3) Try deleting the database and rebuildingHi connor,I've checked your advices but with no success.Log status:
INFO:aiohttp.access:127.0.0.1 [09/Jan/2020:08:06:50 +0000] ""GET / HTTP/1.1"" 200 9011 ""http://localhost:9999/"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0""
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.fireeye.com:443
DEBUG:urllib3.connectionpool:https://www.fireeye.com:443 ""GET /blog/threat-research/2017/12/targeted-attack-in-middle-east-by-apt34.html HTTP/1.1"" 200 None
<class 'str'>
[#] Loading models from pickled file: model_dict.p
/home/pijachu/.local/lib/python3.6/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.1 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)
/home/pijachu/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)
/home/pijachu/.local/lib/python3.6/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.1 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)
INFO:aiohttp.access:127.0.0.1 [09/Jan/2020:08:07:11 +0000] ""POST /rest HTTP/1.1"" 200 160 ""http://localhost:9999/"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0""
INFO:aiohttp.access:127.0.0.1 [09/Jan/2020:08:08:01 +0000] ""GET / HTTP/1.1"" 200 10192 ""http://localhost:9999/"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0""
INFO:aiohttp.access:127.0.0.1 [09/Jan/2020:08:08:01 +0000] ""GET /theme/fonts/glyphicons-halflings-regular.woff2 HTTP/1.1"" 200 18249 ""http://localhost:9999/theme/style/bootstrap-glyphicon.min.css"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0""
INFO:aiohttp.access:127.0.0.1 [09/Jan/2020:08:08:05 +0000] ""GET /edit/ HTTP/1.1"" 404 172 ""http://localhost:9999/"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0""
INFO:aiohttp.access:127.0.0.1 [09/Jan/2020:08:08:05 +0000] ""GET /favicon.ico HTTP/1.1"" 404 172 ""-"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0""Screenshot: Attached.PS: Database has been deleted and rebuilding againSeems something goes wrong after loading a model template...[#] Loading models from pickled file: model_dict.p
/home/pijachu/.local/lib/python3.6/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.1 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)
/home/pijachu/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.
warnings.warn(message, FutureWarning)
/home/pijachu/.local/lib/python3.6/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.1 when using version 0.22. This might lead to breaking code or invalid results. Use at your own risk.
UserWarning)Tested in Windows environment with same resultSome errors fixed with 0.21.1 version scikit-learn for the model.To do that:
pip uninstall scikit-learn
pip install -v scikit-learn==0.21.1However, the issue still persistsHave you checked the current DB and ensured there is data in there? in a terminal go to the database directory and try:
sqlite3 tram.db
select * from attack_uids;Hi Sarah,Table attack_uids:
630 rows
(screenshot attached)Table reports:
4 rows
(screenshot attached)It doesn't seem to be a database problem.I think I have tracked down the root of the problem, I believe it was not well communicated that there was a update to the database schema, so it needs to be rebuilt. Try deleting the tram.db file and starting the program again. We will be adding in line database upgrades in the future.Hi Sarah,I've just rebuilt the database but the problem still persist.Anyway, thank you very much for your help.@Cr4ck3nCV I faced similar issue.
to solve, you need to give title and don't leave that field empty :)because analyze uses title as part of its endpoint:
e.g., title: test
report: http://localhost:9999/edit/testFixed! If the title is empty it doesn't work. Thank you @chubbymaggie",None yet
https://github.com/onnx/sklearn-onnx/issues/336,"Sorry if this is covered somewhere, but is there a python 2 version of sklearn-onnx? The version I install with pip3 works fine. pip2 also happily installs a version of sklearn-onnx (1.6.0) into my python 2.7 local dir, but the version it installs seems to be python 3 code (it crashes on import in a number of places due to python 3-specific syntax).I realize we're not even a month from python 2.7 deprecation, so I can't complain if sklearn-onnx only supports python 3, but if so maybe it would be possible to have pypi reflect this?There is no plan to support 2.7. Many packages stop supporting 2.7. And even if we do so, onnxruntime does not support python 2.7 and we would be unable to check that the converted models return the same values as the original models.Ok, thanks for the reply. I totally understand lack of 2.7 support in 2019. That said, I would still expect the behavior when pip2 is asked to install sklearn-onnx to be a crash or warning of some sort, rather than blithely installing the python 3 version into pip's python 2.7 ~/.local directory. But I'd understand if this was pip's problem rather than sklearn-onnx.",None yet
https://github.com/MOAISUS/pycaret/pull/23,Bumps catboost from 0.20.2 to 0.23.1.Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting @dependabot rebase.Superseded by #29.,dependencies
https://github.com/tahini/tracecompass-ease-scripting/pull/2,"Adds 2 python scripts using clustering libraries from SciPy and Sklearn. The analysis clusters system calls based on their duration, then outputs a xy graph with matplotlib (x for system call duration, y for system call name).Thanks for the scripts! It's great to have example of ML libraries with trace data!Some comments though.Also, I know these are meant to be examples scripts, but I am not very familiar with ML and the output of those example scripts has no meaning to me:For example, ""{'ioctl': 0, 'splice': 1, 'sync_file_range2': 2, 'arm_fadvise64_64': 3, 'clock_gettime': 4, 'read': 5..."", I don't know what it means. But I get errors, maybe there is supposed to be more outputs after that? Anyway, if you could provide some insight in the output, something that will help lay[wo]men make sense of this example, that would be great!With new updates with py4j vs jython scripting engine, these scripts are good examples of mixed engine scripts, and I managed to have them working properly with python2. So I'll re-work then with that in mind and merge this PR.Then we can document that it works only with python2? or you tell me why it does not work with python 3. And let's fix it if possible!@damadux Could you take a look at the damadux-master branch that I update in this repo? If it's ok with you, I'll merge it with master and close this PR. Thanks!Looks good! I will test these with py4j and jython.now merged. Thanks! Test it please, I'll let you figure out how to do so, from the doc and eventual output messages. If it's not clear, patches welcome!Did you still have an error running these examples with Python 3? I had no issues with my machine.Yes, but setting this: matplotlib.use(""GTK3Agg"") fixed it. It was using Qt5 backend I think and there there is a signal that needs to be run from the UI thread. So it's not really python3 problem after all, but pyplot backend. The script in master has that fix.",None yet
https://github.com/scikit-learn/scikit-learn/issues/15842,"Since scikit-learn 0.22, the dict_learning function is defined in sklearn/decomposition/_dict_learning.py and exposed as sklearn.decomposition.dict_learning in the sklearn/decomposition/__init__.py file.At the same time we also have a sklearn/decomposition/dict_learning.py deprecated module for backward compat. This module is not imported by default in sklearn/decomposition/__init__.py to avoid deprecation warning.So far so good. The problem is that if the users or a tool later import sklearn.decomposition.dict_learning (the deprecated module), then the symbol sklearn.decomposition.dict_learning points to the module instead of the function:This can happen (semi-randomly?) when running test discovery with pytest --pyargs sklearn from a non-source folder (without the scikit-learn conftest.py file) or when calling all_estimators() from scikit-learn.Note that in 0.21.3 we did not have the problem because once the sklearn.decomposition.dict_learning module is imported once, it is cached, and therefore, sklearn.decomposition.dict_learning is always referring to the function.We have the same problem for the sklearn.inspection.partial_dependence function / module.I will open a PR with a possible fix.We saw that before with @thomasjpfanThis is only an issue with pytest discovery. If you run the test files independently, you shouldn't get errors.I thought we fixed that by bumping the minimal pytest version, but looking at the git blame and my commit history, I cannot find where/if this was actually done :/The code snippet above is not using pytest.If I recall correctly, it depends on the pytest version as well: https://github.com/scikit-learn/scikit-learn/blob/master/azure-pipelines.yml#L60Oh ups thenYes but we do expect the warning to be raised when doing import sklearn.decomposition.dict_learning, right?Yes. I am trying to come of with a solution. The following deprecated module snippet works:I need to find a way to make the deprecated_modules.py script generate this automatically.Actually I must have made something wrong the first time I tested it because the above snippet does not seem to work. The import mechanism change the parent attribute once the module has been fully imported hence any monkeypatching logic happening here won't work.I opened #15846 but unfortunately it hides the warnings for those 2 modules.I realized that I did not answer this question: in 0.21. Let me answer it now for future reference:In 0.21, doing from sklearn.decomposition import dict_learning you actually trigger the import of the sklearn.decomposition.dict_learning module before setting the attribute sklearn.decomposition.dict_learning to the function.If after you do from sklearn.decomposition.dict_learning import dict_learning, the sklearn.decomposition.dict_learning is already imported and in the sys.modules cache, therefore nothings happen and the sklearn.decomposition.dict_learning attribute still points to the function.",Bug
https://github.com/tensorflow/tensorflow/issues/30918,"System informationDescribe the current behavior
The training data is pre-processed and loaded into memory. The model is compiled and the correct model output is produced with model.summary(). See logs bellow...On model.fit(), nothing happens... The GPU is at 3% utilization and one CPU core is at 100%.Describe the expected behavior
I was expecting the keras training logging to be printed post model.fit(). It doesn't appear to be training at all.Code to reproduce the issueCustom Code:BertLayerOther info / logsmodel.summary()Other logs:How long did you wait for?In the background (it is not printed to console) the weights for the TF Hub module will have to be downloaded (500MB for BERTBASE, 1GB for BERTLARGE), and then the BERT layer instantiated. This does take a fair amount of time for the download, and then to build the graph.Also, note that in the medium post they made a few mistakes e.g. the way they implemented fine_tune_layers is completely wrong.I have a much improved version in this Colab notebook:
https://colab.research.google.com/github/NVAITC/examples/blob/master/keras_bert_amp.ipynb
You are free to take this and use it.I waited for about 48 hours...I will have a look and implement this version and post back my findings.You rock! Thanks.@tlkhI have taken the BERT class from your script and adjusted my code to fit; however, I am still getting the same issue. Training fails to start. I waited 6 hours. Network i/o was not evidencing any download either.** main.py **@jordanparker6 Thanks for the bug! verbose=2 will only print 1 line per epoch. Could it be that your Model is taking a huge amount of time to run through an epoch? Try verbose=1, which outputs on every batch. Please let me know if you still don't see any output@omalleyt12 No change in the output post changing the verbose level. I let it run for 4 days...The only thing that stand out to me is that model.fit(..., use_multiprocessing=True) should be False here, as True is only relevant to keras.Sequence or generator objects. Could you try that and let me know if it starts training?Sorry for the late reply. I will try it tonight and get back to you.@omalleyt12 that didn't work either...@omalleyt12 is the fact that i am using a GTX1080ti with 11gb of GPU RAM an issue?@jordanparker6
Is this still an issue@Saduf2019 no many other libraries have provided a work around. See huggingfaces transformer library or fastbert.@jordanparker6
IN that case please confirm if we may move this to closed status.This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.Closing as stale. Please reopen if you'd like to work on this further.Are you satisfied with the resolution of your issue?
Yes
No",TF 1.14 comp:keras stalled stat:awaiting response type:bug
https://github.com/tensorflow/tensorflow/issues/42197,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_templateSystem informationDescribe the problemProvide the exact sequence of commands / steps that you executed before running into the problemAny other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.After update Anaconda with:
conda update --allImportError Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in
63 try:
---> 64 from tensorflow.python._pywrap_tensorflow_internal import *
65 # This try catch logic is because there is no bazel equivalent for py_extension.ImportError: DLL load failed: No se puede encontrar el módulo especificado.During handling of the above exception, another exception occurred:ImportError Traceback (most recent call last)
in
1 # And the tf and keras framework, thanks to Google
----> 2 import tensorflow as tf
3 from tensorflow import kerasC:\ProgramData\Anaconda3\lib\site-packages\tensorflow_init_.py in
39 import sys as _sys
40
---> 41 from tensorflow.python.tools import module_util as _module_util
42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
43C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python_init_.py in
38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
39
---> 40 from tensorflow.python.eager import context
41
42 # pylint: enable=wildcard-importC:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\context.py in
33 from tensorflow.core.protobuf import config_pb2
34 from tensorflow.core.protobuf import rewriter_config_pb2
---> 35 from tensorflow.python import pywrap_tfe
36 from tensorflow.python import tf2
37 from tensorflow.python.client import pywrap_tf_sessionC:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tfe.py in
26
27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import
---> 28 from tensorflow.python import pywrap_tensorflow
29 from tensorflow.python._pywrap_tfe import *C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in
81 for some common reasons and solutions. Include the entire stack trace
82 above this error message when asking for help."""""" % traceback.format_exc()
---> 83 raise ImportError(msg)
84
85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-longImportError: Traceback (most recent call last):
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in
from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: No se puede encontrar el módulo especificado.Failed to load the native TensorFlow runtime.See https://www.tensorflow.org/install/errorsfor some common reasons and solutions. Include the entire stack trace
above this error message when asking for help.I would appreciate your help to solve my issue.@Libardo1What is make/model of your cpu?
I suspect your cpu model does not support AVX instructions sets.See hardware requirements
Make sure to download the latest microsoft visual c++ redistributable from here.
.Also, please follow the instructions from to install from Tensorflow website.Did you try with tested build configuration from here and see if you are facing the issue.Please, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204This issue is more suitable on Continuum Anaconda repo since its related to TF installation with Anaconda.
Please post it on Continuum Anaconda.
Thanks!(base) PS C:\Users\Libardo> conda infopopulated config files : C:\Users\Libardo.condarc
conda version : 4.8.4
conda-build version : 3.19.2
python version : 3.7.8.final.0
virtual packages :
base environment : C:\ProgramData\Anaconda3 (writable)
channel URLs : https://conda.anaconda.org/conda-forge/win-64
https://conda.anaconda.org/conda-forge/noarch
https://conda.anaconda.org/anaconda-fusion/win-64
https://conda.anaconda.org/anaconda-fusion/noarch
https://repo.anaconda.com/pkgs/main/win-64
https://repo.anaconda.com/pkgs/main/noarch
https://repo.anaconda.com/pkgs/r/win-64
https://repo.anaconda.com/pkgs/r/noarch
https://repo.anaconda.com/pkgs/msys2/win-64
https://repo.anaconda.com/pkgs/msys2/noarch
package cache : C:\ProgramData\Anaconda3\pkgs
C:\Users\Libardo.conda\pkgs
C:\Users\Libardo\AppData\Local\conda\conda\pkgs
envs directories : C:\ProgramData\Anaconda3\envs
C:\Users\Libardo.conda\envs
C:\Users\Libardo\AppData\Local\conda\conda\envs
platform : win-64
user-agent : conda/4.8.4 requests/2.24.0 CPython/3.7.8 Windows/10 Windows/10.0.19041
administrator : False
netrc file : None
offline mode : False(base) PS C:\Users\Libardo> conda list --show-channel-urls_anaconda_depends 2019.03 py37_0 anaconda
_ipyw_jlab_nb_ext_conf 0.1.0 py37_0 defaults
_tflow_select 2.1.0 gpu anaconda
abseil-cpp 20200225.2 ha925a31_2 conda-forge
absl-py 0.8.1 pypi_0 pypi
affine 2.3.0 py_0 conda-forge
alabaster 0.7.12 py_0 conda-forge
anaconda custom py37_1 defaults
anaconda-client 1.7.2 py_0 conda-forge
anaconda-navigator 1.9.12 py37_0 defaults
anaconda-project 0.8.3 py_0 conda-forge
analytics-python 1.2.9 pypi_0 pypi
aniso8601 8.0.0 pypi_0 pypi
appdirs 1.4.3 pypi_0 pypi
archspec 0.1.1 pyh9f0ad1d_0 conda-forge
arctic 1.79.2 pypi_0 pypi
argh 0.26.2 pyh9f0ad1d_1002 conda-forge
argon2-cffi 20.1.0 py37h4ab8f01_1 conda-forge
arrow-cpp 1.0.0 py37h1234567_1_cpu conda-forge
arviz 0.6.1 pypi_0 pypi
asn1crypto 1.4.0 pyh9f0ad1d_0 conda-forge
astor 0.8.0 pypi_0 pypi
astroid 2.4.2 py37hc8dfbb8_0 conda-forge
astropy 4.0.1.post1 py37h8055547_0 conda-forge
astunparse 1.6.3 pypi_0 pypi
atomicwrites 1.4.0 pyh9f0ad1d_0 conda-forge
attrs 19.3.0 py_0 conda-forge
autoimpute 0.11.6 pypi_0 pypi
autokeras 1.0.2 pypi_0 pypi
autopep8 1.5.4 pyh9f0ad1d_0 conda-forge
aws-sdk-cpp 1.7.164 vc14h84f8083_2 conda-forge
babel 2.8.0 py_0 conda-forge
backcall 0.2.0 pyh9f0ad1d_0 conda-forge
backports 1.0 py_2 conda-forge
backports.functools_lru_cache 1.6.1 py_0 conda-forge
backports.os 0.1.1 py37hc8dfbb8_1002 conda-forge
backports.shutil_get_terminal_size 1.0.0 py_3 conda-forge
bamboolib 0.1.1 pypi_0 pypi
bcrypt 3.1.7 py37h8055547_1 conda-forge
beautifulsoup4 4.9.1 py37hc8dfbb8_0 conda-forge
bert-for-tf2 0.13.5 pypi_0 pypi
bitarray 1.4.2 py37h4ab8f01_0 conda-forge
bkcharts 0.2 py37_0 defaults
blas 1.0 mkl defaults
bleach 3.1.5 pyh9f0ad1d_0 conda-forge
blinker 1.4 py_1 conda-forge
blosc 1.20.0 ha925a31_0 conda-forge
bokeh 2.1.1 py37hc8dfbb8_0 conda-forge
boost-cpp 1.72.0 h89d28cc_2 conda-forge
boto 2.49.0 py_0 conda-forge
boto3 1.10.34 pypi_0 pypi
botocore 1.13.34 pypi_0 pypi
bottleneck 1.3.2 py37hbc2f12b_1 conda-forge
branca 0.3.1 py_0 conda-forge
brotli 1.0.7 ha925a31_1004 conda-forge
brotlipy 0.7.0 py37h4ab8f01_1000 conda-forge
bs4 0.0.1 pypi_0 pypi
bzip2 1.0.8 hfa6e2cd_2 conda-forge
c-ares 1.16.1 h62dcd97_0 conda-forge
ca-certificates 2020.6.24 0 anaconda
cachetools 4.1.1 py_0 conda-forge
causalgraphicalmodels 0.0.4 pypi_0 pypi
causalimpact 0.1.6 pypi_0 pypi
causalinference 0.1.2 pypi_0 pypi
certifi 2020.6.20 py37_0 anaconda
cffi 1.14.1 py37h26f1ce3_0 conda-forge
cfitsio 3.470 hbbe6aef_6 conda-forge
cftime 1.0.4.2 pypi_0 pypi
chardet 3.0.4 py37hc8dfbb8_1006 conda-forge
charls 2.1.0 h33f27b4_2 conda-forge
chart-studio 1.1.0 pyh9f0ad1d_0 conda-forge
click 7.1.2 pyh9f0ad1d_0 conda-forge
click-plugins 1.1.1 py_0 conda-forge
cligj 0.5.0 py_0 conda-forge
cloudpickle 1.5.0 py_0 conda-forge
clyent 1.2.2 py_1 conda-forge
cmdstanpy 0.4.0 pypi_0 pypi
colorama 0.4.3 py_0 conda-forge
comtypes 1.1.7 py37hc8dfbb8_1001 conda-forge
conda 4.8.4 py37hc8dfbb8_2 conda-forge
conda-build 3.19.2 py37hc8dfbb8_3 conda-forge
conda-env 2.6.0 1 conda-forge
conda-package-handling 1.6.0 py37h702c6c1_2 conda-forge
conda-verify 3.1.1 py37hc8dfbb8_1001 conda-forge
confound-prediction 0.0.1a1 pypi_0 pypi
console_shortcut 0.1.1 4 defaults
contextlib2 0.6.0.post1 py_0 conda-forge
conv 0.2 pypi_0 pypi
convertdate 2.2.1 pyh9f0ad1d_0 conda-forge
cryptography 3.0 py37h26f1ce3_0 conda-forge
cudatoolkit 10.1.243 h74a9793_0 defaults
cudnn 7.6.5 cuda10.1_0 anaconda
curl 7.71.1 h4b64cdc_4 conda-forge
cvxpy 1.0.25 pypi_0 pypi
cycler 0.10.0 py_2 conda-forge
cython 0.29.21 py37h1834ac0_0 conda-forge
cytoolz 0.10.1 py37hfa6e2cd_0 conda-forge
dask 2.22.0 py_0 conda-forge
dask-core 2.22.0 py_0 conda-forge
datetime 4.3 pypi_0 pypi
decorator 4.4.2 py_0 conda-forge
defusedxml 0.6.0 py_0 conda-forge
deprecation 2.0.7 pypi_0 pypi
diff-match-patch 20200713 pyh9f0ad1d_0 conda-forge
dill 0.3.1.1 pypi_0 pypi
distributed 2.22.0 py37hc8dfbb8_0 conda-forge
docutils 0.16 py37hc8dfbb8_1 conda-forge
dowhy 0.1.1 pypi_0 pypi
dtale 1.5.0 pypi_0 pypi
econml 0.5 pypi_0 pypi
ecos 2.0.7.post1 pypi_0 pypi
eikon 1.0.1 pypi_0 pypi
entrypoints 0.3 py37hc8dfbb8_1001 conda-forge
enum-compat 0.0.3 pypi_0 pypi
ephem 3.7.7.1 py37hfa6e2cd_0 conda-forge
et_xmlfile 1.0.1 py_1001 conda-forge
expat 2.2.9 he025d50_2 conda-forge
fancyimpute 0.5.4 pypi_0 pypi
fastcache 1.1.0 py37h8055547_1 conda-forge
fbprophet 0.6 py37h6538335_0 conda-forge
filelock 3.0.12 pyh9f0ad1d_0 conda-forge
findspark 1.3.0 py_1 conda-forge
fiona 1.8.13 py37hef9e828_1 conda-forge
flake8 3.8.3 py_1 conda-forge
flask 1.1.2 pyh9f0ad1d_0 conda-forge
flask-compress 1.4.0 pypi_0 pypi
flask-restful 0.3.8 pypi_0 pypi
freetype 2.10.2 hd328e21_0 conda-forge
freexl 1.0.5 hd288d7e_1002 conda-forge
fsspec 0.8.0 py_0 conda-forge
future 0.18.2 py37hc8dfbb8_1 conda-forge
gast 0.3.3 pypi_0 pypi
gdal 3.0.4 py37hd44be9e_7 conda-forge
geojson 2.5.0 py_0 conda-forge
geopandas 0.8.1 py_0 conda-forge
geos 3.8.1 he025d50_0 conda-forge
geotiff 1.5.1 h3d29ae3_10 conda-forge
get_terminal_size 1.0.0 h38e98db_0 defaults
gettext 0.19.8.1 hb01d8f6_1002 conda-forge
gevent 20.6.2 py37h4ab8f01_0 conda-forge
gflags 2.2.2 ha925a31_1004 conda-forge
giflib 5.2.1 h2fa13f4_2 conda-forge
glib 2.65.0 he4de6d7_0 conda-forge
glob2 0.7 py_0 conda-forge
glog 0.4.0 h0174b99_3 conda-forge
gmaps 0.9.0 py_0 conda-forge
google-auth 1.7.1 pypi_0 pypi
google-auth-oauthlib 0.4.1 py_2 conda-forge
google-pasta 0.1.8 pypi_0 pypi
greenlet 0.4.16 py37h4ab8f01_0 conda-forge
grpc-cpp 1.30.1 h45b88af_1 conda-forge
grpcio 1.25.0 pypi_0 pypi
h5py 2.10.0 nompi_py37hde23a51_104 conda-forge
hdf4 4.2.13 hf8e6fe8_1003 conda-forge
hdf5 1.10.6 nompi_he0bbb20_101 conda-forge
heapdict 1.0.1 py_0 conda-forge
hiplot 0.1.1 pypi_0 pypi
holidays 0.10.3 pyh9f0ad1d_0 conda-forge
html5lib 1.1 pyh9f0ad1d_0 conda-forge
hummingbird-ml 0.0.2 pypi_0 pypi
hupper 1.10.2 py_0 conda-forge
hypothesis 5.24.0 py_0 conda-forge
icc_rt 2019.0.0 h0cc432a_1 defaults
icu 64.2 he025d50_1 conda-forge
idna 2.10 pyh9f0ad1d_0 conda-forge
imagecodecs 2020.5.30 py37h92c78e3_2 conda-forge
imageio 2.9.0 py_0 conda-forge
imagesize 1.2.0 py_0 conda-forge
importlib-metadata 1.7.0 py37hc8dfbb8_0 conda-forge
importlib_metadata 1.7.0 0 conda-forge
inflection 0.4.0 pypi_0 pypi
iniconfig 1.0.1 pyh9f0ad1d_0 conda-forge
intel-openmp 2019.4 245 defaults
intervaltree 3.0.2 py_0 conda-forge
ipykernel 5.3.4 py37h5ca1d4c_0 conda-forge
ipyleaflet 0.13.3 pyh9f0ad1d_0 conda-forge
ipython 7.17.0 py37hc6149b9_0 conda-forge
ipython_genutils 0.2.0 py_1 conda-forge
ipywidgets 7.5.1 py_0 conda-forge
isort 4.3.21 py37hc8dfbb8_1 conda-forge
itsdangerous 1.1.0 py_0 conda-forge
jdcal 1.4.1 py_0 conda-forge
jedi 0.15.2 py37_0 conda-forge
jinja2 2.11.2 pyh9f0ad1d_0 conda-forge
jmespath 0.9.4 pypi_0 pypi
joblib 0.16.0 py_0 conda-forge
jpeg 9d he774522_0 conda-forge
json5 0.9.4 pyh9f0ad1d_0 conda-forge
jsonschema 3.2.0 py37hc8dfbb8_1 conda-forge
jupyter 1.0.0 py_2 conda-forge
jupyter_client 6.1.6 py_0 conda-forge
jupyter_console 6.1.0 py_1 conda-forge
jupyter_core 4.6.3 py37hc8dfbb8_1 conda-forge
jupyterlab 2.2.4 py_0 conda-forge
jupyterlab_server 1.2.0 py_0 conda-forge
jxrlib 1.1 hfa6e2cd_2 conda-forge
kealib 1.4.13 h3b59ab9_1 conda-forge
keras 2.3.1 py37h21ff451_0 conda-forge
keras-applications 1.0.8 py_1 conda-forge
keras-preprocessing 1.1.2 pypi_0 pypi
keras-tuner 1.0.1 pypi_0 pypi
keyring 21.3.0 py37hc8dfbb8_0 conda-forge
kiwisolver 1.2.0 py37heaa310e_0 conda-forge
kmeans1d 0.2.0 pypi_0 pypi
knnimpute 0.1.0 pypi_0 pypi
korean_lunar_calendar 0.2.1 pyh9f0ad1d_0 conda-forge
krb5 1.17.1 hc04afaa_2 conda-forge
lazy-object-proxy 1.4.3 py37h8055547_2 conda-forge
lcms2 2.11 he1115b7_0 conda-forge
lerc 2.2 ha925a31_0 conda-forge
libaec 1.0.4 he025d50_1 conda-forge
libarchive 3.3.3 h0c0e0cf_1008 conda-forge
libblas 3.8.0 14_mkl conda-forge
libcblas 3.8.0 14_mkl conda-forge
libclang 9.0.1 default_hf44288c_0 conda-forge
libcurl 7.71.1 h4b64cdc_4 conda-forge
libffi 3.2.1 h6538335_1007 conda-forge
libgdal 3.0.4 hf164de3_7 conda-forge
libgpuarray 0.7.6 hfa6e2cd_1003 conda-forge
libiconv 1.15 hfa6e2cd_1006 conda-forge
libkml 1.3.0 h7e985d0_1011 conda-forge
liblapack 3.8.0 14_mkl conda-forge
liblief 0.10.1 ha925a31_0 defaults
libnetcdf 4.7.4 nompi_h256d12c_105 conda-forge
libpng 1.6.37 hfe6a214_1 conda-forge
libpq 12.3 hd9aa61d_0 conda-forge
libprotobuf 3.12.4 h200bbdf_0 conda-forge
libpython 2.0 py37hc8dfbb8_0 conda-forge
libsodium 1.0.17 h2fa13f4_0 conda-forge
libspatialindex 1.9.3 he025d50_3 conda-forge
libspatialite 4.3.0a h51df0ed_1038 conda-forge
libssh2 1.9.0 hb06d900_5 conda-forge
libtiff 4.1.0 h885aae3_6 conda-forge
libutf8proc 2.5.0 h9e6e254_2 conda-forge
libwebp-base 1.1.0 hfa6e2cd_3 conda-forge
libxml2 2.9.10 h5d81f1c_2 conda-forge
libxslt 1.1.33 h579f668_1 conda-forge
libzopfli 1.0.3 ha925a31_0 conda-forge
lightgbm 2.3.1 pypi_0 pypi
llvm-meta 9.0.1 0 conda-forge
llvmlite 0.33.0 py37h8b575af_1 conda-forge
lmu 0.1 dev_0
locket 0.2.0 py_2 conda-forge
lunarcalendar 0.0.9 py_0 conda-forge
lxml 4.5.2 py37h8ba8a40_0 conda-forge
lz4 2.2.1 pypi_0 pypi
lz4-c 1.9.2 h62dcd97_1 conda-forge
lzo 2.10 hfa6e2cd_1000 conda-forge
m2-msys2-runtime 2.5.0.17080.65c939c 3 defaults
m2-patch 2.7.5 2 defaults
m2w64-binutils 2.25.1 5 defaults
m2w64-bzip2 1.0.6 6 defaults
m2w64-crt-git 5.0.0.4636.2595836 2 defaults
m2w64-expat 2.1.1 2 defaults
m2w64-gcc 5.3.0 6 defaults
m2w64-gcc-ada 5.3.0 6 defaults
m2w64-gcc-fortran 5.3.0 6 defaults
m2w64-gcc-libgfortran 5.3.0 6 defaults
m2w64-gcc-libs 5.3.0 7 defaults
m2w64-gcc-libs-core 5.3.0 7 defaults
m2w64-gcc-objc 5.3.0 6 defaults
m2w64-gettext 0.19.7 2 defaults
m2w64-gmp 6.1.0 2 defaults
m2w64-headers-git 5.0.0.4636.c0ad18a 2 defaults
m2w64-isl 0.16.1 2 defaults
m2w64-libiconv 1.14 6 defaults
m2w64-libmangle-git 5.0.0.4509.2e5a9a2 2 defaults
m2w64-libwinpthread-git 5.0.0.4634.697f757 2 defaults
m2w64-make 4.1.2351.a80a8b8 2 defaults
m2w64-mpc 1.0.3 3 defaults
m2w64-mpfr 3.1.4 4 defaults
m2w64-pkg-config 0.29.1 2 defaults
m2w64-toolchain 5.3.0 7 defaults
m2w64-tools-git 5.0.0.4592.90b8472 2 defaults
m2w64-windows-default-manifest 6.4 3 defaults
m2w64-winpthreads-git 5.0.0.4634.697f757 2 defaults
m2w64-xz 5.2.2 2 defaults
m2w64-zlib 1.2.8 10 defaults
mako 1.1.3 pyh9f0ad1d_0 conda-forge
markdown 3.1.1 pypi_0 pypi
markupsafe 1.1.1 py37h8055547_1 conda-forge
matplotlib 3.3.0 1 conda-forge
matplotlib-base 3.3.0 py37h35e8a6e_1 conda-forge
mccabe 0.6.1 py_1 conda-forge
menuinst 1.4.16 py37_0 conda-forge
metaflow 2.0.0 pypi_0 pypi
missingno 0.4.2 pypi_0 pypi
mistune 0.8.4 py37h8055547_1001 conda-forge
mkl 2019.4 245 defaults
mkl-service 2.3.0 py37hfa6e2cd_0 conda-forge
mkl_fft 1.1.0 py37hc8d92b1_1 conda-forge
mkl_random 1.1.0 py37he350917_0 conda-forge
mlxtend 0.17.2 pypi_0 pypi
mock 4.0.2 py37hc8dfbb8_0 conda-forge
mockextras 1.0.2 pypi_0 pypi
more-itertools 8.4.0 py_0 conda-forge
mpmath 1.1.0 py_0 conda-forge
msgpack-python 1.0.0 py37heaa310e_1 conda-forge
msys2-conda-epoch 20160418 1 defaults
multipledispatch 0.6.0 py_0 conda-forge
multiprocess 0.70.9 pypi_0 pypi
munch 2.5.0 py_0 conda-forge
navigator-updater 0.2.1 py37_0 defaults
nbconvert 5.6.1 py37hc8dfbb8_1 conda-forge
nbformat 5.0.7 py_0 conda-forge
nengo 2.8.0 pypi_0 pypi
nengo-gui 0.4.6 pypi_0 pypi
nengolib 0.5.2 pypi_0 pypi
networkx 2.4 py_1 conda-forge
neuraxle 0.3.4 pypi_0 pypi
ninja 1.10.0 h1ad3211_0 conda-forge
nltk 3.4.4 py_0 conda-forge
nose 1.3.7 py37hc8dfbb8_1004 conda-forge
notebook 6.1.1 py37hc8dfbb8_0 conda-forge
numba 0.50.1 py37h3bbf574_1 conda-forge
numexpr 2.7.1 py37h1834ac0_1 conda-forge
numpy 1.18.1 pypi_0 pypi
numpy-base 1.18.5 py37hc3f5095_0 defaults
numpydoc 1.1.0 pyh9f0ad1d_0 conda-forge
oauthlib 3.1.0 pypi_0 pypi
olefile 0.46 py_0 conda-forge
onnx 1.7.0 pypi_0 pypi
onnxconverter-common 1.7.0 pypi_0 pypi
openjpeg 2.3.1 h57dd2e7_3 conda-forge
openpyxl 3.0.4 py_0 conda-forge
openssl 1.1.1g he774522_0 anaconda
opt-einsum 3.1.0 pypi_0 pypi
opt_einsum 3.3.0 py_0 conda-forge
osqp 0.6.1 pypi_0 pypi
packaging 20.4 pyh9f0ad1d_0 conda-forge
pandas 1.1.0 py37h1834ac0_0 conda-forge
pandas-datareader 0.8.1 pypi_0 pypi
pandoc 2.10.1 he774522_0 conda-forge
pandocfilters 1.4.2 py_1 conda-forge
paramiko 2.7.1 pyh9f0ad1d_1 conda-forge
params-flow 0.7.4 pypi_0 pypi
parquet-cpp 1.5.1 2 conda-forge
parso 0.5.2 py_0 defaults
partd 1.1.0 py_0 conda-forge
pastedeploy 2.1.0 pyh9f0ad1d_0 conda-forge
path 15.0.0 py37hc8dfbb8_0 conda-forge
path.py 12.5.0 0 conda-forge
pathlib2 2.3.5 py37hc8dfbb8_1 conda-forge
pathtools 0.1.2 py_1 conda-forge
patsy 0.5.1 py_0 conda-forge
pcre 8.44 h6538335_0 conda-forge
pep8 1.7.1 py_0 conda-forge
pexpect 4.8.0 py37hc8dfbb8_1 conda-forge
phased-lstm-keras 1.0.2 pypi_0 pypi
pickleshare 0.7.5 py37hc8dfbb8_1001 conda-forge
pillow 7.2.0 py37hc826c6e_1 conda-forge
pip 20.0.2 pypi_0 pypi
pkginfo 1.5.0.1 py_0 conda-forge
plaster 1.0 py_0 conda-forge
plaster_pastedeploy 0.7 py_0 conda-forge
plotly 4.3.0 pypi_0 pypi
pluggy 0.13.1 py37hc8dfbb8_2 conda-forge
ply 3.11 py_1 conda-forge
pmdarima 0.0.0 pypi_0 pypi
poppler 0.67.0 h1707e21_8 conda-forge/label/cf202003
poppler-data 0.4.9 1 conda-forge
postgresql 12.3 he14cc48_0 conda-forge
powershell_shortcut 0.0.1 3 defaults
ppscore 0.0.2 pypi_0 pypi
proj 7.0.0 haa36216_5 conda-forge
prometheus_client 0.8.0 pyh9f0ad1d_0 conda-forge
prompt-toolkit 3.0.6 py_0 conda-forge
prompt_toolkit 3.0.6 0 conda-forge
protobuf 3.10.0 pypi_0 pypi
psutil 5.7.2 py37h4ab8f01_0 conda-forge
py 1.9.0 pyh9f0ad1d_0 conda-forge
py-lief 0.10.1 py37ha925a31_0 defaults
py-params 0.9.4 pypi_0 pypi
py4j 0.10.9 pyh9f0ad1d_0 conda-forge
pyarrow 1.0.0 py37h1234567_1_cpu conda-forge
pyasn1 0.4.8 py_0 conda-forge
pyasn1-modules 0.2.7 pypi_0 pypi
pycodestyle 2.6.0 pyh9f0ad1d_0 conda-forge
pycosat 0.6.3 py37h8055547_1004 conda-forge
pycparser 2.20 pyh9f0ad1d_2 conda-forge
pycrypto 2.6.1 py37h8055547_1004 conda-forge
pycurl 7.43.0.5 py37h24bd3af_2 conda-forge
pydocstyle 5.0.2 py_0 conda-forge
pydot 1.4.1 pypi_0 pypi
pyflakes 2.2.0 pyh9f0ad1d_0 conda-forge
pygments 2.6.1 py_0 conda-forge
pygpu 0.7.6 py37h44b1f71_1001 conda-forge
pyjwt 1.7.1 py_0 conda-forge
pylint 2.5.3 py37hc8dfbb8_0 conda-forge
pylops 1.9.0 pypi_0 pypi
pymc3 3.8 pypi_0 pypi
pymeeus 0.3.7 pyh9f0ad1d_0 conda-forge
pymongo 3.9.0 pypi_0 pypi
pymrio 0.4.1 pypi_0 pypi
pynacl 1.3.0 py37h2fa13f4_1001 conda-forge
pyodbc 4.0.30 py37h6538335_0 conda-forge
pyopenssl 19.1.0 py_1 conda-forge
pyparsing 2.4.7 pyh9f0ad1d_0 conda-forge
pyproj 2.6.1.post1 py37h1d8b288_0 conda-forge
pyqt 5.12.3 py37h6538335_1 conda-forge
pyqt5-sip 4.19.18 pypi_0 pypi
pyqtwebengine 5.12.1 pypi_0 pypi
pyramid 1.10.1 py_0 conda-forge
pyreadline 2.1 py37_1001 conda-forge
pyrsistent 0.16.0 py37h8055547_0 conda-forge
pysocks 1.7.1 py37hc8dfbb8_1 conda-forge
pyspark 3.0.0 py_0 conda-forge
pystan 2.19.1.1 pypi_0 pypi
pytables 3.6.1 py37h2d87964_2 conda-forge
pytesseract 0.3.2 pypi_0 pypi
pytest 6.0.1 py37hc8dfbb8_0 conda-forge
pytest-arraydiff 0.3 py_0 conda-forge
pytest-astropy 0.7.0 py_0 conda-forge
pytest-astropy-header 0.1.2 py_0 conda-forge
pytest-doctestplus 0.8.0 py_0 conda-forge
pytest-openfiles 0.5.0 py_0 conda-forge
pytest-remotedata 0.3.1 py_0 conda-forge
python 3.7.8 h60c2a47_1_cpython conda-forge
python-dateutil 2.8.1 py_0 conda-forge
python-graphviz 0.13.2 pypi_0 pypi
python-jsonrpc-server 0.3.4 pyh9f0ad1d_1 conda-forge
python-language-server 0.31.9 py37hc8dfbb8_0 conda-forge
python-libarchive-c 2.9 py37_0 conda-forge
python_abi 3.7 1_cp37m conda-forge
pytorch 1.4.0 py3.7_cuda101_cudnn7_0 pytorch
pytrends 4.7.3 pypi_0 pypi
pytz 2020.1 pyh9f0ad1d_0 conda-forge
pywavelets 1.1.1 py37h44b1f71_1 conda-forge
pywin32 227 py37hfa6e2cd_0 conda-forge
pywin32-ctypes 0.2.0 py37hc8dfbb8_1001 conda-forge
pywinpty 0.5.7 py37_0 conda-forge
pyyaml 5.3.1 py37h8055547_0 conda-forge
pyzmq 19.0.2 py37h453f00a_0 conda-forge
qdarkstyle 2.8.1 pyh9f0ad1d_0 conda-forge
qgrid 1.1.1 pypi_0 pypi
qt 5.12.5 h7ef1ec2_0 conda-forge
qtawesome 0.7.2 pyh9f0ad1d_0 conda-forge
qtconsole 4.7.5 pyh9f0ad1d_0 conda-forge
qtpy 1.9.0 py_0 conda-forge
quandl 3.5.0 pypi_0 pypi
quantecon 0.4.6 pypi_0 pypi
rasterio 1.1.5 py37h02db82b_1 conda-forge
re2 2020.08.01 ha925a31_0 conda-forge
requests 2.24.0 pyh9f0ad1d_0 conda-forge
requests-oauthlib 1.3.0 pyh9f0ad1d_0 conda-forge
retrying 1.3.3 pypi_0 pypi
ripgrep 12.1.1 h301d43c_0 conda-forge
rope 0.17.0 pyh9f0ad1d_0 conda-forge
rpy2 2.9.5 pypi_0 pypi
rsa 4.6 pyh9f0ad1d_0 conda-forge
rtree 0.9.4 py37h804a536_1 conda-forge
ruamel_yaml 0.15.80 py37h8055547_1001 conda-forge
s3transfer 0.2.1 pypi_0 pypi
scikit-image 0.17.2 py37h3bbf574_1 conda-forge
scikit-learn 0.23.2 py37hdc70db3_0 conda-forge
scipy 1.3.1 pypi_0 pypi
scs 2.1.1-2 pypi_0 pypi
seaborn 0.10.1 1 conda-forge
seaborn-base 0.10.1 py_1 conda-forge
selenium 3.141.0 pypi_0 pypi
send2trash 1.5.0 py_0 conda-forge
setuptools 49.3.1 py37hc8dfbb8_0 conda-forge
setuptools-git 1.2 pypi_0 pypi
shapely 1.7.0 py37he1cf020_3 conda-forge
simplegeneric 0.8.1 py_1 conda-forge
singledispatch 3.4.0.3 py37_1000 conda-forge
sip 4.19.20 py37h6538335_0 conda-forge
six 1.15.0 pyh9f0ad1d_0 conda-forge
sklearn 0.0 pypi_0 pypi
sklearn-contrib-py-earth 0.1.0+1.gdde5f89 dev_0
snappy 1.1.8 ha925a31_3 conda-forge
snowballstemmer 2.0.0 py_0 conda-forge
snuggs 1.4.7 py_0 conda-forge
sortedcollections 1.2.1 pyh9f0ad1d_0 conda-forge
sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge
soupsieve 2.0.1 py37hc8dfbb8_0 conda-forge
sparse 0.8.0 pypi_0 pypi
sphinx 3.2.0 py_0 conda-forge
sphinxcontrib 1.0 py37_1 defaults
sphinxcontrib-applehelp 1.0.2 py_0 conda-forge
sphinxcontrib-devhelp 1.0.2 py_0 conda-forge
sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge
sphinxcontrib-jsmath 1.0.1 py_0 conda-forge
sphinxcontrib-qthelp 1.0.3 py_0 conda-forge
sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge
sphinxcontrib-websupport 1.2.4 pyh9f0ad1d_0 conda-forge
spyder 4.1.2 py37hc8dfbb8_0 conda-forge
spyder-kernels 1.9.3 py37hc8dfbb8_0 conda-forge
sqlalchemy 1.3.18 py37h4ab8f01_0 conda-forge
sqlite 3.32.3 he774522_1 conda-forge
statsmodels 0.11.0 pypi_0 pypi
stocker 0.1.5 pypi_0 pypi
sympy 1.6.2 py37hc8dfbb8_0 conda-forge
tabulate 0.8.7 pypi_0 pypi
tb-nightly 1.14.0a20190603 pypi_0 pypi
tbats 1.0.9 pypi_0 pypi
tbb 2020.1 he980bc4_0 conda-forge
tblib 1.6.0 py_0 conda-forge
tensorboard 2.0.2 pypi_0 pypi
tensorboard-plugin-wit 1.7.0 pypi_0 pypi
tensorflow 2.3.0 pypi_0 pypi
tensorflow-addons 0.9.1 pypi_0 pypi
tensorflow-estimator 2.0.1 pypi_0 pypi
tensorflow-gpu 2.0.0 pypi_0 pypi
tensorflow-gpu-estimator 2.1.0 pypi_0 pypi
termcolor 1.1.0 pypi_0 pypi
terminado 0.8.3 py37hc8dfbb8_1 conda-forge
terminaltables 3.1.0 pypi_0 pypi
testpath 0.4.4 py_0 conda-forge
tf-estimator-nightly 1.14.0.dev2019060501 pypi_0 pypi
theano 1.0.4 pypi_0 pypi
themis 0.1.0 pypi_0 pypi
threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge
thrift-cpp 0.13.0 h1907cbf_2 conda-forge
tifffile 2020.7.24 py_0 conda-forge
tigramite 4.1.0 pypi_0 pypi
tiledb 1.7.7 h0b90766_3 conda-forge
tk 8.6.10 hfa6e2cd_0 conda-forge
toml 0.10.1 pyh9f0ad1d_0 conda-forge
toolz 0.10.0 py_0 conda-forge
tornado 6.0.4 py37hfa6e2cd_0 conda-forge
tqdm 4.48.2 pyh9f0ad1d_0 conda-forge
traitlets 4.3.3 py37hc8dfbb8_1 conda-forge
traittypes 0.2.1 py_1 conda-forge
translationstring 1.3 py_0 conda-forge
typed-ast 1.4.1 py37hfa6e2cd_0 conda-forge
typeguard 2.7.1 pypi_0 pypi
typing_extensions 3.7.4.2 py_0 conda-forge
tzlocal 2.0.0 pypi_0 pypi
ujson 1.35 py37h63f7a3c_1002 conda-forge
unicodecsv 0.14.1 py_1 conda-forge
urllib3 1.25.10 py_0 conda-forge
vc 14.1 h869be7e_1 conda-forge
venusian 3.0.0 py_0 conda-forge
vs2015_runtime 14.16.27012 h30e32a0_2 conda-forge
vs2017_win-64 19.16.27038 h2e3bad8_2 conda-forge
vswhere 2.7.1 h21ff451_0 defaults
watchdog 0.10.3 py37hc8dfbb8_1 conda-forge
wcwidth 0.2.5 pyh9f0ad1d_1 conda-forge
weather 1.4.0 py_0 itk
webencodings 0.5.1 py_1 conda-forge
webob 1.8.6 py_0 conda-forge
websocket-client 0.56.0 pypi_0 pypi
werkzeug 0.16.1 py_0 conda-forge
wheel 0.34.2 py_1 conda-forge
widgetsnbextension 3.5.1 py37hc8dfbb8_1 conda-forge
win_inet_pton 1.1.0 py37_0 conda-forge
win_unicode_console 0.5 py37_1000 conda-forge
wincertstore 0.2 py37_1003 conda-forge
winpty 0.4.3 4 conda-forge
wrapt 1.11.2 py37h8055547_0 conda-forge
xarray 0.14.1 pypi_0 pypi
xerces-c 3.2.2 h6538335_1004 conda-forge
xgboost 0.90 pypi_0 pypi
xlrd 1.2.0 pyh9f0ad1d_1 conda-forge
xlsxwriter 1.3.2 pyh9f0ad1d_0 conda-forge
xlwings 0.20.1 py37hc8dfbb8_0 conda-forge
xlwt 1.3.0 py_1 conda-forge
xmltodict 0.12.0 py_0 conda-forge
xz 5.2.5 h62dcd97_1 conda-forge
yaml 0.2.5 he774522_0 conda-forge
yapf 0.30.0 pyh9f0ad1d_0 conda-forge
zeromq 4.3.2 ha925a31_3 conda-forge
zfp 0.5.5 ha925a31_1 conda-forge
zict 2.0.0 py_0 conda-forge
zipp 3.1.0 py_0 conda-forge
zlib 1.2.11 h62dcd97_1007 conda-forge
zope-interface 4.7.1 pypi_0 pypi
zope.deprecation 4.4.0 py_0 conda-forge
zope.event 4.4 pyh9f0ad1d_0 conda-forge
zope.interface 5.1.0 py37h8055547_0 conda-forge
zstd 1.4.5 h1f3a1b7_2 conda-forge@Libardo1Sorry, but we don't provide support for issues with the conda environment.
This issue is more suitable on Continuum Anaconda repo since its related to TF installation with Anaconda.
Please post the issue on Continuum Anaconda..Thanks!OK, TIAAre you satisfied with the resolution of your issue?
Yes
No",stat:awaiting response subtype:windows type:build/install
https://github.com/tensorflow/tensorflow/issues/35155,"System informationDescribe the current behavior
The training is down due to low layer system call (corrupted size vs. prev_size). the process is immediately corrupted without much log to trace.Describe the expected behavior
the estimator (BoostedTreesRegressor) should be trained in normal.Code to reproduce the issueOther info / logs
No problem under environment OSX 10.14.5 with pyenv virtualenv 3.7.0 (default, Nov 22 2019, 12:39:30) \n[Clang 11.0.0 (clang-1100.0.33.12)]No problem when use google colab runtime (Ubuntu 18.04.3 LTS Python 3.6.9 compiled by GCC 8.3.0). See notebook sharedSame problem in Kaggle runtime (Debian GNU/Linux 9 Python 3.6.6 Anaconda GCC 7.3.0) using tensorflow 2.0.0.Full error log in docker using official image tensorflow/tensorflow:2.0.0-py3:@Hujun Can you please test with latest version TensorFlow 2.2?This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.Closing as stale. Please reopen if you'd like to work on this further.",TF 2.0 comp:apis stalled stat:awaiting response stat:contributions welcome type:performance
https://github.com/scikit-learn/scikit-learn/issues/4682,"i am comparing scikit mcd to matlab/libra (https://wis.kuleuven.be/stat/robust/LIBRA) revision from 01/08/2007, and getting different results:matlab results:
6.9839 0.1287
0.1287 0.9619python results:
[[ 6.6928877 0.12337967]
[ 0.12337967 0.92182858]]matlab code:
X = [[1.53356657, -1.49146826],
[-3.53464831, 0.77884215],
[3.179271, -0.54235304],
[1.82244695, 0.1602308],
[-0.80207316, -0.46638638],
[-0.98169198, -0.72384052],
[4.84574713, 1.15995118],
[2.12067491, 1.30272032],
[3.63592934, 0.53520174],
[1.36629677, 0.66372903],
[0.11739395, -0.04946755],
[-1.79636059, -0.83721967],
[0.53642667, 0.13695351],
[-3.06039201, 0.39190745],
[-3.68697525, -1.29121661],
[4.12986671, -1.64405471],
[3.97052261, -0.65406501],
[-1.2143484, 0.88733004],
[-0.80609268, 0.40985212],
[-1.95484672, -0.31931632],
[-0.33245641, 1.75440015],
[-4.29094285, 0.09554176],
[1.77083199, 2.21533913],
[-1.37560369, 0.15368846],
[3.77095853, 2.36153146]];
mcd = mcdcov(X);
mcd.covpython code:
import numpy as np
from sklearn.covariance import MinCovDet
X = np.asarray([[1.53356657, -1.49146826],
[-3.53464831, 0.77884215],
[3.179271, -0.54235304],
[1.82244695, 0.1602308],
[-0.80207316, -0.46638638],
[-0.98169198, -0.72384052],
[4.84574713, 1.15995118],
[2.12067491, 1.30272032],
[3.63592934, 0.53520174],
[1.36629677, 0.66372903],
[0.11739395, -0.04946755],
[-1.79636059, -0.83721967],
[0.53642667, 0.13695351],
[-3.06039201, 0.39190745],
[-3.68697525, -1.29121661],
[4.12986671, -1.64405471],
[3.97052261, -0.65406501],
[-1.2143484, 0.88733004],
[-0.80609268, 0.40985212],
[-1.95484672, -0.31931632],
[-0.33245641, 1.75440015],
[-4.29094285, 0.09554176],
[1.77083199, 2.21533913],
[-1.37560369, 0.15368846],
[3.77095853, 2.36153146]])
mcd = MinCovDet(support_fraction=0.75).fit(X)
print (mcd.covariance_)i guess most of the difference can be explained by the different dof normalization. which is +1 in python relative to matlabNot sure if we should use matlab as reference. Do you have a good intuition about what the dof should be?
Also would be interesting to see what R does.the default behavior in matlab and r is to give the sample stdev/var/cov with dof n-1
the default behavior in numpy is to give the population stdev/var/cov with dof n
the cov functions in scikit are always with n and do not have an n-1 option
the fast_mcd paper (http://www.geo.upm.es/postgrado/CarlosLopez/papers/FastAlgMCD99.pdf) seems to use n in the determinant evaluation and n-1 in the reweighting.This sounds like we might want to change the default behavior and possibly add an option.
But I'm not an expert, so it maybe someone else might want to weight in?
If we change behavior, we probably need to do a deprecation cycle, which is always annoying.i think the issue may be more general. for example also in pca when matlab does centering it uses n-1 for the variances, while in scikit it is nhm... I would expect n there....-1 on changing: compatibility for the sake of compatibility isn't something that we should seek (for instance,  we use a different parametrization of elastic net than glmnet).Beside,  if I am not mistaken, this is not an official Matlab function, but just something downloaded from a Web page.Sent from my phone. Please forgive brevity and mis spellingOn May 8, 2015, 21:57, at 21:57, Andreas Mueller notifications@github.com wrote:So numpy allows an option. Maybe scikit can also have one?I'd rather not. Too many options make a toolkit unusable. Think Apple vs
Linux --I am a Linux user :).This is really a debate between experts that has no practical impact (or
very little).for variance it has no impact, but for the dof it might, right?@GaelVaroquaux you mentioned:I don't believe the LIBRA package, which is written by the authors (the research group actually) that wrote the original MCD and Fast MCD paper, counts as just some random package downloaded from a web page. Of course, the debate is normalizing with a count of 'n' vs. a count of 'n-1'. If you can determine what that subset of data points used to calculate the MCD, you can just re-scale the whole thing by doing mcd.covariance_ * m / (m - 1), where m is the number of points in the subset. If you know to look out for it then there isn't much to worry about. Perhaps the MinCovDet class should store the final subset of data used to find the MCD, and allow the user to query that data. Then if they want to re-scale, they can do so themselves by checking len(mcd.data_subset_) or something.As it currently stands this is not really even possible to do since the user has no means to introspect (as far as I can tell) the final subset of data points that can be used to determine the MCD.NOTE: For the record, in the opening example, 24 points are used as the subset, as removing one point from X minimizes the covariance determinant. I don't know which point specifically, but nonetheless:To add to my last point, it does seem as though numpy.cov scales by n - 1. The reason I say this is because of the following snippet. Note that you'll probably need to apply my patch in PR #8328 for this to even work (because of singular covariance):Which should output the following:In the above case the covariance matrix was singular, so it already had the minimum covariance determinant (thus all the points construct the subset that provides the MCD). As you can see, we appropriately match the scaling of numpy.cov by scaling by n - 1 instead of by n. I would like to believe this suggests that we should go forward with n - 1 scaling, since it will not only keep us consistent with MATLAB, but likewise with numpy's covariance measure itself.removing from the milestone due to inactivityI'm pretty sure I fixed this in #8328, which was in 2017.Probably good to close, not just remove from the milestone :)Thanks @ThatGeoGuy",Bug
https://github.com/dask/dask-ml/pull/432,"What does this PR implement?
This PR makes the following changes:The biggest change is clearly explaining the issues in model selection. It reorganizes into this:With this documentation, changing the default decay_rate in IncrementalSearchCV makes sense (see #432 (comment)). If that's the default, it's the only class that fills the ""memory constrained by not compute constrained"" class of problems (almost; some of dask-searchcv spills in here too).Other issues/PRs
This PR closes #388.The documentation refactor has introduced the ability to add new classes, now that it's clear what the limitations of each approach are.This fits well with the ""computationally constrained but not memory constrained"" class of model selection problems. I'm leaving it as a separate branch right now because I think #221 is too bloated it needs a lot more work.My criteria for changing the default: where would IncrementalSearchCV provide the most value?I think it provides the most value in ""memory but not compute constrained"" problems. This means that decay_rate should default to 0 because this isn't compute constrained.The reason I think it fits best here is because IncrementalSearchCV works better for large datasets. It can go over the entire dataset because it provides access to partial_fit. dask-searchcv can't do this because it only has access to fit, so it fits to each chunk of the data independently (_search.py#L416-L422). I should add to dask-searchcv's docs to reflect this.Without changing the default, there isn't a class that fits into ""memory but not compute constrained"". I don't think DaskGridSearchCV(Incremental(model), params) would work, especially if more than one pass through data wanted.This idea matches the documentation:TODO:I don't think that's correct. The algorithms in _search.py mirror scikit-learn. I believe that the Xs and ys you linked to are replicas of the training dataset (e.g.Thanks for the review @TomAugspurger! I found it useful, especially the wording choices.Right. I've written the docs to mention that each CV split is gathered to one machine for most estimators. Can you review this?I know dask_ml.Incremental aliases fit to call partial_fit on each chunk of the Dask array. Would that work with dask-searchcv's searches? I believe dask-searchcv computes each CV split regardless of what fit can take, but I'm having a hard time seeing that.This PR now depends on #221 (but implicitly, which is why the Travis CI tests are failing).I'm drafting a paper for the SciPy 2019 proceedings. In this, I explicitly mention that decay_rate=0 by default. I use this to show that IncrementalSearchCV is best suited for memory-constrained problems.To the extent possible, I'd recommend keeping documentation changes, additional tests, and behavior changes separate. That makes things much easier for me to review.I've tried to do that with two PRs:Will that be sufficient, or should I remove the IncrementalSearchCV deprecation as well? That'll mean this PR is only modifies *.rst files/docstrings and #651 only modifies *.py files.I'm still not 100% sure on the reasons for deprecaing decay_rate and the inclusion of InverseDecaySearchCV. Can you try re-explaining it to me?Here are some arguments for deprecating decay_rate:I don't know how decay_rate performs if decay_rate not in [0, 1]. I don't even really know how decay_rate=1 performs, but at least know the motivation and have performance results for a related implementation, SuccessiveHalvingSearchCV.(1) is the motivation for included the deprecation in this PR.I've changed some tests for various tangential reasons (faster, silencing warnings, etc). These comments address these changes, or ask for clarifying comments.FYI, going to ignore the sklearn dev failures for now. We'll need to add n_features_in_ to each estimator, which will take a bit of work.It looks like all the CI failures are unrelated. There's a couple other warnings besides the one with n_features_in_:n_features_in_:PartialPassiveAggressiveClassifier.average_intercept_:Passing keyword arguments:There are lots of warnings about keyword arguments and n_features_in_.I fixed the PartialPassiveAggressiveClassifier.average_intercept_ in another branch.I think this is ready to go. All good on your end?Going to merge so we can include this in the 1.4.0 release. I can make another release anytime if needed.Yup! I'm glad to see this merged.",None yet
https://github.com/scikit-learn-contrib/metric-learn/pull/270,"The problem is due to a deprecation warning for check_is_fitted in the new version of sklearn (see below)This PR fixes this by not using the attributes argument when calling check_is_fitted.The warning raised in the failing tests is from sklearn (see scikit-learn/scikit-learn#14545):
Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.check_is_fitted now only checks the presence of any fitted attribute (ending with a trailing underscore)It should be fine for us to simply remove the passed attributes. Will doRemoving the passed attributes does make the Python > 3.4 tests pass but makes the other fail, as sklearn dropped support for Python < 3.5.One way to fix this would be to keep the attributes and relax the tests to let this particular warning go through, but this will make the code break when sklearn releases its next version.So I would favor a solution based on checking Python version with sys.version_info and call check_is_fitted accordingly. What do you think? @perimosocordiae @terrytangyuan @wdevazelhesBTW this is an example of the problems we are likely to see more and more as we continue to support Python < 3.5 while sklearn does not. I think we should also drop support for Python < 3.5 in the next release, see also discussion in #166I implemented the strategy proposed above and all tests are now passing, so I think it is ready to merge.Great!",None yet
https://github.com/daniel-rychlewski/hsi-toolbox/issues/1,"Hi Daniel,Thanks for sharing your toolbox. The following error is produced when doing a simple run./hsi-toolbox-master/DeepHyperXvis.py': [Errno 2] No such file or directoryI've also edited out all the references to DeepHyperX as there is no associated py file such as: DeepHyperX.batch, DeepHyperX.datasets, DeepHyperX.image_compression.Are there some files missing?Cheers,BbopHello Bbop,Thanks for your kind feedback. I have tried to reproduce the error by downloading the project and running DeepHyperX/main.py and was not able to do it, because even if I delete vis.py, the main program does not complain about the file being missing. The three files batch.py, datasets.py and image_compression.py are present in the DeepHyperX directory of the repository (https://github.com/daniel-rychlewski/hsi-toolbox/tree/master/DeepHyperX), so may I suggest that you check if they are present on your machine and share any relevant details on how to reproduce the issue (file name of the program you ran, arguments used, OS, Python environment - version and packages installed)?Kind regards,
Daniel RychlewskiThanks Daniel,I moved the vis.py to the parent directory and renamed it DeepHyperXvis.py and that worked.I am working on band selection and NN for a custom data set and not using distiller at all.I'm not really good at python and will list out issues I've run into and maybe I can help others.I will get more detail as I work through it. Thanks for responding =)There were some requirement issues such as protobuf and winml and I installed the some packages manually. I will repeat the environment setup to get details. One I remember clearly is manually installing pytorch.I also edited the batch.py to contain my system details.Python 3.6.10 :: Anaconda, Inc.populated config files : C:\Users\bbop1.condarc
conda version : 4.8.3
conda-build version : 3.18.11
python version : 3.7.6.final.0
virtual packages : __cuda=11.0
base environment : C:\Users\bbop1\anaconda3 (writable)
channel URLs : https://repo.anaconda.com/pkgs/main/win-64
https://repo.anaconda.com/pkgs/main/noarch
https://repo.anaconda.com/pkgs/r/win-64
https://repo.anaconda.com/pkgs/r/noarch
https://repo.anaconda.com/pkgs/msys2/win-64
https://repo.anaconda.com/pkgs/msys2/noarch
package cache : C:\Users\bbop1\anaconda3\pkgs
C:\Users\bbop1.conda\pkgs
C:\Users\bbop1\AppData\Local\conda\conda\pkgs
envs directories : C:\Users\bbop1\anaconda3\envs
C:\Users\bbop1.conda\envs
C:\Users\bbop1\AppData\Local\conda\conda\envs
platform : win-64
user-agent : conda/4.8.3 requests/2.22.0 CPython/3.7.6 Windows/10 Windows/10.0.18362
administrator : False
netrc file : None
offline mode : Falseabsl-py 0.9.0 pypi_0 pypi
astropy 4.0.1.post1 py36h68a101e_0 conda-forge
astunparse 1.6.3 pypi_0 pypi
atomicwrites 1.4.0 pypi_0 pypi
attrs 19.3.0 py_0 conda-forge
backcall 0.1.0 py_0 conda-forge
blas 1.0 mkl
bleach 3.1.5 pyh9f0ad1d_0 conda-forge
bqplot 0.12.12 pypi_0 pypi
brotlipy 0.7.0 py36h779f372_1000 conda-forge
ca-certificates 2020.4.5.1 hecc5488_0 conda-forge
cachetools 4.1.0 pypi_0 pypi
certifi 2020.4.5.1 py36h9f0ad1d_0 conda-forge
cffi 1.14.0 py36ha419a9e_0 conda-forge
chardet 3.0.4 py36h9f0ad1d_1006 conda-forge
colorama 0.4.3 py_0 conda-forge
confuse 1.1.0 pyh9f0ad1d_0 conda-forge
cryptography 2.9.2 py36hef61171_0 conda-forge
cudatoolkit 10.2.89 h74a9793_1
cycler 0.10.0 py_2 conda-forge
decorator 4.4.2 py_0 conda-forge
defusedxml 0.6.0 py_0 conda-forge
deprecated 1.2.10 pypi_0 pypi
entrypoints 0.3 py36h9f0ad1d_1001 conda-forge
fire 0.3.1 pypi_0 pypi
freetype 2.9.1 ha9979f8_1
future 0.18.2 pypi_0 pypi
gast 0.3.3 pypi_0 pypi
gitdb 4.0.5 pypi_0 pypi
gitdb2 4.0.2 pypi_0 pypi
gitpython 2.1.11 pypi_0 pypi
google-auth 1.16.0 pypi_0 pypi
google-auth-oauthlib 0.4.1 pypi_0 pypi
google-pasta 0.2.0 pypi_0 pypi
grpcio 1.29.0 pypi_0 pypi
gym 0.12.5 pypi_0 pypi
h5py 2.10.0 pypi_0 pypi
htmlmin 0.1.12 py_1 conda-forge
icc_rt 2019.0.0 h0cc432a_1
idna 2.9 py_1 conda-forge
imagehash 4.1.0 pyh9f0ad1d_0 conda-forge
imageio 2.8.0 pypi_0 pypi
importlib-metadata 1.6.0 py36h9f0ad1d_0 conda-forge
importlib_metadata 1.6.0 0 conda-forge
intel-openmp 2020.1 216
ipykernel 5.3.0 py36h5ca1d4c_0 conda-forge
ipython 7.14.0 pypi_0 pypi
ipython-genutils 0.2.0 pypi_0 pypi
ipython_genutils 0.2.0 py_1 conda-forge
ipywidgets 7.5.1 py_0 conda-forge
jedi 0.17.0 py36h9f0ad1d_0 conda-forge
jinja2 2.11.2 pyh9f0ad1d_0 conda-forge
joblib 0.15.1 py_0
jpeg 9b hb83a4c4_2
jsonpatch 1.25 pypi_0 pypi
jsonpointer 2.0 pypi_0 pypi
jsonschema 3.2.0 py36h9f0ad1d_1 conda-forge
jupyter 1.0.0 pypi_0 pypi
jupyter-console 6.1.0 pypi_0 pypi
jupyter_client 6.1.3 py_0 conda-forge
jupyter_core 4.6.3 py36h9f0ad1d_1 conda-forge
keras 2.3.1 pypi_0 pypi
keras-applications 1.0.8 pypi_0 pypi
keras-preprocessing 1.1.2 pypi_0 pypi
keras2onnx 1.6.1 pypi_0 pypi
kiwisolver 1.2.0 py36h246c5b5_0 conda-forge
libpng 1.6.37 h2a8f88b_0
libsodium 1.0.17 h2fa13f4_0 conda-forge
libtiff 4.1.0 h56a325e_0
lightgbm 2.3.1 pypi_0 pypi
llvmlite 0.32.1 py36ha925a31_0
m2w64-gcc-libgfortran 5.3.0 6
m2w64-gcc-libs 5.3.0 7
m2w64-gcc-libs-core 5.3.0 7
m2w64-gmp 6.1.0 2
m2w64-libwinpthread-git 5.0.0.4634.697f757 2
markdown 3.2.2 pypi_0 pypi
markupsafe 1.1.1 py36h68a101e_1 conda-forge
matplotlib-base 3.2.1 py36hf0cddfc_0 conda-forge
missingno 0.4.2 py_1 conda-forge
mistune 0.8.4 py36h68a101e_1001 conda-forge
mkl 2020.1 216
mkl-service 2.3.0 py36hb782905_0
mkl_fft 1.0.15 py36h14836fe_0
mkl_random 1.1.1 py36h47e9c7a_0
mlxtend 0.17.2 pypi_0 pypi
more-itertools 8.3.0 pypi_0 pypi
msys2-conda-epoch 20160418 1
munch 2.5.0 pypi_0 pypi
nbconvert 5.6.1 py36h9f0ad1d_1 conda-forge
nbformat 5.0.6 py_0 conda-forge
networkx 2.4 py_1 conda-forge
ninja 1.9.0 py36h74a9793_0
notebook 6.0.3 py36h9f0ad1d_0 conda-forge
numba 0.49.1 py36h47e9c7a_0
numpy 1.18.1 py36h93ca92e_0
numpy-base 1.18.1 py36hc3f5095_1
oauthlib 3.1.0 pypi_0 pypi
olefile 0.46 py36_0
onnx 1.5.0 pypi_0 pypi
onnx-tf 1.5.0 pypi_0 pypi
onnxconverter-common 1.6.1 pypi_0 pypi
onnxmltools 1.5.1 pypi_0 pypi
onnxruntime 1.3.0 pypi_0 pypi
openssl 1.1.1g he774522_0 conda-forge
opt-einsum 3.2.1 pypi_0 pypi
packaging 20.4 pyh9f0ad1d_0 conda-forge
pandas 1.0.3 py36h47e9c7a_0
pandas-profiling 2.8.0 py_0 conda-forge
pandoc 2.9.2.1 0 conda-forge
pandocfilters 1.4.2 pypi_0 pypi
parso 0.7.0 pyh9f0ad1d_0 conda-forge
patsy 0.5.1 py_0 conda-forge
phik 0.10.0 py_0 conda-forge
pickleshare 0.7.5 py36h9f0ad1d_1001 conda-forge
pillow 7.1.2 py36hcc1f983_0
pip 20.0.2 py36_3
pluggy 0.13.1 pypi_0 pypi
pretrainedmodels 0.7.4 pypi_0 pypi
prometheus_client 0.8.0 pyh9f0ad1d_0 conda-forge
prompt-toolkit 3.0.5 py_0 conda-forge
protobuf 3.8.0 pypi_0 pypi
py 1.8.1 pypi_0 pypi
pyasn1 0.4.8 pypi_0 pypi
pyasn1-modules 0.2.8 pypi_0 pypi
pycparser 2.20 py_0 conda-forge
pydot 1.4.1 pypi_0 pypi
pyglet 1.5.5 pypi_0 pypi
pygments 2.6.1 py_0 conda-forge
pyopenssl 19.1.0 py_1 conda-forge
pyparsing 2.4.7 pyh9f0ad1d_0 conda-forge
pyrsistent 0.16.0 py36h68a101e_0 conda-forge
pysocks 1.7.1 py36h9f0ad1d_1 conda-forge
pytest 5.4.2 pypi_0 pypi
python 3.6.10 h9f7ef89_2
python-dateutil 2.8.1 py_0
python-graphviz 0.14 pypi_0 pypi
python_abi 3.6 1_cp36m conda-forge
pytorch 1.5.0 py3.6_cuda102_cudnn7_0 pytorch
pytz 2020.1 py_0
pywavelets 1.1.1 py36h7725771_1 conda-forge
pywin32 227 pypi_0 pypi
pywinpty 0.5.7 py36_0 conda-forge
pyyaml 5.3.1 pypi_0 pypi
pyzmq 19.0.1 pypi_0 pypi
qgrid 1.3.1 pypi_0 pypi
qtconsole 4.7.4 pypi_0 pypi
qtpy 1.9.0 pypi_0 pypi
requests 2.23.0 pyh8c360ce_2 conda-forge
requests-oauthlib 1.3.0 pypi_0 pypi
rsa 4.0 pypi_0 pypi
scikit-image 0.17.2 pypi_0 pypi
scikit-learn 0.22.1 py36h6288b17_0
scipy 1.4.1 py36h9439919_0
seaborn 0.10.1 py_0 conda-forge
send2trash 1.5.0 py_0 conda-forge
setuptools 46.4.0 py36_0
six 1.14.0 py36_0
skl2onnx 1.6.1 pypi_0 pypi
smmap 3.0.4 pypi_0 pypi
spectral 0.21 pypi_0 pypi
sqlite 3.31.1 h2a8f88b_1
statsmodels 0.11.1 py36h68a101e_1 conda-forge
tabulate 0.8.7 pypi_0 pypi
tangled-up-in-unicode 0.0.6 pyh9f0ad1d_0 conda-forge
tbb 2020.1 he980bc4_0 conda-forge
tensorboard 2.2.2 pypi_0 pypi
tensorboard-plugin-wit 1.6.0.post3 pypi_0 pypi
tensorflow 2.2.0 pypi_0 pypi
tensorflow-estimator 2.2.0 pypi_0 pypi
termcolor 1.1.0 pypi_0 pypi
terminado 0.8.3 pypi_0 pypi
testpath 0.4.4 py_0 conda-forge
tf2onnx 1.5.3 pypi_0 pypi
tifffile 2020.5.25 pypi_0 pypi
tk 8.6.8 hfa6e2cd_0
torchfile 0.1.0 pypi_0 pypi
torchnet 0.0.4 pypi_0 pypi
torchsummary 1.5.1 pypi_0 pypi
torchvision 0.6.0 py36_cu102 pytorch
tornado 6.0.4 py36hfa6e2cd_0 conda-forge
tqdm 4.46.0 pyh9f0ad1d_0 conda-forge
traitlets 4.3.3 py36h9f0ad1d_1 conda-forge
traittypes 0.2.1 pypi_0 pypi
typing 3.7.4.1 pypi_0 pypi
typing-extensions 3.7.4.2 pypi_0 pypi
umap 0.1.1 pypi_0 pypi
urllib3 1.25.9 py_0 conda-forge
vc 14.1 h0510ff6_4
visdom 0.1.8.9 pypi_0 pypi
visions 0.4.4 pyh9f0ad1d_0 conda-forge
vs2015_runtime 14.16.27012 hf0eaf9b_2
wcwidth 0.1.9 pyh9f0ad1d_0 conda-forge
webencodings 0.5.1 pypi_0 pypi
websocket-client 0.57.0 pypi_0 pypi
werkzeug 1.0.1 pypi_0 pypi
wheel 0.34.2 py36_0
widgetsnbextension 3.5.1 py36_0 conda-forge
win_inet_pton 1.1.0 py36_0 conda-forge
wincertstore 0.2 py36h7fe50ca_0
winmltools 1.5.1 pypi_0 pypi
winpty 0.4.3 4 conda-forge
wrapt 1.12.1 pypi_0 pypi
xlsxwriter 1.2.8 pypi_0 pypi
xz 5.2.5 h62dcd97_0
yaml 0.2.4 he774522_0 conda-forge
zeromq 4.3.2 h6538335_2 conda-forge
zipp 3.1.0 py_0 conda-forge
zlib 1.2.11 h62dcd97_4
zstd 1.3.7 h508b16e_0Issue 1 - main.py lineit does not find the DeepHyperX.X files and I removed the DeepHyperX in the name and they worked. As below.from DeepHyperX.batch import STORE_EXPERIMENT_LOCATION,
PYTHON_INTERPRETER_LOCATION, VIS_PY_LOCATION
from DeepHyperX.datasets import get_dataset, HyperX, open_file, DATASETS_CONFIG
from DeepHyperX.image_compression import apply_band_selection_choice
from DeepHyperX.models import get_model, train, test, save_model
from DeepHyperX.utils import metrics, convert_to_color_, convert_from_color_,
display_dataset, display_predictions, explore_spectrums, plot_spectrums,
sample_gt, build_dataset, show_results, compute_imf_weights, get_device, start_mem_measurement, stop_mem_measurement
from DeepHyperX.utils import print_memory_metricsfrom batch import STORE_EXPERIMENT_LOCATION,
PYTHON_INTERPRETER_LOCATION, VIS_PY_LOCATION
from datasets import get_dataset, HyperX, open_file, DATASETS_CONFIG
from image_compression import apply_band_selection_choice
from models import get_model, train, test, save_model
from utils import metrics, convert_to_color_, convert_from_color_,
display_dataset, display_predictions, explore_spectrums, plot_spectrums,
sample_gt, build_dataset, show_results, compute_imf_weights, get_device, start_mem_measurement, stop_mem_measurement
from utils import print_memory_metricsIssue 2 - main.pyfrom pandas.tests.sparse.frame.test_to_from_scipy import scipypandas.tests.sparse.frame.test_to_from_scipy didn't work.I removed it and just imported scipy and it worked.Issue 3 - main.py line 633 conf_temp.get_values()pandas.get_values() is deprecated.
https://pandas.pydata.org/pandas-docs/version/0.25.3/reference/api/pandas.DataFrame.get_values.htmlreplaced with conf_temp.valuesMy test arguments have been:python main.py --download PaviaC
python main.py --model nn --dataset PaviaC --training_sample 0.1 --cuda 0Cheers,ChrisHello Chris,Thank you very much for sharing your experiences, I do hope your comment will help someone else out. For me, the command ""python main.py --model nn --dataset PaviaC --training_sample 0.1 --cuda 0 --epoch 1"" did work without issues, though. The fact that there is no path delimiter in the file 'DeepHyperXvis.py' makes me wonder if perhaps the path in batch.py is set correctly in the sense that either '/' or '\\' needs to be the line delimiter, not just a plain backslash '\' because that's the escape character and 'DeepHyperX\vis.py' probably wouldn't work (might ask for DeepHyperXvis.py), while both 'DeepHyperX/vis.py' and 'DeepHyperX\\vis.py' probably should work fine on Windows. Could you try that out please?Issue 2: For me, 'conda list' does show the line
""pandas 0.24.2 pypi_0 pypi""
but I do see that you're using a newer version of pandas than that.Issue 3: Thanks for your input, it's probably wise to replace get_values() with .values. However, I cannot promise that I will be updating the project to eliminate deprecation warnings.Kind regards,
Daniel RychlewskiHi Daniel,Thanks for explaining. I've input a new custom dataset, however it throws the following error:(anotherOne) C:\Users\bbop1\hsi-toolbox-master\DeepHyperX>python main.py --model nn --dataset selene --training_sample 0.1 --cuda 0
C:\Users\bbop1\anaconda3\envs\anotherOne\lib\site-packages\sklearn\externals\joblib_init_.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
warnings.warn(msg, category=FutureWarning)
Computation on CUDA GPU device 0
Setting up a new session...
Traceback (most recent call last):
File ""main.py"", line 224, in
FOLDER)
File ""C:\Users\bbop1\hsi-toolbox-master\DeepHyperX\datasets.py"", line 192, in get_dataset
img, gt, rgb_bands, ignored_labels, label_values, palette = CUSTOM_DATASETS_CONFIG[dataset_name]'loader'
File ""C:\Users\bbop1\hsi-toolbox-master\DeepHyperX\custom_datasets.py"", line 90, in
'loader': lambda folder: selene_loader(folder)
File ""C:\Users\bbop1\hsi-toolbox-master\DeepHyperX\custom_datasets.py"", line 460, in selene_loader
img = open_file(folder + CUSTOM_DATASETS_CONFIG['selene']['img'])['selene']
KeyError: 'selene'Knowing that PaviaC works well, I tried renaming it the same as PaviaC however it stll won't work. Do you know why this may be occuring?Thanks,Chrisdef selene_loader(folder):
img = open_file(folder + CUSTOM_DATASETS_CONFIG['selene']['img'])['selene']
gt = open_file(folder + CUSTOM_DATASETS_CONFIG['selene']['gt'])['selene_gt']
gt = gt.astype('uint8')Hello Chris,I'm glad I could help. As for the integration of a new dataset, I do think you have followed the steps correctly, i.e. to add the dataset file and expand the custom loader, so my guess is that the dataset file does actually not have the key 'selene' and that's ok because that varies by dataset, even for the existing datasets in custom_datasets.py if you look at the loaders. It means that
open_file(folder + CUSTOM_DATASETS_CONFIG['selene']['img'])
has already given you the dataset to read in. Perhaps there is no key or the key is named differently - I suggest that you set a breakpoint / add a watch for the above line to evaluate what you actually get. Then you'll see how the key is called or if there is a key at all.Kind regards,
Daniel RychlewskiSince the original problem is resolved, I am closing the issue.Thanks Daniel,How are they keys created? I've tried googling but don't know what it means.I remove the keys and get the following error:(anotherOne) C:\Users\bbop1\hsi-toolbox-master\DeepHyperX>python main.py --model nn --dataset selene --training_sample 0.1 --cuda 0
C:\Users\bbop1\anaconda3\envs\anotherOne\lib\site-packages\sklearn\externals\joblib_init_.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
warnings.warn(msg, category=FutureWarning)
Computation on CUDA GPU device 0
Setting up a new session...
Traceback (most recent call last):
File ""main.py"", line 224, in
FOLDER)
File ""C:\Users\bbop1\hsi-toolbox-master\DeepHyperX\datasets.py"", line 213, in get_dataset
nan_mask = np.isnan(img.sum(axis=-1))
AttributeError: 'dict' object has no attribute 'sum'Ground Truth was created in Matlab by manually labeling pixels with 1-4.Hello Chris,Python can store Key-Value-Pairs in dictionaries. You use the key to access the value stored for that key or store a value. Please google ""Python dictionary"" to find out about this concept and examples, which might help to clarify. If you know a different programming language like Java, the equivalent to dictionaries would be a Map.The error message you're posting says that 'img' is a dictionary, but you would need a numpy array so that its contents (pixels) could be summed up using the sum function. This makes me guess that there is a key you need to use to correctly access the actual contents of the dataset first, i.e. all pixels of the dataset in a multidimensional array. However, it is up to you to find out how you get this array which constitutes the dataset.Kind regards,
Daniel RychlewskiThanks heaps Daniel, you're a legend.The mat files held onto their original label when I renamed them for python.",None yet
https://github.com/titipata/detecting-scientific-claim/issues/26,"This is the same as: allenai/allennlp#4431 (comment)I'm trying to load a model saved to a local path in the Kaggle environment, using the allennlp.models.load_archive() method but I get the below error:
""ConfigurationError: discourse_classifier is not a registered name for Model. You probably need to use the --include-package flag to load your custom code. Alternatively, you can specify your choices using fully-qualified paths, e.g. {""model"": ""my_module.models.MyModel""} in which case they will be automatically imported correctly.""Model location: https://s3-us-west-2.amazonaws.com/pubmed-rct/model.tar.gzTraceback:OS: Kaggle kernelPython version: 3.7.6",None yet
https://github.com/beijbom/coralnet/issues/182,"(Django itself gets its own update issues.) Not going to create issues for these updates all the time. But right now we have a lot of versions to catch up on, so it's appropriate.For the packages that depend on Django, try to update Django from 1.9 to 1.11 (#49 ) at the same time as those packages. A lot of these have dropped support for Django 1.9 since it wasn't an LTS release.Django dependent:Not Django dependent:Also update the remaining celery-related packages.Once PRs #257 and #260 are taken care of, I'm ready to wrap up package updates for the time being. Here's where we'll be at after those PRs:Meanwhile, related to vision backend tests, I'll plug a reminder about the swappable_backend branch. Just in case the ideas there end up being helpful.Sounds good. Thanks for the reminder about the swappable backend. I will create a ticket for that. The backend stuff need some dev-ops TLC.@StephenChan : actually. Can you point me to specific issues you have / tests that you can not run with the current setup? I just want to make sure I capture most of them in the ticket.I'll need to take another look to refresh my memory on the details. I'll do that tomorrow and get back to you.Okay, I took a look:One thought about my swappable_backend design: I structured it such that the mock backend wouldn't even need S3/SQS at all. However, it could also be useful to test the S3/SQS functions without testing spacer. So, another possible mock-backend design would be one that writes messages / job results to AWS, but still doesn't actually use spacer.Also, I've taken a closer look at the packages where I said we'd want VB tests:More test coverage will also make life easier when upgrading to Python 3. Perhaps we can start using the coverage package to find code that needs tests. pip install coverage, then follow the instructions in that docs link. I tried it and it seems pretty handy.Oh, and I can certainly help with writing the tests themselves. There's a certain point where that's mostly busy-work. But I figured I should leave the VB code re-organization decisions up to you, as long as you have time.Thanks @StephenChan :This all makes sense. I'll create a separate issue for this and add to the project.This might get merged with #185 somehow, especially if I can manage to upgrade to Python 3.x without updating many packages. I'll see how it goes though.Since that did happen (PR #309), here's what I'm doing with the Beta 2 package issues:",system
https://github.com/dynamicslab/pysindy/pull/1,"Changed BaseOptimizer class so that it inherits from sklearn.linear_model.LinearRegression instead of sklearn.linear_model.base.LinearModel. sklearn was throwing a warning that we won't be able to import from sklearn.linear_model.base in the future, i.e.I also",None yet
https://github.com/Ohjeah/sparsereg/pull/24,"Changed BaseOptimizer class so that it inherits from sklearn.linear_model.LinearRegression instead of sklearn.linear_model.base.LinearModel. sklearn was throwing a warning that we won't be able to import from sklearn.linear_model.base in the future, i.e.I alsoSorry, this pull request was meant to be submitted elsewhere.",None yet
https://github.com/allenai/allennlp/issues/4431,"I'm trying to load a model saved to a local path in the Kaggle environment, using the allennlp.models.load_archive() method but I get the below error:
""ConfigurationError: discourse_classifier is not a registered name for Model. You probably need to use the --include-package flag to load your custom code. Alternatively, you can specify your choices using fully-qualified paths, e.g. {""model"": ""my_module.models.MyModel""} in which case they will be automatically imported correctly.""Model location: https://s3-us-west-2.amazonaws.com/pubmed-rct/model.tar.gzTraceback:OS: Kaggle kernelPython version: 3.7.6You need to be sure that your discourse_classifier model has been registered. Assuming you did that with a @Model.register decorator, the easiest way to do that is just to import your model class in your script. You'll likely also have to do the same for your dataset reader class.",bug
https://github.com/titipata/detecting-scientific-claim/issues/26,"This is the same as: allenai/allennlp#4431 (comment)I'm trying to load a model saved to a local path in the Kaggle environment, using the allennlp.models.load_archive() method but I get the below error:
""ConfigurationError: discourse_classifier is not a registered name for Model. You probably need to use the --include-package flag to load your custom code. Alternatively, you can specify your choices using fully-qualified paths, e.g. {""model"": ""my_module.models.MyModel""} in which case they will be automatically imported correctly.""Model location: https://s3-us-west-2.amazonaws.com/pubmed-rct/model.tar.gzTraceback:OS: Kaggle kernelPython version: 3.7.6",None yet
https://github.com/neoclide/coc.nvim/issues/1672,"I'm wondering if coc-references is supposed to highlight the reference/symbol in the auto preview window by default or some configuration is required.
It seems from https://github.com/neoclide/coc.nvim/wiki/Using-coc-list coc is able to highlight the reference. However, in my setup, once I trigger coc-references, coc opens the auto preview window but does not highlight the symbol.If coc does highlight the symbol, I'm wondering if it is possible to highlight the line that contains the symbol (e.g. using set cursorline) similar to vscode.My mini.vimScreenshotMy CocInfovim version: NVIM v0.4.3
node version: v12.16.1
coc.nvim version: 0.0.77-b7a597f7ae
term: iTerm.app
platform: darwinStarting Jedi Python language engine.
##########Linting Output - pylint##########
************* Module pie
130,38,error,E0602:Undefined variable 'basestring' (undefined-variable)
134,39,error,E0602:Undefined variable 'basestring' (undefined-variable)
276,29,error,E0602:Undefined variable 'xrange' (undefined-variable)
281,21,error,E0602:Undefined variable 'xrange' (undefined-variable)
283,30,error,E0602:Undefined variable 'xrange' (undefined-variable)
516,22,error,E1101:Instance of 'dict' has no 'iterkeys' member (no-member)Your code has been rated at 9.32/10 (previous run: 9.32/10, +0.00)2020-03-20 23:06:31,220 UTC - INFO - pyls.python_ls - Starting PythonLanguageServer IO language server
2020-03-20 23:06:31,221 UTC - DEBUG - pyls_jsonrpc.endpoint - Handling request from client {'jsonrpc': '2.0', 'id': 0, 'method': 'initialize', 'params': {'processId': 85428, 'rootPath': '/Users/mstelmar/1ip/pie', 'rootUri': 'file:///Users/mstelmar/1ip/pie', 'capabilities': {'workspace': {'applyEdit': True, 'workspaceEdit': {'documentChanges': True, 'resourceOperations': ['create', 'rename', 'delete'], 'failureHandling': 'textOnlyTransactional'}, 'didChangeConfiguration': {'dynamicRegistration': True}, 'didChangeWatchedFiles': {'dynamicRegistration': True}, 'symbol': {'dynamicRegistration': True, 'symbolKind': {'valueSet': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]}}, 'executeCommand': {'dynamicRegistration': True}, 'configuration': True, 'workspaceFolders': True}, 'textDocument': {'publishDiagnostics': {'relatedInformation': True, 'versionSupport': False}, 'synchronization': {'dynamicRegistration': True, 'willSave': True, 'willSaveWaitUntil': True, 'didSave': True}, 'completion': {'dynamicRegistration': True, 'contextSupport': True, 'completionItem': {'snippetSupport': True, 'commitCharactersSupport': True, 'documentationFormat': ['markdown', 'plaintext'], 'deprecatedSupport': True, 'preselectSupport': True}, 'completionItemKind': {'valueSet': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]}}, 'hover': {'dynamicRegistration': True, 'contentFormat': ['markdown', 'plaintext']}, 'signatureHelp': {'dynamicRegistration': True, 'signatureInformation': {'documentationFormat': ['markdown', 'plaintext'], 'parameterInformation': {'labelOffsetSupport': True}}}, 'definition': {'dynamicRegistration': True}, 'references': {'dynamicRegistration': True}, 'documentHighlight': {'dynamicRegistration': True}, 'documentSymbol': {'dynamicRegistration': True, 'symbolKind': {'valueSet': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]}, 'hierarchicalDocumentSymbolSupport': True}, 'codeAction': {'dynamicRegistration': True, 'isPreferredSupport': True, 'codeActionLiteralSupport': {'codeActionKind': {'valueSet': ['', 'quickfix', 'refactor', 'refactor.extract', 'refactor.inline', 'refactor.rewrite', 'source', 'source.organizeImports']}}}, 'codeLens': {'dynamicRegistration': True}, 'formatting': {'dynamicRegistration': True}, 'rangeFormatting': {'dynamicRegistration': True}, 'onTypeFormatting': {'dynamicRegistration': True}, 'rename': {'dynamicRegistration': True, 'prepareSupport': True}, 'documentLink': {'dynamicRegistration': True, 'tooltipSupport': True}, 'typeDefinition': {'dynamicRegistration': True}, 'implementation': {'dynamicRegistration': True}, 'declaration': {'dynamicRegistration': True}, 'colorProvider': {'dynamicRegistration': True}, 'foldingRange': {'dynamicRegistration': True, 'rangeLimit': 5000, 'lineFoldingOnly': True}, 'selectionRange': {'dynamicRegistration': True}}, 'window': {'workDoneProgress': True}}, 'initializationOptions': {}, 'trace': 'off', 'workspaceFolders': [{'uri': 'file:///Users/mstelmar/1ip/pie', 'name': 'pie'}]}}
2020-03-20 23:06:31,222 UTC - DEBUG - pyls.python_ls - Language server initialized with 85428 file:///Users/mstelmar/1ip/pie /Users/mstelmar/1ip/pie {}
2020-03-20 23:06:31,327 UTC - WARNING - pyls.config.config - Failed to load pyls entry point 'pydocstyle': No module named 'pydocstyle'
2020-03-20 23:06:31,407 UTC - WARNING - pyls.config.config - Failed to load pyls entry point 'yapf': No module named 'yapf'
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin autopep8 from <module 'pyls.plugins.autopep8_format' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/autopep8_format.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin flake8 from <module 'pyls.plugins.flake8_lint' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/flake8_lint.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin folding from <module 'pyls.plugins.folding' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/folding.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin jedi_completion from <module 'pyls.plugins.jedi_completion' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/jedi_completion.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin jedi_definition from <module 'pyls.plugins.definition' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/definition.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin jedi_highlight from <module 'pyls.plugins.highlight' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/highlight.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin jedi_hover from <module 'pyls.plugins.hover' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/hover.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin jedi_references from <module 'pyls.plugins.references' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/references.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin jedi_signature_help from <module 'pyls.plugins.signature' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/signature.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin jedi_symbols from <module 'pyls.plugins.symbols' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/symbols.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin mccabe from <module 'pyls.plugins.mccabe_lint' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/mccabe_lint.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin preload from <module 'pyls.plugins.preload_imports' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/preload_imports.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin pycodestyle from <module 'pyls.plugins.pycodestyle_lint' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/pycodestyle_lint.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin pyflakes from <module 'pyls.plugins.pyflakes_lint' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/pyflakes_lint.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin pylint from <module 'pyls.plugins.pylint_lint' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/pylint_lint.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin rope_completion from <module 'pyls.plugins.rope_completion' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/rope_completion.py'>
2020-03-20 23:06:31,446 UTC - INFO - pyls.config.config - Loaded pyls plugin rope_rename from <module 'pyls.plugins.rope_rename' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/rope_rename.py'>
2020-03-20 23:06:31,446 UTC - DEBUG - pyls.config.config - pyls_settings [hook]
config: <pyls.config.config.Config object at 0x101857ed0>2020-03-20 23:06:31,446 UTC - DEBUG - pyls.config.config - finish pyls_settings --> [{'plugins': {'rope_completion': {'enabled': False}}}, {'plugins': {'pylint': {'enabled': False, 'args': []}}}, {'plugins': {'preload': {'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, {'plugins': {'flake8': {'enabled': False}}}] [hook]2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - Got user config from PyCodeStyleConfig: {}
2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - With user configuration: {}
2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - With plugin configuration: {'plugins': {'flake8': {'enabled': False}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': False}, 'preload': {'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - With lsp configuration: {'plugins': {'flake8': {'enabled': False}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': False}, 'preload': {'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - Got project config from PyCodeStyleConfig: {}
2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - With project configuration: {'plugins': {'flake8': {'enabled': False}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': False}, 'preload': {'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:31,447 UTC - INFO - pyls.config.config - Disabled plugins: [<module 'pyls.plugins.flake8_lint' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/flake8_lint.py'>, <module 'pyls.plugins.pylint_lint' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/pylint_lint.py'>, <module 'pyls.plugins.rope_completion' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/rope_completion.py'>]
2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - pyls_dispatchers [hook]
config: <pyls.config.config.Config object at 0x101857ed0>
workspace: <pyls.workspace.Workspace object at 0x101866bd0>
document: None2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - finish pyls_dispatchers --> [] [hook]2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - pyls_initialize [hook]
config: <pyls.config.config.Config object at 0x101857ed0>
workspace: <pyls.workspace.Workspace object at 0x101866bd0>
document: None2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - Got user config from PyCodeStyleConfig: {}
2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - With user configuration: {}
2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - With plugin configuration: {'plugins': {'flake8': {'enabled': False}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': False}, 'preload': {'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:31,447 UTC - DEBUG - pyls.config.config - With lsp configuration: {'plugins': {'flake8': {'enabled': False}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': False}, 'preload': {'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:31,448 UTC - DEBUG - pyls.config.config - Got project config from PyCodeStyleConfig: {}
2020-03-20 23:06:31,448 UTC - DEBUG - pyls.config.config - With project configuration: {'plugins': {'flake8': {'enabled': False}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': False}, 'preload': {'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:31,450 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module PIL
2020-03-20 23:06:31,451 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module array
2020-03-20 23:06:31,452 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module audioop
2020-03-20 23:06:31,452 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module binascii
2020-03-20 23:06:31,453 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module cmath
2020-03-20 23:06:31,453 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module collections
2020-03-20 23:06:31,453 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module datetime
2020-03-20 23:06:31,453 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module errno
2020-03-20 23:06:31,454 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module gc
2020-03-20 23:06:31,455 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module imp
2020-03-20 23:06:31,455 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module itertools
2020-03-20 23:06:31,455 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module marshal
2020-03-20 23:06:31,455 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module math
2020-03-20 23:06:31,629 UTC - DEBUG - matplotlib - $HOME=/Users/mstelmar
2020-03-20 23:06:31,629 UTC - DEBUG - matplotlib - CONFIGDIR=/Users/mstelmar/.matplotlib
2020-03-20 23:06:31,629 UTC - DEBUG - matplotlib - matplotlib data path: /usr/local/lib/python3.7/site-packages/matplotlib/mpl-data
2020-03-20 23:06:31,632 UTC - DEBUG - matplotlib - loaded rc file /usr/local/lib/python3.7/site-packages/matplotlib/mpl-data/matplotlibrc
2020-03-20 23:06:31,634 UTC - DEBUG - matplotlib - matplotlib version 3.1.1
2020-03-20 23:06:31,634 UTC - DEBUG - matplotlib - interactive is False
2020-03-20 23:06:31,634 UTC - DEBUG - matplotlib - platform is darwin
2020-03-20 23:06:31,634 UTC - DEBUG - matplotlib - loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', 'zipimport', '_frozen_importlib_external', '_io', 'marshal', 'posix', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', 'main', 'encodings.latin_1', 'io', 'abc', '_abc', '_bootlocale', '_locale', 'site', 'os', 'stat', '_stat', '_collections_abc', 'posixpath', 'genericpath', 'os.path', '_sitebuiltins', 'types', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'functools', '_functools', 'mpl_toolkits', 'google', 'sitecustomize', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'pyls', 'pluggy', 'pluggy._version', 'pluggy.manager', 'inspect', 'dis', 'opcode', '_opcode', 'collections.abc', 'linecache', 'tokenize', 'token', 'pluggy._tracing', 'pluggy.callers', 'pluggy.hooks', 'importlib_metadata', 'future', 'csv', '_csv', 'zipp', 'zipfile', 'time', 'shutil', 'fnmatch', 'errno', 'zlib', 'bz2', '_compression', 'threading', 'traceback', '_weakrefset', '_bz2', 'lzma', '_lzma', 'pwd', 'grp', 'struct', '_struct', 'binascii', 'importlib_metadata._compat', 'email', 'configparser', 'pathlib', 'ntpath', 'urllib', 'urllib.parse', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'string', '_string', 'email.base64mime', 'base64', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'random', 'math', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'socket', '_socket', 'selectors', 'select', 'datetime', '_datetime', 'email._parseaddr', 'calendar', 'locale', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'pyls._version', 'json', 'json.decoder', 'json.scanner', '_json', 'json.encoder', 'pyls.main', 'argparse', 'gettext', 'logging', 'weakref', 'atexit', 'logging.config', 'logging.handlers', 'pickle', '_compat_pickle', '_pickle', 'queue', '_queue', 'copy', 'socketserver', 'decimal', 'numbers', '_decimal', 'ujson', 'pyls.python_ls', 'pyls_jsonrpc', 'pyls_jsonrpc._version', 'pyls_jsonrpc.dispatchers', 'pyls_jsonrpc.endpoint', 'uuid', '_uuid', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'pyls_jsonrpc.exceptions', 'pyls_jsonrpc.streams', 'pyls.lsp', 'pyls._utils', 'distutils', 'distutils.version', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso._compatibility', 'platform', 'subprocess', 'signal', '_posixsubprocess', 'parso.utils', 'ast', '_ast', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.grammar', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'parso.file_io', 'jedi._compatibility', 'pkgutil', 'jedi.file_io', 'jedi.parser_utils', 'textwrap', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.api.classes', 'jedi.evaluate', 'jedi.evaluate.utils', 'jedi.evaluate.imports', 'jedi.evaluate.sys_path', 'jedi.evaluate.cache', 'jedi.evaluate.base_context', 'jedi.common', 'jedi.common.context', 'jedi.evaluate.helpers', 'jedi.common.utils', 'jedi.evaluate.compiled', 'jedi.evaluate.compiled.context', 'jedi.evaluate.filters', 'jedi.evaluate.flow_analysis', 'jedi.evaluate.recursion', 'jedi.evaluate.names', 'jedi.evaluate.lazy_context', 'jedi.evaluate.compiled.access', 'jedi.evaluate.compiled.getattr_static', 'jedi.evaluate.signature', 'jedi.evaluate.analysis', 'jedi.evaluate.gradual', 'jedi.evaluate.gradual.typeshed', 'jedi.evaluate.gradual.stub_context', 'jedi.evaluate.context', 'jedi.evaluate.context.module', 'jedi.evaluate.context.klass', 'jedi.evaluate.arguments', 'jedi.evaluate.context.iterable', 'jedi.evaluate.param', 'jedi.evaluate.docstrings', 'jedi.evaluate.context.function', 'jedi.evaluate.parser_cache', 'jedi.plugins', 'jedi.evaluate.context.instance', 'jedi.evaluate.gradual.typing', 'jedi.evaluate.syntax_tree', 'jedi.evaluate.finder', 'jedi.evaluate.gradual.conversion', 'jedi.evaluate.gradual.annotation', 'jedi.evaluate.context.decorator', 'jedi.api.keywords', 'pydoc', 'sysconfig', '_sysconfigdata_m_darwin_darwin', '_osx_support', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.interpreter', 'jedi.evaluate.compiled.mixed', 'jedi.api.helpers', 'jedi.api.completion', 'jedi.api.file_name', 'jedi.api.environment', 'filecmp', 'jedi.evaluate.compiled.subprocess', 'jedi.evaluate.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.api.project', 'jedi.evaluate.usages', 'jedi.evaluate.gradual.utils', 'jedi.plugins.registry', 'jedi.plugins.stdlib', 'jedi.plugins.flask', 'pyls.uris', 'pyls.config', 'pyls.config.config', 'pkg_resources', 'plistlib', 'xml', 'xml.parsers', 'xml.parsers.expat', 'pyexpat.errors', 'pyexpat.model', 'pyexpat', 'xml.parsers.expat.model', 'xml.parsers.expat.errors', 'tempfile', 'pkg_resources.extern', 'pkg_resources._vendor', 'pkg_resources._vendor.six', 'pkg_resources.extern.six', 'pkg_resources._vendor.six.moves', 'pkg_resources.extern.six.moves', 'pkg_resources._vendor.six.moves.urllib', 'pkg_resources.py31compat', 'pkg_resources._vendor.appdirs', 'pkg_resources.extern.appdirs', 'pkg_resources._vendor.packaging', 'pkg_resources._vendor.packaging.about', 'pkg_resources.extern.packaging', 'pkg_resources.extern.packaging.version', 'pkg_resources.extern.packaging._structures', 'pkg_resources.extern.packaging.specifiers', 'pkg_resources.extern.packaging._compat', 'pkg_resources.extern.packaging.requirements', 'pkg_resources._vendor.pyparsing', 'pprint', 'pkg_resources.extern.pyparsing', 'pkg_resources.extern.six.moves.urllib', 'pkg_resources.extern.packaging.markers', 'pkg_resources.py2_warn', 'pyls.hookspecs', 'pyls.workspace', 'concurrent.futures.thread', 'pyls.config.flake8_conf', 'pyls.config.source', 'pyls.config.pycodestyle_conf', 'pycodestyle', 'optparse', 'pyls.plugins', 'pyls.plugins.autopep8_format', 'autopep8', 'pyls.plugins.flake8_lint', 'pyls.plugins.folding', 'pyls.plugins.jedi_completion', 'pyls.plugins.definition', 'pyls.plugins.highlight', 'pyls.plugins.hover', 'pyls.plugins.references', 'pyls.plugins.signature', 'pyls.plugins.symbols', 'pyls.plugins.mccabe_lint', 'mccabe', 'pyls.plugins.preload_imports', 'pyls.plugins.pycodestyle_lint', 'pyls.plugins.pyflakes_lint', 'pyflakes', 'pyflakes.api', 'pyflakes.checker', 'doctest', 'pdb', 'cmd', 'bdb', 'code', 'codeop', 'glob', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'pyflakes.messages', 'pyflakes.reporter', 'pyls.plugins.pylint_lint', 'pylint', 'pylint.pkginfo', 'pylint.epylint', 'shlex', 'pyls.plugins.rope_completion', 'rope', 'rope.contrib', 'rope.contrib.codeassist', 'rope.base', 'rope.base.codeanalyze', 'rope.base.evaluate', 'rope.base.builtins', 'rope.base.utils', 'rope.base.utils.pycompat', 'rope.base.pynames', 'rope.base.pyobjects', 'rope.base.fscommands', 'rope.base.ast', 'rope.base.exceptions', 'rope.base.arguments', 'rope.base.astutils', 'rope.base.worder', 'rope.base.simplify', 'rope.base.libutils', 'rope.base.project', 'rope.base.resourceobserver', 'rope.base.taskhandle', 'rope.base.prefs', 'rope.base.history', 'rope.base.change', 'rope.base.pycore', 'rope.base.resources', 'rope.base.oi', 'rope.base.oi.doa', 'hmac', 'rope.base.oi.objectinfo', 'rope.base.oi.objectdb', 'rope.base.oi.memorydb', 'rope.base.oi.transform', 'rope.base.oi.soa', 'rope.base.oi.soi', 'rope.base.oi.type_hinting', 'rope.base.oi.type_hinting.factory', 'rope.base.oi.type_hinting.interfaces', 'rope.base.oi.type_hinting.providers', 'rope.base.oi.type_hinting.providers.composite', 'rope.base.oi.type_hinting.providers.interfaces', 'rope.base.oi.type_hinting.providers.inheritance', 'rope.base.oi.type_hinting.utils', 'rope.base.oi.type_hinting.providers.docstrings', 'rope.base.oi.
type_hinting.providers.numpydocstrings', 'rope.base.oi.type_hinting.providers.pep0484_type_comments', 'rope.base.oi.type_hinting.resolvers', 'rope.base.oi.type_hinting.resolvers.composite', 'rope.base.oi.type_hinting.resolvers.interfaces', 'rope.base.oi.type_hinting.resolvers.types', 'rope.base.oi.type_hinting.evaluate', 'rope.base.stdmods', 'rope.base.pyobjectsdef', 'rope.base.pyscopes', 'rope.base.pynamesdef', 'rope.contrib.fixsyntax', 'rope.refactor', 'rope.refactor.importutils', 'rope.refactor.occurrences', 'rope.refactor.rename', 'rope.refactor.importutils.module_imports', 'rope.refactor.importutils.actions', 'rope.refactor.importutils.importinfo', 'rope.refactor.topackage', 'rope.refactor.functionutils', 'pyls.plugins.rope_rename', 'PIL', 'PIL._version', 'array', 'audioop', 'cmath', 'imp', 'matplotlib', 'matplotlib.cbook', 'gzip', 'numpy', 'numpy._globals', 'numpy.config', 'numpy.version', 'numpy._distributor_init', 'numpy.core', 'numpy.core.info', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core._exceptions', 'numpy.core._asarray', 'numpy.core._ufunc_config', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.shape_base', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', '_ctypes', 'ctypes', 'ctypes._endian', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.linalg', 'numpy.linalg.info', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.mixins', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft.info', 'numpy.fft.pocketfft', 'numpy.fft.pocketfft_internal', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random._pickle', 'numpy.random.mtrand', 'cython_runtime', 'numpy.random.common', 'numpy.random.bounded_integers', 'numpy.random.mt19937', 'numpy.random.bit_generator', '_cython_0_29_13', 'secrets', 'numpy.random.entropy', 'numpy.random.philox', 'numpy.random.pcg64', 'numpy.random.sfc64', 'numpy.random.generator', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.testing', 'numpy.testing._private', 'numpy.testing._private.utils', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'six', 'six.moves', 'matplotlib._version', 'matplotlib.ft2font', 'dateutil', 'dateutil._version', 'kiwisolver']
2020-03-20 23:06:31,634 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module matplotlib
2020-03-20 23:06:31,635 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module mmap
2020-03-20 23:06:31,636 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module numpy
2020-03-20 23:06:31,636 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module operator
2020-03-20 23:06:31,636 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module os
2020-03-20 23:06:31,636 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module os.path
2020-03-20 23:06:32,039 UTC - DEBUG - matplotlib - CACHEDIR=/Users/mstelmar/.matplotlib
2020-03-20 23:06:32,043 UTC - DEBUG - matplotlib.font_manager - Using fontManager instance from /Users/mstelmar/.matplotlib/fontlist-v310.json
2020-03-20 23:06:32,176 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module pandas
2020-03-20 23:06:32,177 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module parser
2020-03-20 23:06:32,183 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module scipy
2020-03-20 23:06:32,183 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module signal
2020-03-20 23:06:32,351 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module sklearn
2020-03-20 23:06:32,352 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module statsmodels
2020-03-20 23:06:32,352 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module sys
2020-03-20 23:06:32,353 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module time
2020-03-20 23:06:32,353 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module xxsubtype
2020-03-20 23:06:32,353 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module zipimport
2020-03-20 23:06:32,353 UTC - DEBUG - pyls.plugins.preload_imports - Preloaded module zlib
2020-03-20 23:06:32,353 UTC - DEBUG - pyls.config.config - finish pyls_initialize --> [] [hook]2020-03-20 23:06:32,353 UTC - DEBUG - pyls.config.config - pyls_commands [hook]
config: <pyls.config.config.Config object at 0x101857ed0>
workspace: <pyls.workspace.Workspace object at 0x101866bd0>
document: None2020-03-20 23:06:32,353 UTC - DEBUG - pyls.config.config - finish pyls_commands --> [] [hook]2020-03-20 23:06:32,353 UTC - DEBUG - pyls.config.config - pyls_experimental_capabilities [hook]
config: <pyls.config.config.Config object at 0x101857ed0>
workspace: <pyls.workspace.Workspace object at 0x101866bd0>
document: None2020-03-20 23:06:32,353 UTC - DEBUG - pyls.config.config - finish pyls_experimental_capabilities --> [] [hook]2020-03-20 23:06:32,353 UTC - INFO - pyls.python_ls - Server capabilities: {'codeActionProvider': True, 'codeLensProvider': {'resolveProvider': False}, 'completionProvider': {'resolveProvider': False, 'triggerCharacters': ['.']}, 'documentFormattingProvider': True, 'documentHighlightProvider': True, 'documentRangeFormattingProvider': True, 'documentSymbolProvider': True, 'definitionProvider': True, 'executeCommandProvider': {'commands': []}, 'hoverProvider': True, 'referencesProvider': True, 'renameProvider': True, 'foldingRangeProvider': True, 'signatureHelpProvider': {'triggerCharacters': ['(', ',', '=']}, 'textDocumentSync': {'change': 2, 'save': {'includeText': True}, 'openClose': True}, 'workspace': {'workspaceFolders': {'supported': True, 'changeNotifications': True}}, 'experimental': {}}
2020-03-20 23:06:32,353 UTC - DEBUG - pyls_jsonrpc.endpoint - Got result from synchronous request handler: {'capabilities': {'codeActionProvider': True, 'codeLensProvider': {'resolveProvider': False}, 'completionProvider': {'resolveProvider': False, 'triggerCharacters': ['.']}, 'documentFormattingProvider': True, 'documentHighlightProvider': True, 'documentRangeFormattingProvider': True, 'documentSymbolProvider': True, 'definitionProvider': True, 'executeCommandProvider': {'commands': []}, 'hoverProvider': True, 'referencesProvider': True, 'renameProvider': True, 'foldingRangeProvider': True, 'signatureHelpProvider': {'triggerCharacters': ['(', ',', '=']}, 'textDocumentSync': {'change': 2, 'save': {'includeText': True}, 'openClose': True}, 'workspace': {'workspaceFolders': {'supported': True, 'changeNotifications': True}}, 'experimental': {}}}
2020-03-20 23:06:32,355 UTC - DEBUG - pyls_jsonrpc.endpoint - Handling notification from client {'jsonrpc': '2.0', 'method': 'initialized', 'params': {}}
2020-03-20 23:06:32,355 UTC - DEBUG - pyls.config.config - pyls_initialized [hook]
config: <pyls.config.config.Config object at 0x101857ed0>
workspace: <pyls.workspace.Workspace object at 0x101866bd0>
document: None2020-03-20 23:06:32,355 UTC - DEBUG - pyls.config.config - finish pyls_initialized --> [] [hook]2020-03-20 23:06:32,359 UTC - DEBUG - pyls_jsonrpc.endpoint - Handling notification from client {'jsonrpc': '2.0', 'method': 'workspace/didChangeConfiguration', 'params': {'settings': {'pyls': {'enable': True, 'trace': {'server': 'off'}, 'commandPath': '', 'configurationSources': ['pycodestyle'], 'plugins': {'jedi_completion': {'enabled': True}, 'jedi_hover': {'enabled': True}, 'jedi_references': {'enabled': True}, 'jedi_signature_help': {'enabled': True}, 'jedi_symbols': {'enabled': True, 'all_scopes': True}, 'mccabe': {'enabled': True, 'threshold': 15}, 'preload': {'enabled': True}, 'pycodestyle': {'enabled': True}, 'pydocstyle': {'enabled': False, 'match': '(?!test_).\.py', 'matchDir': '[^\.].'}, 'pyflakes': {'enabled': True}, 'rope_completion': {'enabled': True}, 'yapf': {'enabled': True}}}}}}
2020-03-20 23:06:32,359 UTC - INFO - pyls.config.config - Updated settings to {'enable': True, 'trace': {'server': 'off'}, 'commandPath': '', 'configurationSources': ['pycodestyle'], 'plugins': {'jedi_completion': {'enabled': True}, 'jedi_hover': {'enabled': True}, 'jedi_references': {'enabled': True}, 'jedi_signature_help': {'enabled': True}, 'jedi_symbols': {'enabled': True, 'all_scopes': True}, 'mccabe': {'enabled': True, 'threshold': 15}, 'preload': {'enabled': True}, 'pycodestyle': {'enabled': True}, 'pydocstyle': {'enabled': False, 'match': '(?!test_).\.py', 'matchDir': '[^\.].'}, 'pyflakes': {'enabled': True}, 'rope_completion': {'enabled': True}, 'yapf': {'enabled': True}}}
2020-03-20 23:06:32,359 UTC - DEBUG - pyls.config.config - Got user config from PyCodeStyleConfig: {}
2020-03-20 23:06:32,359 UTC - DEBUG - pyls.config.config - With user configuration: {}
2020-03-20 23:06:32,359 UTC - DEBUG - pyls.config.config - With plugin configuration: {'plugins': {'flake8': {'enabled': False}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': False}, 'preload': {'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:32,359 UTC - DEBUG - pyls.config.config - With lsp configuration: {'plugins': {'pycodestyle': {'enabled': True}, 'jedi_references': {'enabled': True}, 'pydocstyle': {'enabled': False, 'match': '(?!test_).\.py', 'matchDir': '[^\.].'}, 'jedi_symbols': {'enabled': True, 'all_scopes': True}, 'flake8': {'enabled': False}, 'preload': {'enabled': True, 'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}, 'yapf': {'enabled': True}, 'jedi_hover': {'enabled': True}, 'jedi_completion': {'enabled': True}, 'jedi_signature_help': {'enabled': True}, 'pyflakes': {'enabled': True}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': True}, 'mccabe': {'enabled': True, 'threshold': 15}}, 'enable': True, 'commandPath': '', 'trace': {'server': 'off'}, 'configurationSources': ['pycodestyle'], 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:32,360 UTC - DEBUG - pyls.config.config - Got project config from PyCodeStyleConfig: {}
2020-03-20 23:06:32,360 UTC - DEBUG - pyls.config.config - With project configuration: {'plugins': {'pycodestyle': {'enabled': True}, 'jedi_references': {'enabled': True}, 'pydocstyle': {'enabled': False, 'match': '(?!test_).\.py', 'matchDir': '[^\.].'}, 'jedi_symbols': {'enabled': True, 'all_scopes': True}, 'flake8': {'enabled': False}, 'preload': {'enabled': True, 'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}, 'yapf': {'enabled': True}, 'jedi_hover': {'enabled': True}, 'jedi_completion': {'enabled': True}, 'jedi_signature_help': {'enabled': True}, 'pyflakes': {'enabled': True}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': True}, 'mccabe': {'enabled': True, 'threshold': 15}}, 'configurationSources': ['pycodestyle'], 'enable': True, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}, 'trace': {'server': 'off'}, 'commandPath': ''}
2020-03-20 23:06:32,360 UTC - INFO - pyls.config.config - Disabled plugins: [None, <module 'pyls.plugins.flake8_lint' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/flake8_lint.py'>, <module 'pyls.plugins.pylint_lint' from '/usr/local/lib/python3.7/site-packages/pyls/plugins/pylint_lint.py'>]
2020-03-20 23:06:32,366 UTC - DEBUG - pyls_jsonrpc.endpoint - 2020-03-20 23:06:35,800 UTC - DEBUG - pyls.config.config - pyls_references [hook]
config: <pyls.config.config.Config object at 0x101857ed0>
workspace: <pyls.workspace.Workspace object at 0x101866bd0>
document: file:///Users/mstelmar/1ip/pie/pie.py
position: {'line': 102, 'character': 9}
exclude_declaration: True2020-03-20 23:06:35,800 UTC - DEBUG - pyls.config.config - Got user config from PyCodeStyleConfig: {}
2020-03-20 23:06:35,800 UTC - DEBUG - pyls.config.config - With user configuration: {}
2020-03-20 23:06:35,800 UTC - DEBUG - pyls.config.config - With plugin configuration: {'plugins': {'flake8': {'enabled': False}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': False}, 'preload': {'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:35,800 UTC - DEBUG - pyls.config.config - With lsp configuration: {'plugins': {'pycodestyle': {'enabled': True}, 'jedi_references': {'enabled': True}, 'pydocstyle': {'enabled': False, 'match': '(?!test_).\.py', 'matchDir': '[^\.].'}, 'jedi_symbols': {'enabled': True, 'all_scopes': True}, 'flake8': {'enabled': False}, 'preload': {'enabled': True, 'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}, 'yapf': {'enabled': True}, 'jedi_hover': {'enabled': True}, 'jedi_completion': {'enabled': True}, 'jedi_signature_help': {'enabled': True}, 'pyflakes': {'enabled': True}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': True}, 'mccabe': {'enabled': True, 'threshold': 15}}, 'enable': True, 'commandPath': '', 'trace': {'server': 'off'}, 'configurationSources': ['pycodestyle'], 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}}
2020-03-20 23:06:35,800 UTC - DEBUG - pyls.config.config - Got project config from PyCodeStyleConfig: {}
2020-03-20 23:06:35,801 UTC - DEBUG - pyls.config.config - With project configuration: {'plugins': {'pycodestyle': {'enabled': True}, 'jedi_references': {'enabled': True}, 'pydocstyle': {'enabled': False, 'match': '(?!test_).\.py', 'matchDir': '[^\.].'}, 'jedi_symbols': {'enabled': True, 'all_scopes': True}, 'flake8': {'enabled': False}, 'preload': {'enabled': True, 'modules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}, 'yapf': {'enabled': True}, 'jedi_hover': {'enabled': True}, 'jedi_completion': {'enabled': True}, 'jedi_signature_help': {'enabled': True}, 'pyflakes': {'enabled': True}, 'pylint': {'enabled': False, 'args': []}, 'rope_completion': {'enabled': True}, 'mccabe': {'enabled': True, 'threshold': 15}}, 'configurationSources': ['pycodestyle'], 'enable': True, 'rope': {'extensionModules': ['OpenGL', 'PIL', 'array', 'audioop', 'binascii', 'cPickle', 'cStringIO', 'cmath', 'collections', 'datetime', 'errno', 'exceptions', 'gc', 'imageop', 'imp', 'itertools', 'marshal', 'math', 'matplotlib', 'mmap', 'mpmath', 'msvcrt', 'networkx', 'nose', 'nt', 'numpy', 'operator', 'os', 'os.path', 'pandas', 'parser', 'rgbimg', 'scipy', 'signal', 'skimage', 'sklearn', 'statsmodels', 'strop', 'sympy', 'sys', 'thread', 'time', 'wx', 'xxsubtype', 'zipimport', 'zlib']}, 'trace': {'server': 'off'}, 'commandPath': ''}
2020-03-20 23:06:36,013 UTC - DEBUG - pyls.config.config - finish pyls_references --> [[{'uri': 'file:///Users/mstelmar/1ip/pie/pie.py', 'range': {'start': {'line': 102, 'character': 4}, 'end': {'line': 102, 'character': 15}}}, {'uri': 'file:///Users/mstelmar/1ip/pie/pie.py', 'range': {'start': {'line': 343, 'character': 8}, 'end': {'line': 343, 'character': 19}}}]] [hook]2020-03-20 23:06:36,013 UTC - DEBUG - pyls_jsonrpc.endpoint - Got result from synchronous request handler: [{'uri': 'file:///Users/mstelmar/1ip/pie/pie.py', 'range': {'start': {'line': 102, 'character': 4}, 'end': {'line': 102, 'character': 15}}}, {'uri': 'file:///Users/mstelmar/1ip/pie/pie.py', 'range': {'start': {'line': 343, 'character': 8}, 'end': {'line': 343, 'character': 19}}}]Checkout :hi Search or change highlight group by list.previewHighlightGroup configuration.Thanks, I've figured the problem. I added rules in my init.vim to remove line highlight (cursorline) when a window was not in focus. By removing the second line below I was able to solve the problem.
autocmd WinEnter * set cul
autocmd WinLeave * set nocul",Can't reproduce
https://github.com/scikit-learn/scikit-learn/issues/9250,"This is maybe for 1.0. Recently we started marking files like model_selection._split private, so that everything has a single canonical import:For many (older?) models that's not the case, we haveetc.
I think it would be nice to make all the files that are not the canonical import (according to the API documentation) private (with deprecation obviously).
That might be a bit annoying for existing users that used long import paths, but it makes auto-complete much more helpful and the module structure much less confusing for newcomers.For example sklearn.linear_model.ridge is a module, while sklearn.linear_model.ridge_regression is a function that implements ridge regression and sklearn.linear_model.Ridge is a class that implements ridge regression. From the names this is totally unclear.I would support this as it seems like it could clear up a lot of confusion. Do you believe there are any people directly trying to use the functions that would be made private?Since #14939 is merged, we seem to be deprecating them already for 0.22. This now seems like an easy one for some of the upcoming sprints. Although if we want it done before the end of october, we should do it ourselves maybe.Below is the list of the folders whose files need to be preceded by an underscore. The procedure is pretty straightforward:(Note that this only addresses the folders. Files like sklearn/pipeline.py may still contain non-deprecated things.)ping @thomasjpfan @glemaitre @adrinjalali let's pick one each and review each-other? this should go relatively fastI'll pick clusterI'm taking the tree and ensemble, since they're interrelated.I'll take preprocessing and inspection at firstpretty sure folks have not been using git move correctly. Try doing a single commit that does the move and commit that, before adding anything else. Your PRs don't show files as moved, which means that something went wrong.I did move, as you see in this commit df7f54d but once you create a file with the same name, github gets confused.I'm working on mixture and svmWorking on covarianceworking on cross_decompositionworking on metrics and metrics/clusterWorking on feature_extractionWorking on inspectionWorking on preprocessingWorking on linear_models@thomasjpfan From the list, I only did not review the PR for the decomposition. I did not open a PR because you seem to have work on it. Could you open a PR?ping @NicolasHug @thomasjpfan what about sklearn.feature_selection?Is #15367 a blocker?Working on feature_selectionThanks I missed this oneYes but it's tracked by another issueI had a ton of private datasets, such as names, text and geographical data. I used to use the following utils to download and manage these:What's the recommended approach now?you can import from sklearn.datasets._base import ...However, now you are aware that _base will be private and you might have to change your code at a new scikit-learn release since we might not provide backward-compatible code.This said, be aware that we are going to define a developer API which will define some backward support for these types of utilities used in third-party.",API Blocker
https://github.com/freqtrade/freqtrade/issues/2684,"If (.env) dwhee@dwVB:~/freqtrade$ python3 ./freqtrade/main.py -c config.json -s bbrsi
/home/dwhee/freqtrade/.env/lib/python3.6/site-packages/sklearn/externals/joblib/init.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
warnings.warn(msg, category=FutureWarning)
/home/dwhee/freqtrade/.env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.
warnings.warn(message, FutureWarning)
Fatal exception!
Traceback (most recent call last):
File ""./freqtrade/main.py"", line 42, in main
config = Configuration(args).get_config()
File ""/home/dwhee/freqtrade/freqtrade/configuration.py"", line 299, in get_config
self.config = self.load_config()
File ""/home/dwhee/freqtrade/freqtrade/configuration.py"", line 47, in load_config
config = self._load_config_file(self.args.config)
File ""/home/dwhee/freqtrade/freqtrade/configuration.py"", line 81, in _load_config_file
conf = json.load(file)
File ""/usr/lib/python3.6/json/init.py"", line 299, in load
parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
File ""/usr/lib/python3.6/json/init.py"", line 354, in loads
return _default_decoder.decode(s)
File ""/usr/lib/python3.6/json/decoder.py"", line 339, in decode
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
File ""/usr/lib/python3.6/json/decoder.py"", line 357, in raw_decode
raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 55 column 11 (char 1429)
you have discovered a bug in the bot, please search our issue tracker.
If it hasn't been reported, please create a new issue.Explain the problem you have encounteredjson.decoder.JSONDecodeError: Expecting value: line 55 column 11 (char 1429)
-- wrong symbol at your config, at line 55 and column 11.This is what I have, is it wrong?:
},
""pair_whitelist"": [
""ETH/BTC"",
""XRP/BTC"",
""WAVES/BTC"",
""FET/BTC"",
""BNB/BTC"",
""XTZ/BTC"",
""LINK/BTC"",
""LTC/BTC"",
""MATIC/BTC"",
""TOMO/BTC"",
""ATOM/BTC"",
""EOS/BTC"",
""BCH/BTC"",
""XMR/BTC"",
""VET/BTC"",
""THETA/BTC"",
""ZEC/BTC"",
""ENJ/BTC"",
], #this is line 54
""pair_blacklist"": [
""DOGE/BTC""
]
},Json does not allow a list to have a ""trailing"" colon.These lines should look as follows:That fixed that problem. Now when I run I get this issue:
Traceback (most recent call last):
File ""./freqtrade/main.py"", line 45, in main
freqtrade = FreqtradeBot(config)
File ""/home/dwhee/freqtrade/freqtrade/freqtradebot.py"", line 54, in init
self.strategy: IStrategy = StrategyResolver(self.config).strategy
File ""/home/dwhee/freqtrade/freqtrade/resolvers/strategy_resolver.py"", line 40, in init
extra_dir=config.get('strategy_path'))
File ""/home/dwhee/freqtrade/freqtrade/resolvers/strategy_resolver.py"", line 141, in _load_strategy
"" or contains Python code errors"".format(strategy_name)
ImportError: Impossible to load Strategy 'bbrsi'. This class does not exist or contains Python code errors
which file do I need to fix and where in the file? I get confused when it says 4 different referenceswell - you need to have a file with a valid strategy class named bbrsi (case sensitive) in user_data/strategies (or whatever you specified as --strategy-path.Thank you.From my_init_ file:
from freqtrade.strategy.default_strategy import DefaultStrategy # noqa: F401
from freqtrade.strategy.bbrsi import bbrsi
logger = logging.getLogger(name)from my bbrsi.py file:
import talib.abstract as ta
from pandas import DataFrameimport freqtrade.vendor.qtpylib.indicators as qtpylib
from freqtrade.indicator_helpers import fishers_inverse
from freqtrade.strategy.interface import IStrategyclass bbrsi(IStrategy):Both files are under freqtrade/strategyAll is resolved. Thank you for all of your help, that is what makes this community so great!",Question
https://github.com/scikit-learn/scikit-learn/pull/16401,"Fixes #16391Add deprecation notice to fit_grid_point function because it is not used anywhere except testThis needs a test to make sure the deprecation is raised with the expected message.LGTM.Actually not completely good:0.22 was already released. The next release will be 0.23.Add a comment above test_fit_grid_point to remove it in 0.25.We will squash your PR when it is merge. There is no need to push force push your changes.Please add an entry to the change log at doc/whats_new/v0.23.rst with tag API. Like the other entries there, please reference this pull request with :pr: and credit yourself (and other contributors if applicable) with :user:.Also update doc/modules/classes.rst and move model_selection.fit_grid_point to the deprecated section.Thank you @ariepratama !sorry to give you headache when reviewing my codes @thomasjpfan",None yet
https://github.com/scikit-learn/scikit-learn/pull/15105,"Ping @adrinjalali is this what was decided during the sprint?(CI will break because I'm not catching the deprecation warnings)Left a few notes as I was reading. Yeah this is what we had in mind IIRC (from the estimator's perspective).The implementation in the pipeline would be trickier.Are you sure? for the fit_param, we could just do e.g. pipe.fit(X, y, warm_start_with={'hist_gbdt__max_iter': 100}and the _warmstartable_parameters attribute could just be a property that we would set depending on the last step? (This is assuming that only the last step of the pipeline can be warm-started which I guess is reasonable)I was thinking of allowing warm start, or refitting the pipeline, having changed a parameter anywhere, and only refit the steps after the changed one. So in a sense, pipeline can warm start with almost any of its parametersYou mean, when doing pipe.set_params({'second_step__something': 12}) then not fitting the first step again when calling pip.fit()?I feel like this is yet another kind of warm-starting, mostly orthogonal to the kind of warm-starting that this API is concerned with?So for now, we forget about warm start on the pipeline for this PR, and we should test for metaestimators such as pipeline and Voting estimators.Are you happy with how it looks so far @adrinjalali ? (test failure is unrelated)I think we should write a SLEP for this.Once we decide on the API, I think we should put the warm startable params higher in the meta-estimator hierarchy (I think), but otherwise looks good.And yeah we should write a slep for the new API.@jnothman I'd appreciate your thoughts on this one.The benefits of this approach aren't clear to me, and we have enough SLEPs to deal with ATM. Closing, might re-open one day",None yet
https://github.com/scikit-learn/scikit-learn/pull/13565,"I would have though there are more estimators which support sample_weights than not (and hence set the default value of the parameter to True).From this snippet I tried I observed the reverse:Also it makes sense to me to play it safe and let the default be falseI'm holding off on this for a bit.There are many complications to using estimator tags inside the code of the estimators (for now all is fine since they're only used in the estimator checks, but not anymore in this PR).A few issues:Of course it should still be possible to introduce the tag and only use it in the estimator checks.EDIT: point 1 and 2 have been resolved now and child classes can override parents tags",Needs work
https://github.com/zooniverse/aggregation-for-caesar/issues/224,"Cairo Geniza team ran into an issue where hdbscan 0.8.19 was trying to import joblib.parallel.cpu_count function from sklearn.externals, but in scikit-learn>=0.21 joblib must be installed on its own. Adding hdbscan >0.8.20 requirement would fix this, but perhaps unpinning previous scikit-learn version?Looks like the deprecation warning finally caught up with us (see #170)?",None yet
https://github.com/scikit-learn/scikit-learn/pull/15832,"Fix #15788.Recently, numpy deprecates automatic conversion of ragged arrays to object (see numpy/numpy#14794 for details). Therefore, this kind of arrays must be explicitly created with the parameter dtype=object (NEP 34 - Disallow inferring dtype=object for sequences).This issue has been easily solved for the module sklearn/neighbors/_base.py (lines 279 and 280) since the returned array is always ragged.However, also affects to methods like is_multilabel (line 138) and target_type (line 251) in module sklearn/utils/multiclass or check_array (line 515) in sklearn/utils/validation.py (among others) because the deprecation also affects to any call that internally calls np.asarray (for instance, the assert_equal family of functions calls np.asarray). Thus, these cases require more work, investigation and effort.The PR implementing the NEP 34 causes a lot of problems also in NumPy (numpy/numpy#15053) Pandas (pandas-dev/pandas#30035) and Astropy (astropy/astropy#9716). Therefore, they reverse the PR (numpy/numpy#15066) and leave it out of the 1.18.x and 1.19.x series and work on it in the master.Therefore, this issue should not cause more problems in the travis CRON (not for the moment).Any comments will be appreciated, since the issue is causing more problems than expected.I think this patch correct. Are there any other scikit-learn issues with this change in numpy, i.e. is this patch sufficient for our tests to pass?Yes, it is. In fact, NumPy core developers decided to revert back the PR causing the original issue and it was ""automatically"" solved (see, for instance, https://travis-ci.org/scikit-learn/scikit-learn/builds/622285240?utm_source=github_status&utm_medium=notification).In the future, I think that it will be important to take care of calls to np.asarray method that could receive object dtypes. Let us see what NumPy developers decide.",None yet
https://github.com/scikit-learn/scikit-learn/issues/13791,"When using the 'n_values' parameter, the code works as expected and I get the deprecation warning. However, when the code is changed to what's suggested in deprecation warning, getting a ValueError.
This works:from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(n_values= 8,sparse = False)
o = ohe.fit_transform(np.array([[3, 5, 1]]))
o = o.reshape(1,3,8)
print(o)[[[0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.]]] C:\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:331: DeprecationWarning: Passing 'n_values' is deprecated in version 0.20 and will be removed in 0.22. You can use the 'categories' keyword instead. 'n_values=n' corresponds to 'categories=[range(n)]'. warnings.warn(msg, DeprecationWarning)from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(categories = [range(8)],sparse = False)
o = ohe.fit_transform(np.array([[3, 5, 1]]))
o = o.reshape(1,3,8)
print(o)``ValueError Traceback (most recent call last)
in
2 import pandas as pd
3 ohe = OneHotEncoder(categories = [range(8)],sparse = False)
----> 4 o = ohe.fit_transform(np.array([[3, 5, 1]]))
5 # pd.get_dummies(np.array([3, 5, 1]))
6 o.reshape(1,3,8)C:\Anaconda3\lib\site-packages\sklearn\preprocessing_encoders.py in fit_transform(self, X, y)
516 self._categorical_features, copy=True)
517 else:
--> 518 return self.fit(X).transform(X)
519
520 def _legacy_transform(self, X):C:\Anaconda3\lib\site-packages\sklearn\preprocessing_encoders.py in fit(self, X, y)
427 return self
428 else:
--> 429 self._fit(X, handle_unknown=self.handle_unknown)
430 return self
431C:\Anaconda3\lib\site-packages\sklearn\preprocessing_encoders.py in _fit(self, X, handle_unknown)
70 ""supported for numerical categories"")
71 if len(self._categories) != n_features:
---> 72 raise ValueError(""Shape mismatch: if n_values is an array,""
73 "" it has to be of shape (n_features,)."")
74ValueError: Shape mismatch: if n_values is an array, it has to be of shape (n_features,).``System:
python: 3.7.1 | packaged by conda-forge | (default, Nov 13 2018, 19:01:41) [MSC v.1900 64 bit (AMD64)]
executable: C:\Anaconda3\python.exe
machine: Windows-10-10.0.16299-SP0BLAS:
macros:
lib_dirs:
cblas_libs: cblasPython deps:
pip: 18.1
setuptools: 40.6.3
sklearn: 0.20.3
numpy: 1.15.4
scipy: 1.1.0
Cython: 0.29.2
pandas: 0.23.4This was a test code. What I am trying to do is replicate the 'n_values' parameter setting with 'categorical' parameter. E.g. if I am trying to one hot encode the 'n' number of distinct data points, I would need 'n' categories beforehand even if I have only 3 categories to onehotencode now.Can I do some work on this?A pull request improving the documentation would be very welcome, @rick2047hi @jnothman, I thought about it and its a bit weird to put it in the way you are suggesting. In the legacy mode all you need to specify is how many categories you want, but in the new way you need to specify both how many features you have and how many categories you have. Example,Is that intentional? If yes then I have a pull request ready (currently in my commit, here).Something like this worked for me.Here I am giving 0 and 1 for a column named countries.",None yet
https://github.com/tensorflow/community/issues/108,"ImportError Traceback (most recent call last)
in ()----> 3 import sklearn
4 from sklearn.cluster import KMeans as km
5 from sklearn.preprocessing import StandardScaler./anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/init.py in ()
74 else:
75 from . import __check_build
---> 76 from .base import clone
77 from .utils._show_versions import show_versions
78./anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/base.py in ()
14
15 from . import version
---> 16 from .utils import _IS_32BIT
17
18 _DEFAULT_TAGS = {./anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/utils/init.py in ()
18 from ..exceptions import DataConversionWarning
19 from .deprecation import deprecated
---> 20 from .validation import (as_float_array,
21 assert_all_finite,
22 check_random_state, column_or_1d, check_array,ImportError: cannot import name 'check_scalar'Hi Ksenia, it seems that this is about scikit learn and neither about TensorFlow nor about community proposals for TensorFlow. Please post an issue at the right place.Apologies, but this issue tracker is for TF Community governance. I encourage you to address questions to the tensorflow/tensorflow issue tracker, discuss@tensorflow.org or StackOverflow.",None yet
https://github.com/flatironinstitute/CaImAn/issues/693,"Windows / python 3.8 / Anaconda jupyer notebook
4. Which of the demo scripts you're using for your analysis (if applicable):C:\Users\USER\Anaconda3\envs\caiman\lib\site-packages\sklearn\utils\deprecation.py:144: FutureWarning: The sklearn.decomposition.incremental_pca module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.
warnings.warn(message, FutureWarning)
C:\Users\USER\Anaconda3\envs\caiman\lib\site-packages\tensorflow\python\framework\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
C:\Users\USER\Anaconda3\envs\caiman\lib\site-packages\tensorflow\python\framework\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
C:\Users\USER\Anaconda3\envs\caiman\lib\site-packages\tensorflow\python\framework\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
C:\Users\USER\Anaconda3\envs\caiman\lib\site-packages\tensorflow\python\framework\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
C:\Users\USER\Anaconda3\envs\caiman\lib\site-packages\tensorflow\python\framework\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
C:\Users\USER\Anaconda3\envs\caiman\lib\site-packages\tensorflow\python\framework\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
np_resource = np.dtype([(""resource"", np.ubyte, 1)])Anaconda will support version 0.23 in the future so you'll be able to use it. Also in our environment file we specify python 3.7 so we cannot support any installations with python 3.8 at this point.
Please stick to the versions supported by anaconda and specified by the caiman environment.",None yet
https://github.com/uber/causalml/issues/96,"xgboost 0.90 deprecated reg:linear in favor of reg:squarederror. When you run the package tests with current xgboost, it dumps a huge number of deprecation warnings.It looks like reg:linear is only mentioned by the XGBRRegressor constructor, but it's a part of the existing public interface (""Effect learner objective has to be rank:pairwise or reg:linear""), and reg:linear is required by versions before 0.90.I'd suggest checking the xgboost version and replacing the objective with reg:squarederror if the version is 0.90 or greater, and tweaking the public interface to allow effect_learner_objective='reg:squarederror' itself and apply the same logic to pass the appropriate objective needed by the particular xgboost version.I dug in a bit, and it looks like the majority of these warnings are actually within xgboost itself -- the current release has some defaults still set to ""reg:linear"" despite it being deprecated: https://github.com/dmlc/xgboost/blob/release_0.90/python-package/xgboost/sklearn.py#L139That means that even lines like learner = BaseSRegressor(learner=XGBRegressor()) that don't explictly specify reg:linear are throwing warnings. Those look like they can be cleaned up with a helper function that ensures XGBRegressor gets a non-deprecated objective function.",warning
https://github.com/scikit-learn/scikit-learn/pull/16587,"branch for the 0.22.2 bug fix release.
I only took commits from the milestone. If you think about other commits that need to be in 0.22.2, please tell me.Will add #16586 when mergedI added #16232 which skips a failing doctestand #16040 which avoids a deprecation warning for pandas sparse arrayI just also merged #16586 and this PR LGTM.Actually I merged #16586 too quickly: doc/index.rst also needs to be updated to add a link to 0.22.2 from the news section of the home page.I'm not sure about that. This file no longer exists, it's now in template/index.html and the version seems automatically detected. It's correctly updated in the artifactsyou must have been looking to the old maintainers guide :)indeed it's now in template/index.html but we still need a new line such as as:Right but that should be done after the release right ?Yes but why not make it part of the release PR?Actually maybe it's better no to update the homepage of scikit-learn before the wheels are on pypi.org.As you wish. But building the website twice is kind of a waste :)I kinda tend to put it in the release PR.The docs are updated when you merge this PR anyway. It's just a matter of what we include before, what after. I find it nicer if the tag is to the point where the wheels are made from.I'm a bit lost here. updating the doc here will only impact the stable version of the web site when merged, right ?
we'll then need to also update the dev version in a PR pointing to master ?Yes it only changes the stable one. For certain things you don't need to change anything with a bug-fix release. For instance, with this release the 0.23 doesn't change in the dev/master. But You need to add the bug-fix entry also to the main page in master if you add it here.So I checked and for 0.22.1 it was done in 2 PRs, one for the dev branch and one backported to 0.22.X after the release.I'm ok to include it here and then make a PR to master after the release. Is it ok for you ?Sounds good to meI made a sdist and built it in a fresh env and observed weird behavior. pip first gather numpy scipy and joblib then complains that numpy is not installed but keep working and build and install sklearn successfully. Can someone try locally and tell me if he reproduces ?Then I ran the test suite and there's one test failing: test_ard_accuracy_on_easy_problemThat seems to be a ""known"" issue. Although we don't seem to have worked on it: #15420, #16097, #15186I still don't get in which order the setups are run. But I expect numpy to be installed by the time setup.py runs, which needs numpy. I vaguely remember having to install numpy before installing certain packages cause it wouldn't understand that it's the setup itself which needs numpy.To get the wheels to build on the MacPython CI it might safer to skip the test_ard_accuracy_on_easy_problem test in the 0.22.X branch with a link to the issue tracker.This will probably be fixed in master with pyproject.toml that explicitly gives build dependencies. But I think it's not a problem to leave it the way it is for 0.22.2.It's green, should we merge and tag ?LGTM@ogrisel wanna double check and merge?fwi the PR on MacPython /scikit-learn-wheels (MacPython/scikit-learn-wheels#47) is green when tested against this branchGo!",None yet
https://github.com/flatironinstitute/CaImAn/issues/708,,None
https://github.com/scikit-learn/scikit-learn/issues/2879,,None
https://github.com/tensorflow/tensorflow/issues/23198,,None
https://github.com/civisanalytics/civisml-extensions/pull/47,,None
https://github.com/scikit-learn/scikit-learn/pull/15071,,None
https://github.com/conda/conda/issues/9658,,None
https://github.com/flatironinstitute/CaImAn/issues/691,,None
https://github.com/tensorflow/tensorflow/issues/34452,,None
https://github.com/lmcinnes/umap/issues/252,,None
https://github.com/dask/dask-ml/issues/560,,None
https://github.com/USGS-Astrogeology/PyHAT_Point_Spectra_GUI/issues/159,,None
https://github.com/vaexio/vaex/issues/363,,None
https://github.com/scikit-learn/scikit-learn/issues/15884,,None
https://github.com/nilearn/nilearn/pull/2256,,None
https://github.com/scikit-learn/scikit-learn/issues/14905,,None
https://github.com/scikit-learn/scikit-learn/issues/15805,,None
https://github.com/bioinfo-ut/PhenotypeSeeker/issues/10,,None
https://github.com/snipsco/snips-nlu/issues/864,,None
https://github.com/scikit-learn-contrib/imbalanced-learn/issues/297,,None
https://github.com/Qiskit/qiskit-aqua/pull/722,,None
https://github.com/scikit-learn/scikit-learn/pull/15382,"Fixes #14625
(supersedes) Closes #14637Changes default of DummyClasssifer to prior (with deprecation)Moved this ahead because this needs a deprecation cycle to complete.CC @glemaitre @NicolasHug @jnothmanLooks good but still a few commentGood to merge, @NicolasHug?thanks @thomasjpfan",None yet
